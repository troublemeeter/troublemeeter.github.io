<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>MySQL 8.0.19 Windows以及Navicat15破解版安装教程</title>
    <url>/install-mysql/</url>
    <content><![CDATA[<p>Windows10系统<br>MySQL安装版本：8.0.19<br>Navicat安装版本：premium 15 for mysql</p>
<p>Hint:</p>
<pre><code>SQL为结构化查询语句
MySQL, SQL Server, Oracle等为数据库管理系统
Navicat, SQLyog等为数据库管理工具</code></pre><a id="more"></a>
<h1 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h1><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>官网下载地址，选择64位安装包：mysql-8.0.19-winx64.zip。</p>
<pre><code>https://dev.mysql.com/downloads/mysql/</code></pre><p>然而下载速度很慢，可以切换国内镜像。搜狐镜像也比较慢，网易镜像速度飞快。</p>
<pre><code>http://mirrors.163.com/mysql/Downloads/MySQL-8.0/
http://mirrors.sohu.com/mysql/MySQL-8.0/</code></pre><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><p>解压压缩包至指定的安装目录如：<code>D:\Program\MySQL</code>。</p>
<h3 id="添加配置文件"><a href="#添加配置文件" class="headerlink" title="添加配置文件"></a>添加配置文件</h3><p>手动新建配置文件<code>my.ini</code>，写入内容（无需手动创建data文件夹）：</p>
<pre><code>[mysqld]
# 设置3306端口
port=3306
# 设置mysql的安装目录
basedir=D:\Program\MySQL
# 设置mysql数据库的数据的存放目录
datadir=D:\Program\MySQL\data
# 允许最大连接数
max_connections=200
# 允许连接失败的次数。
max_connect_errors=10
# 服务端使用的字符集默认为utf8mb4
character-set-server=utf8mb4
# 创建新表时将使用的默认存储引擎
default-storage-engine=INNODB
# 默认使用“mysql_native_password”插件认证
#mysql_native_password
default_authentication_plugin=mysql_native_password
[mysql]
# 设置mysql客户端默认字符集
default-character-set=utf8mb4
[client]
# 设置mysql客户端连接服务端时默认使用的端口
port=3306
default-character-set=utf8mb4</code></pre><h3 id="初始化数据库"><a href="#初始化数据库" class="headerlink" title="初始化数据库"></a>初始化数据库</h3><p>管理员身份打开cmd窗口，执行如下命令，将生成root用户的初始密码，用作后续登录。</p>
<pre><code>mysqld --initialize --user=mysql --console</code></pre><h2 id="安装数据库"><a href="#安装数据库" class="headerlink" title="安装数据库"></a>安装数据库</h2><ol>
<li>执行命令：<code>mysqld -install</code></li>
<li>启动服务：<code>net start mysql</code></li>
<li>登录数据库：<code>mysql -u root -p</code>，并输入刚刚得到的密码</li>
<li>修改密码：<code>alter user &#39;root&#39;@&#39;localhost&#39; identified by &#39;yourpassword&#39;</code></li>
<li>退出：<code>exit</code></li>
</ol>
<h3 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h3><p>若安装过程中报错，<code>VCRUNTIME140_1.dll</code>问题，微软官网下载适用于Visual Studio 2015、2017 和 2019 的 Microsoft Visual C++ 可再发行软件包：</p>
<pre><code>https://support.microsoft.com/zh-cn/help/2977003/the-latest-supported-visual-c-downloads</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><pre><code>https://blog.csdn.net/qq_37350706/article/details/81707862</code></pre><h1 id="Navicat"><a href="#Navicat" class="headerlink" title="Navicat"></a>Navicat</h1><p>参考如下，玄学破解，需要多试几次。</p>
<pre><code>https://www.cnblogs.com/runw/p/12255962.html</code></pre><p>官方使用手册</p>
<pre><code>https://www.navicat.com.cn/support/online-manual</code></pre>]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP 汇总链接合集</title>
    <url>/nlp-url-collection/</url>
    <content><![CDATA[<a id="more"></a>

<ol>
<li>中文自然语言处理 Chinese NLP<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;chinesenlp.xyz&#x2F;#&#x2F;zh&#x2F;docs&#x2F;</span></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>常见机器学习分类模型Python实现</title>
    <url>/classification-models/</url>
    <content><![CDATA[<p>机器学习分类模型代码实现：KNN, LDA, NB, LR, CART, SVM, RF, XGBOOST, LightGBDT</p>
<a id="more"></a>

<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score, GridSearchCV</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, accuracy_score, roc_curve, auc</span></pre></td></tr><tr><td class="code"><pre><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>]=[<span class="string">'SimHei'</span>]  </span></pre></td></tr><tr><td class="code"><pre><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>]=<span class="literal">False</span>  </span></pre></td></tr><tr><td class="code"><pre><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.read_table(<span class="string">'australian.dat'</span>,sep=<span class="string">' '</span>, header=<span class="literal">None</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">df.columns = [<span class="string">'A1'</span>, <span class="string">'A2'</span>, <span class="string">'A3'</span>, <span class="string">'A4'</span>, <span class="string">'A5'</span>, <span class="string">'A6'</span>, <span class="string">'A7'</span>, <span class="string">'A8'</span>, <span class="string">'A9'</span>, <span class="string">'A10'</span>, <span class="string">'A11'</span>, <span class="string">'A12'</span>, <span class="string">'A13'</span>, <span class="string">'A14'</span>, <span class="string">'Default'</span>]</span></pre></td></tr></table></figure>

<ol>
<li><p>分类变量处理：哑变量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">categorical_columns = [<span class="string">'A1'</span>, <span class="string">'A4'</span>, <span class="string">'A5'</span>, <span class="string">'A6'</span>, <span class="string">'A8'</span>, <span class="string">'A9'</span>, <span class="string">'A11'</span>, <span class="string">'A12'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">df = pd.get_dummies(df, columns = categorical_columns)</span></pre></td></tr><tr><td class="code"><pre><span class="line">df.describe()</span></pre></td></tr></table></figure>
<div>
<style scoped>
 .dataframe tbody tr th:only-of-type {
     vertical-align: middle;
 }

<p> .dataframe tbody tr th {</p>
<pre><code>vertical-align: top;</code></pre><p> }</p>
<p> .dataframe thead th {</p>
<pre><code>text-align: right;</code></pre><p> }<br></style></p>
<table border="1" class="dataframe">
<thead>
 <tr style="text-align: right;">
   <th></th>
   <th>A2</th>
   <th>A3</th>
   <th>A7</th>
   <th>A10</th>
   <th>A13</th>
   <th>A14</th>
   <th>Default</th>
   <th>A1_0</th>
   <th>A1_1</th>
   <th>A4_1</th>
   <th>...</th>
   <th>A6_9</th>
   <th>A8_0</th>
   <th>A8_1</th>
   <th>A9_0</th>
   <th>A9_1</th>
   <th>A11_0</th>
   <th>A11_1</th>
   <th>A12_1</th>
   <th>A12_2</th>
   <th>A12_3</th>
 </tr>
</thead>
<tbody>
 <tr>
   <th>count</th>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.00000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>...</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
 </tr>
 <tr>
   <th>mean</th>
   <td>31.568203</td>
   <td>4.758725</td>
   <td>2.223406</td>
   <td>2.40000</td>
   <td>184.014493</td>
   <td>1018.385507</td>
   <td>0.444928</td>
   <td>0.321739</td>
   <td>0.678261</td>
   <td>0.236232</td>
   <td>...</td>
   <td>0.011594</td>
   <td>0.476812</td>
   <td>0.523188</td>
   <td>0.572464</td>
   <td>0.427536</td>
   <td>0.542029</td>
   <td>0.457971</td>
   <td>0.082609</td>
   <td>0.905797</td>
   <td>0.011594</td>
 </tr>
 <tr>
   <th>std</th>
   <td>11.853273</td>
   <td>4.978163</td>
   <td>3.346513</td>
   <td>4.86294</td>
   <td>172.159274</td>
   <td>5210.102598</td>
   <td>0.497318</td>
   <td>0.467482</td>
   <td>0.467482</td>
   <td>0.425074</td>
   <td>...</td>
   <td>0.107128</td>
   <td>0.499824</td>
   <td>0.499824</td>
   <td>0.495080</td>
   <td>0.495080</td>
   <td>0.498592</td>
   <td>0.498592</td>
   <td>0.275490</td>
   <td>0.292323</td>
   <td>0.107128</td>
 </tr>
 <tr>
   <th>min</th>
   <td>13.750000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.00000</td>
   <td>0.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>...</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
 </tr>
 <tr>
   <th>25%</th>
   <td>22.670000</td>
   <td>1.000000</td>
   <td>0.165000</td>
   <td>0.00000</td>
   <td>80.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>...</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
 </tr>
 <tr>
   <th>50%</th>
   <td>28.625000</td>
   <td>2.750000</td>
   <td>1.000000</td>
   <td>0.00000</td>
   <td>160.000000</td>
   <td>6.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
   <td>...</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
 </tr>
 <tr>
   <th>75%</th>
   <td>37.707500</td>
   <td>7.207500</td>
   <td>2.625000</td>
   <td>3.00000</td>
   <td>272.000000</td>
   <td>396.500000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
   <td>...</td>
   <td>0.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
 </tr>
 <tr>
   <th>max</th>
   <td>80.250000</td>
   <td>28.000000</td>
   <td>28.500000</td>
   <td>67.00000</td>
   <td>2000.000000</td>
   <td>100001.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>...</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
 </tr>
</tbody>
</table>
<p>8 rows × 43 columns</p>
</div>
</li>
<li><p>连续变量处理：标准化<br>先划分训练集和测试集，在求训练集的连续变量每一列的方差和均值，用得到的训练集的方差和均值分别对训练集和测试集做标准化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df, test_df = train_test_split(df, test_size = <span class="number">0.3</span>, random_state= <span class="number">2019</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">train_y = train_df.Default</span></pre></td></tr><tr><td class="code"><pre><span class="line">test_y = test_df.Default</span></pre></td></tr><tr><td class="code"><pre><span class="line">train_x = train_df.drop([<span class="string">'Default'</span>], axis = <span class="number">1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">test_x = test_df.drop([<span class="string">'Default'</span>], axis = <span class="number">1</span>)</span></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">continus_keys = train_x.keys()[:<span class="number">6</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">continus_keys</span></pre></td></tr></table></figure>
<p> Index([‘A2’, ‘A3’, ‘A7’, ‘A10’, ‘A13’, ‘A14’], dtype=’object’)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> continus_keys:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    std,mean = [np.std(train_x[each]),np.mean(train_x[each])]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    f = <span class="keyword">lambda</span> x: (x-mean)/(std)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    train_x[each] = train_x[each].apply(f)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    test_x[each] = test_x[each].apply(f)</span></pre></td></tr></table></figure>
<p>训练集和测试集的维度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_x.shape,train_x.shape</span></pre></td></tr><tr><td class="code"><pre><span class="line">((<span class="number">207</span>, <span class="number">42</span>), (<span class="number">483</span>, <span class="number">42</span>))</span></pre></td></tr></table></figure></li>
<li><p>roc曲线函数和混淆矩阵函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_roc_curve</span><span class="params">(test_y, predictions)</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    false_positive_rate, recall, thresholds = roc_curve(test_y, predictions)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    roc_auc = auc(false_positive_rate, recall)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.title(<span class="string">'Receiver Operating Characteristic (ROC)'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.plot(false_positive_rate, recall, <span class="string">'b'</span>, label = <span class="string">'AUC = %0.3f'</span> %roc_auc)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.legend(loc=<span class="string">'lower right'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.plot([<span class="number">0</span>,<span class="number">1</span>], [<span class="number">0</span>,<span class="number">1</span>], <span class="string">'r--'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xlim([<span class="number">0.0</span>,<span class="number">1.0</span>])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.ylim([<span class="number">0.0</span>,<span class="number">1.0</span>])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.ylabel(<span class="string">'Recall'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xlabel(<span class="string">'Fall-out (1-Specificity)'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.show()</span></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_confusion_matrix</span><span class="params">(test_y, predictions)</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    cm = confusion_matrix(test_y, predictions)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    labels = [<span class="string">'No Default'</span>, <span class="string">'Default'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span></pre></td></tr><tr><td class="code"><pre><span class="line">    sns.heatmap(cm,xticklabels=labels, yticklabels=labels, annot=<span class="literal">True</span>, fmt=<span class="string">'d'</span>, cmap=<span class="string">"Blues"</span>, vmin = <span class="number">0.2</span>);</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.title(<span class="string">'Confusion Matrix'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.ylabel(<span class="string">'True Class'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xlabel(<span class="string">'Predicted Class'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.show()</span></pre></td></tr><tr><td class="code"><pre><span class="line">    target_names = [<span class="string">'class 0'</span>, <span class="string">'class 1'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(classification_report(test_y, predictions, target_names=target_names))</span></pre></td></tr><tr><td class="code"><pre><span class="line">    recall = metrics.recall_score(test_y, predictions)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    f1 = metrics.f1_score(test_y, predictions)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    acc = metrics.accuracy_score(test_y, predictions)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    pre = metrics.precision_score(test_y, predictions)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    auc = metrics.roc_auc_score(test_y,predictions)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">return</span> [recall,pre,acc,f1,auc]</span></pre></td></tr></table></figure>


</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">RESULT = &#123;&#125;</span></pre></td></tr></table></figure>

<h1 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span></pre></td></tr></table></figure>

<p>GridSearchCV 为网格搜索， 在训练集上进行10折交叉验证，目的找到最优参数<br>搜索参数：<br>k_range：领域大小<br>p: 距离定义,p=2:欧式距离;p=1：曼哈顿距离<br>这里距离定义都是用于连续变量，但是数据集中还有分类变量，存在一点不合理，ppt还是别展示，避免被问到</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knn = KNeighborsClassifier()</span></pre></td></tr><tr><td class="code"><pre><span class="line">k_range = list(range(<span class="number">5</span>,<span class="number">15</span>))</span></pre></td></tr><tr><td class="code"><pre><span class="line">p = [<span class="number">1</span>,<span class="number">2</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">param_gridknn = dict(n_neighbors = k_range,p=p)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridKNN = GridSearchCV(knn, param_gridknn, cv=<span class="number">10</span>, scoring=<span class="string">'roc_auc'</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridKNN.fit(train_x,train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'best params are:'</span>,str(gridKNN.best_params_))</span></pre></td></tr></table></figure>

<pre><code>Fitting 10 folds for each of 20 candidates, totalling 200 fits


[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    7.4s


best params are: {&apos;n_neighbors&apos;: 10, &apos;p&apos;: 1}


[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    8.6s finished</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">10</span>, p=<span class="number">1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">knn.fit(train_x, train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_Knn_prob = knn.predict(test_x)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_Knn_01 = np.where(predictions_Knn_prob &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) </span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_knn = accuracy_score(test_y, predictions_Knn_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of KNN model:'</span>, acc_knn)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y, predictions_Knn_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_Knn_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'KNN'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of KNN model: 0.8647342995169082


/usr/local/lib/python3.6/dist-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family [&apos;sans-serif&apos;] not found. Falling back to DejaVu Sans.
  (prop.get_family(), self.defaultFamily[fontext]))
/usr/local/lib/python3.6/dist-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family [&apos;sans-serif&apos;] not found. Falling back to DejaVu Sans.
  (prop.get_family(), self.defaultFamily[fontext]))
/usr/local/lib/python3.6/dist-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family [&apos;sans-serif&apos;] not found. Falling back to DejaVu Sans.
  (prop.get_family(), self.defaultFamily[fontext]))</code></pre><p><img src="/images/classification-models/output_19_2.png" alt="png"></p>
<p><img src="/images/classification-models/output_19_3.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.86      0.89      0.88       113
     class 1       0.87      0.83      0.85        94

   micro avg       0.86      0.86      0.86       207
   macro avg       0.86      0.86      0.86       207
weighted avg       0.86      0.86      0.86       207</code></pre><h1 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lda = LinearDiscriminantAnalysis()</span></pre></td></tr><tr><td class="code"><pre><span class="line">lda.fit(train_x, train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LDA_01 = lda.predict(test_x)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LDA_prob = lda.predict_proba(test_x)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LDA_prob = predictions_LDA_prob[:,<span class="number">1</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_lda = accuracy_score(test_y, predictions_LDA_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of LDA model:'</span>, acc_lda)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y, predictions_LDA_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_LDA_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'LDA'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of LDA model: 0.8357487922705314


/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn(&quot;Variables are collinear.&quot;)</code></pre><p><img src="/images/classification-models/output_22_2.png" alt="png"></p>
<p><img src="/images/classification-models/output_22_3.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.93      0.75      0.83       113
     class 1       0.76      0.94      0.84        94

   micro avg       0.84      0.84      0.84       207
   macro avg       0.85      0.84      0.84       207
weighted avg       0.85      0.84      0.84       207</code></pre><h1 id="NB"><a href="#NB" class="headerlink" title="NB"></a>NB</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> BernoulliNB, GaussianNB, MultinomialNB</span></pre></td></tr></table></figure>

<p>GaussianNB就是先验为高斯分布的朴素贝叶斯，<br>MultinomialNB就是先验为多项式分布的朴素贝叶斯，<br>而BernoulliNB就是先验为伯努利分布的朴素贝叶斯。<br>贝叶斯方法数据使用未经标准化的数据，保持原有数据的分布</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df1, test_df1 = train_test_split(df, test_size = <span class="number">0.3</span>, random_state= <span class="number">2019</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">train_y1 = train_df1.Default</span></pre></td></tr><tr><td class="code"><pre><span class="line">test_y1 = test_df1.Default</span></pre></td></tr><tr><td class="code"><pre><span class="line">train_x1 = train_df1.drop([<span class="string">'Default'</span>], axis = <span class="number">1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">test_x1 = test_df1.drop([<span class="string">'Default'</span>], axis = <span class="number">1</span>)</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classifiers = &#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'BNB'</span>: BernoulliNB(),</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'GNB'</span>: GaussianNB(),</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'MNB'</span>: MultinomialNB()&#125;</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> name, clf <span class="keyword">in</span> classifiers.items():</span></pre></td></tr><tr><td class="code"><pre><span class="line">    scores = cross_val_score(clf, train_x1, train_y1, cv=<span class="number">10</span>, scoring=<span class="string">'roc_auc'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(name,<span class="string">'\t--&gt; '</span>,scores.mean())</span></pre></td></tr></table></figure>

<pre><code>BNB     --&gt;  0.9022927689594358
GNB     --&gt;  0.9014189514189515
MNB     --&gt;  0.6811768478435146</code></pre><p>可见BernoulliNB在NB大类中相对较优</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf = BernoulliNB() </span></pre></td></tr><tr><td class="code"><pre><span class="line">clf.fit(train_x1, train_y1)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#Predict on test set</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_Naive_Bayes_prob = clf.predict_proba(test_x1)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_Naive_Bayes_prob = predictions_Naive_Bayes_prob[:,<span class="number">1</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_Naive_Bayes_01 = clf.predict(test_x1)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#Print accuracy</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_Naive = accuracy_score(test_y, predictions_Naive_Bayes_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Naive Bayes model:'</span>, acc_Naive)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y1, predictions_Naive_Bayes_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_Naive_Bayes_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'MNB'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of Naive Bayes model: 0.8357487922705314</code></pre><p><img src="/images/classification-models/output_30_1.png" alt="png"></p>
<p><img src="/images/classification-models/output_30_2.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.88      0.81      0.84       113
     class 1       0.79      0.87      0.83        94

   micro avg       0.84      0.84      0.84       207
   macro avg       0.84      0.84      0.84       207
weighted avg       0.84      0.84      0.84       207</code></pre><h1 id="LR"><a href="#LR" class="headerlink" title="LR"></a>LR</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span></pre></td></tr></table></figure>

<p>先不做特征处理，不做标准化，直接进行拟合 </p>
<p>参数选择：<br>正则化：l1,l2<br>正则化参数：C<br>样本权重：平衡权重，均匀权重</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = LogisticRegression()</span></pre></td></tr><tr><td class="code"><pre><span class="line">penalty  = [<span class="string">'l1'</span>,<span class="string">'l2'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">C = [<span class="number">1</span>,<span class="number">0.3</span>,<span class="number">0.1</span>,<span class="number">0.03</span>,<span class="number">0.01</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">class_weight = [<span class="string">'balanced'</span>,<span class="literal">None</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">param_gridknn = dict(penalty=penalty,C=C,class_weight=class_weight)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridLR = GridSearchCV(lr, param_gridknn, cv=<span class="number">10</span>, scoring=<span class="string">'roc_auc'</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridLR.fit(train_x1,train_y1)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'best params are:'</span>,str(gridLR.best_params_))</span></pre></td></tr></table></figure>

<pre><code>Fitting 10 folds for each of 20 candidates, totalling 200 fits


[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.
[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    5.0s


best params are: {&apos;C&apos;: 1, &apos;class_weight&apos;: None, &apos;penalty&apos;: &apos;l1&apos;}


[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    5.3s finished
/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logreg = LogisticRegression(class_weight=<span class="literal">None</span>,C=<span class="number">1</span>,penalty=<span class="string">'l1'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">logreg.fit(train_x1, train_y1)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_prob = logreg.predict_proba(test_x1)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_prob = predictions_LogReg_prob[:,<span class="number">1</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_01 = logreg.predict(test_x1)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_logit = accuracy_score(test_y1, predictions_LogReg_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Logistic Regression model:'</span>, acc_logit)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y1, predictions_LogReg_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y1, predictions_LogReg_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'lr'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)


Overall accuracy of Logistic Regression model: 0.8647342995169082</code></pre><p><img src="/images/classification-models/output_35_2.png" alt="png"></p>
<p><img src="/images/classification-models/output_35_3.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.93      0.81      0.87       113
     class 1       0.81      0.93      0.86        94

   micro avg       0.86      0.86      0.86       207
   macro avg       0.87      0.87      0.86       207
weighted avg       0.87      0.86      0.86       207</code></pre><p>对’A3’, ‘A7’, ‘A10’, ‘A13’, ‘A14’连续变量离散化后(全部等宽离散化为4组)，进行拟合<br>之所以选择’A3’, ‘A7’, ‘A10’, ‘A13’, ‘A14’使因为这些列的数据分布很不均匀，而A2较均匀。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.read_table(<span class="string">'australian.dat'</span>,sep=<span class="string">' '</span>, header=<span class="literal">None</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">df.columns = [<span class="string">'A1'</span>, <span class="string">'A2'</span>, <span class="string">'A3'</span>, <span class="string">'A4'</span>, <span class="string">'A5'</span>, <span class="string">'A6'</span>, <span class="string">'A7'</span>, <span class="string">'A8'</span>, <span class="string">'A9'</span>, <span class="string">'A10'</span>, <span class="string">'A11'</span>, <span class="string">'A12'</span>, <span class="string">'A13'</span>, <span class="string">'A14'</span>, <span class="string">'Default'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">df1 = df</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> [<span class="string">'A3'</span>, <span class="string">'A7'</span>, <span class="string">'A10'</span>, <span class="string">'A13'</span>, <span class="string">'A14'</span>]:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    df1[each] = pd.cut(df1[each],bins=<span class="number">4</span>,labels=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span></pre></td></tr><tr><td class="code"><pre><span class="line">df1 = pd.get_dummies(df1)</span></pre></td></tr><tr><td class="code"><pre><span class="line">train_df1, test_df1 = train_test_split(df1, test_size = <span class="number">0.3</span>, random_state= <span class="number">2019</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">train_y1 = train_df1.Default</span></pre></td></tr><tr><td class="code"><pre><span class="line">test_y1 = test_df1.Default</span></pre></td></tr><tr><td class="code"><pre><span class="line">train_x1 = train_df1.drop([<span class="string">'Default'</span>], axis = <span class="number">1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">test_x1 = test_df1.drop([<span class="string">'Default'</span>], axis = <span class="number">1</span>)</span></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead.
  &quot;&quot;&quot;Entry point for launching an IPython kernel.</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = LogisticRegression()</span></pre></td></tr><tr><td class="code"><pre><span class="line">penalty  = [<span class="string">'l1'</span>,<span class="string">'l2'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">C = [<span class="number">1</span>,<span class="number">0.3</span>,<span class="number">0.1</span>,<span class="number">0.03</span>,<span class="number">0.01</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">class_weight = [<span class="string">'balanced'</span>,<span class="literal">None</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">param_gridknn = dict(penalty=penalty,C=C,class_weight=class_weight)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridLR = GridSearchCV(lr, param_gridknn, cv=<span class="number">10</span>, scoring=<span class="string">'roc_auc'</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridLR.fit(train_x1,train_y1)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'best params are:'</span>,str(gridLR.best_params_))</span></pre></td></tr></table></figure>

<pre><code>Fitting 10 folds for each of 20 candidates, totalling 200 fits


[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    6.1s
[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    6.7s finished


best params are: {&apos;C&apos;: 1, &apos;class_weight&apos;: None, &apos;penalty&apos;: &apos;l1&apos;}</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logreg = LogisticRegression(class_weight=<span class="literal">None</span>,C=<span class="number">1</span>,penalty=<span class="string">'l1'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">logreg.fit(train_x1, train_y1)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_prob = logreg.predict_proba(test_x1)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_prob = predictions_LogReg_prob[:,<span class="number">1</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_01 = logreg.predict(test_x1)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_logit = accuracy_score(test_y1, predictions_LogReg_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Logistic Regression model:'</span>, acc_logit)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y1, predictions_LogReg_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y1, predictions_LogReg_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'lr1'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)


Overall accuracy of Logistic Regression model: 0.855072463768116</code></pre><p><img src="/images/classification-models/output_39_2.png" alt="png"></p>
<p><img src="/images/classification-models/output_39_3.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.93      0.80      0.86       113
     class 1       0.79      0.93      0.85        94

   micro avg       0.86      0.86      0.86       207
   macro avg       0.86      0.86      0.86       207
weighted avg       0.87      0.86      0.86       207</code></pre><p>特征离散化和不离散化结果几乎一致，没有特别的差距，应该是数据集的特点</p>
<h1 id="Decision-Tree-：Cart"><a href="#Decision-Tree-：Cart" class="headerlink" title="Decision Tree ：Cart"></a>Decision Tree ：Cart</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span></pre></td></tr></table></figure>

<p>参数选择：<br>树深：max_depth  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dt = DecisionTreeClassifier()</span></pre></td></tr><tr><td class="code"><pre><span class="line">max_depth = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>, <span class="literal">None</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">param_griddt = dict(max_depth = max_depth)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridKNN = GridSearchCV(dt, param_griddt, cv=<span class="number">10</span>, scoring=<span class="string">'roc_auc'</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridKNN.fit(train_x,train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'best params are:'</span>,str(gridKNN.best_params_))</span></pre></td></tr></table></figure>

<pre><code>Fitting 10 folds for each of 5 candidates, totalling 50 fits
best params are: {&apos;max_depth&apos;: 2}


[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    5.7s finished</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dt = DecisionTreeClassifier(max_depth=<span class="number">2</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">dt.fit(train_x, train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_dt_prob = dt.predict(test_x)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_dt_01 = np.where(predictions_dt_prob &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) </span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_dt = accuracy_score(test_y, predictions_dt_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Decision Tree model:'</span>, acc_dt)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y, predictions_dt_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_dt_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'DT'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of Decision Tree model: 0.8405797101449275</code></pre><p><img src="/images/classification-models/output_45_1.png" alt="png"></p>
<p><img src="/images/classification-models/output_45_2.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.94      0.75      0.84       113
     class 1       0.76      0.95      0.84        94

   micro avg       0.84      0.84      0.84       207
   macro avg       0.85      0.85      0.84       207
weighted avg       0.86      0.84      0.84       207</code></pre><h1 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span></pre></td></tr></table></figure>

<p>参数选择：<br>核函数：多项式，sigmoid，高斯核函数，线性核函数，线性核函数<br>gamma:核函数系数<br>C：正则项系数 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">svm = SVC()</span></pre></td></tr><tr><td class="code"><pre><span class="line">param_gridsvm = [&#123;<span class="string">'kernel'</span>: [<span class="string">'poly'</span>, <span class="string">'rbf'</span>, <span class="string">'sigmoid'</span>], <span class="string">'gamma'</span>: [<span class="number">0.001</span>,<span class="number">0.003</span>,<span class="number">0.01</span>,<span class="number">0.03</span>,<span class="number">0.1</span>,<span class="number">1</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">                     <span class="string">'C'</span>: [<span class="number">0.01</span>,<span class="number">0.03</span>,<span class="number">0.1</span>,<span class="number">1</span>, <span class="number">3</span>,<span class="number">10</span>,<span class="number">30</span>, <span class="number">100</span>,<span class="number">300</span>, <span class="number">1000</span>]&#125;,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    &#123;<span class="string">'kernel'</span>: [<span class="string">'linear'</span>], <span class="string">'C'</span>: [<span class="number">0.01</span>,<span class="number">0.03</span>,<span class="number">0.1</span>,<span class="number">1</span>, <span class="number">3</span>,<span class="number">10</span>,<span class="number">30</span>, <span class="number">100</span>,<span class="number">300</span>, <span class="number">1000</span>]&#125;]</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridsvm = GridSearchCV(svm, param_gridsvm, cv=<span class="number">10</span>, scoring=<span class="string">'roc_auc'</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridsvm.fit(train_x,train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'best params are:'</span>,str(gridsvm.best_params_))</span></pre></td></tr></table></figure>

<pre><code>Fitting 10 folds for each of 190 candidates, totalling 1900 fits


[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.
[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:    6.3s
[Parallel(n_jobs=-1)]: Done 1156 tasks      | elapsed:   25.0s


best params are: {&apos;C&apos;: 0.1, &apos;gamma&apos;: 0.1, &apos;kernel&apos;: &apos;poly&apos;}


[Parallel(n_jobs=-1)]: Done 1900 out of 1900 | elapsed:  1.9min finished
/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.
  DeprecationWarning)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">svm = SVC(C=<span class="number">0.1</span>, gamma=<span class="number">0.1</span>, kernel=<span class="string">'poly'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">svm.fit(train_x, train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_svm_prob = svm.predict(test_x)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_svm_01 = np.where(predictions_svm_prob &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) </span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_svm = accuracy_score(test_y, predictions_svm_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of SVM model:'</span>, acc_knn)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y, predictions_svm_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_svm_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'SVM'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of SVM model: 0.8647342995169082</code></pre><p><img src="/images/classification-models/output_50_1.png" alt="png"></p>
<p><img src="/images/classification-models/output_50_2.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.87      0.86      0.87       113
     class 1       0.83      0.85      0.84        94

   micro avg       0.86      0.86      0.86       207
   macro avg       0.85      0.85      0.85       207
weighted avg       0.86      0.86      0.86       207</code></pre><h1 id="RF"><a href="#RF" class="headerlink" title="RF"></a>RF</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span></pre></td></tr></table></figure>

<p>参数选择：<br>n_estimaors: 森林中树的个数<br>max_depth: 最大树深  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rf = RandomForestClassifier()</span></pre></td></tr><tr><td class="code"><pre><span class="line">n_estimators = list(range(<span class="number">5</span>,<span class="number">101</span>,<span class="number">10</span>))</span></pre></td></tr><tr><td class="code"><pre><span class="line">max_depth = list(range(<span class="number">2</span>,<span class="number">20</span>,<span class="number">2</span>))</span></pre></td></tr><tr><td class="code"><pre><span class="line">param_gridrf = dict(n_estimators = n_estimators, max_depth=max_depth)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridRF = GridSearchCV(rf, param_gridrf, cv=<span class="number">10</span>, scoring=<span class="string">'roc_auc'</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridRF.fit(train_x,train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'best params are:'</span>,str(gridRF.best_params_))</span></pre></td></tr></table></figure>

<pre><code>Fitting 10 folds for each of 90 candidates, totalling 900 fits


[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    7.2s
[Parallel(n_jobs=-1)]: Done 348 tasks      | elapsed:   18.9s
[Parallel(n_jobs=-1)]: Done 848 tasks      | elapsed:   37.3s


best params are: {&apos;max_depth&apos;: 10, &apos;n_estimators&apos;: 95}


[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed:   40.0s finished</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rf = RandomForestClassifier(n_estimators=<span class="number">95</span>, max_depth=<span class="number">10</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">rf.fit(train_x, train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_rf_prob = rf.predict(test_x)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_rf_01 = np.where(predictions_rf_prob &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) </span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_rf = accuracy_score(test_y, predictions_rf_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Random Forest model:'</span>, acc_rf)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y, predictions_rf_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_rf_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'RF'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of Random Forest model: 0.855072463768116</code></pre><p><img src="/images/classification-models/output_55_1.png" alt="png"></p>
<p><img src="/images/classification-models/output_55_2.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.90      0.82      0.86       113
     class 1       0.81      0.89      0.85        94

   micro avg       0.86      0.86      0.86       207
   macro avg       0.86      0.86      0.85       207
weighted avg       0.86      0.86      0.86       207</code></pre><h1 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span></pre></td></tr></table></figure>

<p>参数选择：<br>学习率<br>弱学习器的个数<br>最大树深<br>样本采样比列<br>叶子权重正则系数<br>叶子个数正则系数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">XGB = xgb.XGBClassifier()</span></pre></td></tr><tr><td class="code"><pre><span class="line">param_gridxgb = dict(</span></pre></td></tr><tr><td class="code"><pre><span class="line">    learning_rate = [<span class="number">1</span>,<span class="number">0.1</span>,<span class="number">0.01</span>], </span></pre></td></tr><tr><td class="code"><pre><span class="line">    n_estimators = [<span class="number">8</span>,<span class="number">16</span>,<span class="number">32</span>,<span class="number">64</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    max_depth = [<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    subsample = [<span class="number">1</span>,<span class="number">0.8</span>,<span class="number">0.6</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    reg_alpha = [<span class="number">1</span>,<span class="number">0.1</span>,<span class="number">0</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    reg_lambda = [<span class="number">1</span>,<span class="number">0.1</span>,<span class="number">0</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridxgb = GridSearchCV(XGB, param_gridxgb, cv=<span class="number">5</span>, scoring=<span class="string">'roc_auc'</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridxgb.fit(train_x,train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'best params are:'</span>,str(gridxgb.best_params_))</span></pre></td></tr></table></figure>

<pre><code>Fitting 5 folds for each of 1620 candidates, totalling 8100 fits


[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.
[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:    4.6s
[Parallel(n_jobs=-1)]: Done 1481 tasks      | elapsed:   37.6s
[Parallel(n_jobs=-1)]: Done 3481 tasks      | elapsed:  1.6min
[Parallel(n_jobs=-1)]: Done 5277 tasks      | elapsed:  2.7min
[Parallel(n_jobs=-1)]: Done 7077 tasks      | elapsed:  3.8min


best params are: {&apos;learning_rate&apos;: 0.1, &apos;max_depth&apos;: 4, &apos;n_estimators&apos;: 32, &apos;reg_alpha&apos;: 0, &apos;reg_lambda&apos;: 0.1, &apos;subsample&apos;: 0.6}


[Parallel(n_jobs=-1)]: Done 8100 out of 8100 | elapsed:  4.5min finished</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gridxgb.best_params_</span></pre></td></tr></table></figure>




<pre><code>{&apos;learning_rate&apos;: 0.1,
 &apos;max_depth&apos;: 4,
 &apos;n_estimators&apos;: 32,
 &apos;reg_alpha&apos;: 0,
 &apos;reg_lambda&apos;: 0.1,
 &apos;subsample&apos;: 0.6}</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">% matplotlib inline</span></pre></td></tr><tr><td class="code"><pre><span class="line">xgdmat = xgb.DMatrix(train_x, train_y) <span class="comment"># Create our DMatrix to make XGBoost more efficient</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">testdmat = xgb.DMatrix(test_x)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># final_gb = xgb.train(gridxgb.best_params_, xgdmat)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># predictions_xgb_prob = final_gb.predict(testdmat)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># predictions_xgb_01 = np.where(predictions_xgb_prob &gt; 0.5, 1, 0) #Turn probability to 0-1 binary output</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># xgb.plot_importance(final_gb)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">trainxdmat = xgb.DMatrix(train_x) <span class="comment"># Create our DMatrix to make XGBoost more efficient</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">trainydmat = xgb.DMatrix(train_y) <span class="comment"># Create our DMatrix to make XGBoost more efficient</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">XGB = xgb.XGBClassifier(learning_rate=<span class="number">0.1</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                        max_depth=<span class="number">4</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                        n_estimators= <span class="number">32</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                        reg_alpha= <span class="number">0</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                        reg_lambda= <span class="number">0.1</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                        subsample= <span class="number">0.6</span>,                    </span></pre></td></tr><tr><td class="code"><pre><span class="line">                       )</span></pre></td></tr><tr><td class="code"><pre><span class="line">XGB.fit(train_x.values,train_y.values)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'1'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_xgb_prob = XGB.predict(test_x.values)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_xgb_01 = np.where(predictions_xgb_prob &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) <span class="comment">#Turn probability to 0-1 binary output</span></span></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version
  if getattr(data, &apos;base&apos;, None) is not None and \
/usr/local/lib/python3.6/dist-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version
  data.base is not None and isinstance(data, np.ndarray) \


1</code></pre><p><img src="/images/classification-models/output_61_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">acc_lgbm = accuracy_score(test_y,predictions_xgb_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Light GBM model:'</span>, acc_lgbm)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y, predictions_rf_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_rf_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'XGB'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of Light GBM model: 0.8647342995169082</code></pre><p><img src="/images/classification-models/output_62_1.png" alt="png"></p>
<p><img src="/images/classification-models/output_62_2.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.90      0.82      0.86       113
     class 1       0.81      0.89      0.85        94

   micro avg       0.86      0.86      0.86       207
   macro avg       0.86      0.86      0.85       207
weighted avg       0.86      0.86      0.86       207</code></pre><h1 id="LightGBM（GBDT）"><a href="#LightGBM（GBDT）" class="headerlink" title="LightGBM（GBDT）"></a>LightGBM（GBDT）</h1><p>先单纯使用GBDT</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data=lgb.Dataset(train_x,label=train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">params = &#123;<span class="string">'boosting_type'</span>: <span class="string">'gbdt'</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'max_depth'</span> : <span class="number">-1</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'objective'</span>: <span class="string">'binary'</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'nthread'</span>: <span class="number">5</span>, <span class="comment"># Updated from nthread</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'num_leaves'</span>: <span class="number">64</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'learning_rate'</span>: <span class="number">0.05</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'max_bin'</span>: <span class="number">512</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'subsample_for_bin'</span>: <span class="number">200</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'subsample'</span>: <span class="number">1</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'subsample_freq'</span>: <span class="number">1</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'reg_alpha'</span>: <span class="number">5</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'reg_lambda'</span>: <span class="number">10</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'min_split_gain'</span>: <span class="number">0.5</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'min_child_weight'</span>: <span class="number">1</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'min_child_samples'</span>: <span class="number">5</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'scale_pos_weight'</span>: <span class="number">1</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'num_class'</span> : <span class="number">1</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'metric'</span> : <span class="string">'binary_error'</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">          &#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">gridParams = &#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'learning_rate'</span>: [<span class="number">1</span>,<span class="number">0.1</span>,<span class="number">0.01</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'n_estimators'</span>: [<span class="number">8</span>,<span class="number">16</span>,<span class="number">32</span>,<span class="number">64</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'num_leaves'</span>: [<span class="number">50</span>,<span class="number">100</span>,<span class="number">150</span>,<span class="number">200</span>,<span class="number">250</span>,<span class="number">300</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'boosting_type'</span> : [<span class="string">'gbdt'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'objective'</span> : [<span class="string">'binary'</span>], </span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'colsample_bytree'</span> : [<span class="number">1</span>,<span class="number">0.8</span>,<span class="number">0.6</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'subsample'</span> : [<span class="number">1</span>,<span class="number">0.8</span>,<span class="number">0.6</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'reg_alpha'</span> : [<span class="number">10</span>,<span class="number">3</span>,<span class="number">1</span>, <span class="number">0.1</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'reg_lambda'</span> : [<span class="number">10</span>,<span class="number">3</span>,<span class="number">1</span>, <span class="number">0.1</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">mdl = lgb.LGBMClassifier(boosting_type= <span class="string">'gbdt'</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          objective = <span class="string">'binary'</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          n_jobs = <span class="number">5</span>, <span class="comment"># Updated from 'nthread'</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">          silent = <span class="literal">True</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          max_depth = params[<span class="string">'max_depth'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">          max_bin = params[<span class="string">'max_bin'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">          subsample_for_bin = params[<span class="string">'subsample_for_bin'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">          subsample = params[<span class="string">'subsample'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">          subsample_freq = params[<span class="string">'subsample_freq'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">          min_split_gain = params[<span class="string">'min_split_gain'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">          min_child_weight = params[<span class="string">'min_child_weight'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">          min_child_samples = params[<span class="string">'min_child_samples'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">          scale_pos_weight = params[<span class="string">'scale_pos_weight'</span>])</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">mdl.get_params().keys()</span></pre></td></tr></table></figure>




<pre><code>dict_keys([&apos;boosting_type&apos;, &apos;class_weight&apos;, &apos;colsample_bytree&apos;, &apos;importance_type&apos;, &apos;learning_rate&apos;, &apos;max_depth&apos;, &apos;min_child_samples&apos;, &apos;min_child_weight&apos;, &apos;min_split_gain&apos;, &apos;n_estimators&apos;, &apos;n_jobs&apos;, &apos;num_leaves&apos;, &apos;objective&apos;, &apos;random_state&apos;, &apos;reg_alpha&apos;, &apos;reg_lambda&apos;, &apos;silent&apos;, &apos;subsample&apos;, &apos;subsample_for_bin&apos;, &apos;subsample_freq&apos;, &apos;max_bin&apos;, &apos;scale_pos_weight&apos;])</code></pre><p>这里用的4折交叉验证，因为参数实在太多，跑不动，太慢了，其实用10折比较好<br>后来把正则项系数改大了，因为后面发现过拟合了，但是还没跑，就用原来的参数吧；或者你有时间就直接运行一下就可以</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grid = GridSearchCV(mdl, gridParams, verbose=<span class="number">1</span>, cv=<span class="number">4</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">grid.fit(train_x, train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(grid.best_params_)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(grid.best_score_)</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">% matplotlib inline</span></pre></td></tr><tr><td class="code"><pre><span class="line">best_p = &#123;<span class="string">'boosting_type'</span>: <span class="string">'gbdt'</span>, <span class="string">'colsample_bytree'</span>: <span class="number">0.6</span>, <span class="string">'learning_rate'</span>: <span class="number">1</span>, <span class="string">'n_estimators'</span>: <span class="number">16</span>, <span class="string">'num_leaves'</span>: <span class="number">50</span>, <span class="string">'objective'</span>: <span class="string">'binary'</span>, <span class="string">'reg_alpha'</span>: <span class="number">0.01</span>, <span class="string">'reg_lambda'</span>: <span class="number">1</span>, <span class="string">'subsample'</span>: <span class="number">1</span>&#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line">lgbm = lgb.train(best_p,<span class="comment">#grid.best_params_,</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">                 train_data,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                 <span class="number">2500</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                 verbose_eval= <span class="number">4</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">                 )</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_lgbm_prob = lgbm.predict(test_x)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_lgbm_01 = np.where(predictions_lgbm_prob &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) </span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">lgb.plot_importance(lgbm, max_num_features=<span class="number">21</span>, importance_type=<span class="string">'split'</span>)</span></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument
  warnings.warn(&quot;Found `{}` in params. Will use it instead of argument&quot;.format(alias))





&lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff7884feb00&gt;</code></pre><p><img src="/images/classification-models/output_69_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">acc_lgbm = accuracy_score(test_y,predictions_lgbm_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Light GBM model:'</span>, acc_lgbm)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y, predictions_lgbm_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_lgbm_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'GBDT'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of Light GBM model: 0.8695652173913043</code></pre><p><img src="/images/classification-models/output_70_1.png" alt="png"></p>
<p><img src="/images/classification-models/output_70_2.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.92      0.83      0.87       113
     class 1       0.82      0.91      0.86        94

   micro avg       0.87      0.87      0.87       207
   macro avg       0.87      0.87      0.87       207
weighted avg       0.88      0.87      0.87       207</code></pre><h1 id="GBDT＋LR"><a href="#GBDT＋LR" class="headerlink" title="GBDT＋LR"></a>GBDT＋LR</h1><p>1.通过GBDT进行特征转换(为了发现有效的特征和特征组合): 判断样本落在哪个叶子节点上,每个叶子节点作为lr的一维特征,通过遍历树，就得到了该样本对应的所有LR特征。构造的新特征向量是取值onehot化的<br>2.转换得到新的特征后，用lr分类  </p>
<p>继续使用上面GBDT模型，得到新的特征</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_pred = lgbm.predict(train_x, pred_leaf=<span class="literal">True</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># y_pred = final_gb.predict(testdmat,pred_leaf=True)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">y_pred.shape</span></pre></td></tr><tr><td class="code"><pre><span class="line">num_leaf = max([max(each) <span class="keyword">for</span> each <span class="keyword">in</span> y_pred])</span></pre></td></tr><tr><td class="code"><pre><span class="line">num_leaf</span></pre></td></tr></table></figure>




<pre><code>17</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'Writing transformed training data'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># num_leaf = grid.best_params_['num_leaves']</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">transformed_training_matrix = np.zeros([len(y_pred), len(y_pred[<span class="number">0</span>]) * num_leaf],</span></pre></td></tr><tr><td class="code"><pre><span class="line">                                       dtype=np.int64)  <span class="comment"># N * num_tress * num_leafs</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(y_pred)):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    temp = np.arange(len(y_pred[<span class="number">0</span>])) * num_leaf + np.array(y_pred[i])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    transformed_training_matrix[i][temp] += <span class="number">1</span></span></pre></td></tr></table></figure>

<pre><code>Writing transformed training data</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_pred = lgbm.predict(test_x, pred_leaf=<span class="literal">True</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Writing transformed testing data'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">transformed_testing_matrix = np.zeros([len(y_pred), len(y_pred[<span class="number">0</span>]) * num_leaf], dtype=np.int64)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(y_pred)):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    temp = np.arange(len(y_pred[<span class="number">0</span>])) * num_leaf + np.array(y_pred[i])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    transformed_testing_matrix[i][temp] += <span class="number">1</span></span></pre></td></tr></table></figure>

<pre><code>Writing transformed testing data</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transformed_testing_matrix.shape,transformed_training_matrix.shape</span></pre></td></tr></table></figure>




<pre><code>((207, 272), (483, 272))</code></pre><p>训练集和测试集新的特征的维度</p>
<p>将GBDT换成XGoost，通过XGboost进行特征转换</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_pred = XGB.apply(train_x.values)</span></pre></td></tr><tr><td class="code"><pre><span class="line">num_leaf = max([max(each) <span class="keyword">for</span> each <span class="keyword">in</span> y_pred])</span></pre></td></tr><tr><td class="code"><pre><span class="line">y_pred[<span class="number">13</span>]</span></pre></td></tr></table></figure>




<pre><code>array([23, 25, 23, 17, 21, 24, 25, 22, 25,  6, 21, 22, 21, 19, 14, 12, 23,
       12, 12, 22,  7, 14, 22, 11, 13, 14, 19, 11, 16, 12, 21, 16],
      dtype=int32)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Writing transformed training data'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># num_leaf = grid.best_params_['num_leaves']</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">transformed_training_matrix = np.zeros([len(y_pred), len(y_pred[<span class="number">0</span>]) * num_leaf],</span></pre></td></tr><tr><td class="code"><pre><span class="line">                                       dtype=np.int64)  <span class="comment"># N * num_tress * num_leafs</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(y_pred)):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    temp = np.arange(len(y_pred[<span class="number">0</span>])) * num_leaf + np.array(y_pred[i])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    transformed_training_matrix[i][temp] += <span class="number">1</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="code"><pre><span class="line">y_pred = XGB.apply(test_x.values)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Writing transformed testing data'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">transformed_testing_matrix = np.zeros([len(y_pred), len(y_pred[<span class="number">0</span>]) * num_leaf], dtype=np.int64)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(y_pred)):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    temp = np.arange(len(y_pred[<span class="number">0</span>])) * num_leaf + np.array(y_pred[i])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    transformed_testing_matrix[i][temp] += <span class="number">1</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">transformed_testing_matrix.shape,transformed_training_matrix.shape</span></pre></td></tr></table></figure>

<pre><code>Writing transformed training data
Writing transformed testing data





((207, 832), (483, 832))</code></pre><p>用新的特征进行lr分类,先做参数选择</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = LogisticRegression()</span></pre></td></tr><tr><td class="code"><pre><span class="line">penalty  = [<span class="string">'l1'</span>,<span class="string">'l2'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">C = [<span class="number">10</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">0.3</span>,<span class="number">0.1</span>,<span class="number">0.03</span>,<span class="number">0.01</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">class_weight = [<span class="string">'balanced'</span>,<span class="literal">None</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">param_gridknn = dict(penalty=penalty,C=C,class_weight=class_weight)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridLR = GridSearchCV(lr, param_gridknn, cv=<span class="number">10</span>, scoring=<span class="string">'roc_auc'</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridLR.fit(transformed_training_matrix,train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'best params are:'</span>,str(gridLR.best_params_))</span></pre></td></tr></table></figure>

<pre><code>Fitting 10 folds for each of 28 candidates, totalling 280 fits


[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.


best params are: {&apos;C&apos;: 0.3, &apos;class_weight&apos;: &apos;balanced&apos;, &apos;penalty&apos;: &apos;l2&apos;}


[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed:    5.3s finished
/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logreg = LogisticRegression(class_weight=<span class="string">'balanced'</span>,C=<span class="number">0.3</span>,penalty=<span class="string">'l2'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">logreg.fit(transformed_training_matrix,train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_prob = logreg.predict_proba(transformed_testing_matrix)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_prob = predictions_LogReg_prob[:,<span class="number">1</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_01 = logreg.predict(transformed_testing_matrix)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_logit = accuracy_score(test_y, predictions_LogReg_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Logistic Regression model:'</span>, acc_logit)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y, predictions_LogReg_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_LogReg_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'XGB_lr'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)


Overall accuracy of Logistic Regression model: 0.8599033816425121</code></pre><p><img src="/images/classification-models/output_83_2.png" alt="png"></p>
<p><img src="/images/classification-models/output_83_3.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.92      0.81      0.86       113
     class 1       0.80      0.91      0.86        94

   micro avg       0.86      0.86      0.86       207
   macro avg       0.86      0.86      0.86       207
weighted avg       0.87      0.86      0.86       207</code></pre><h1 id="结果汇总"><a href="#结果汇总" class="headerlink" title="结果汇总"></a>结果汇总</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_name = RESULT.keys()</span></pre></td></tr><tr><td class="code"><pre><span class="line">model_preformance = RESULT.values()</span></pre></td></tr><tr><td class="code"><pre><span class="line">preformance1 = [<span class="string">'recall'</span>,<span class="string">'precision'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">preformance2 = [<span class="string">'acc'</span>,<span class="string">'f1'</span>,<span class="string">'auc'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">xx = list(range(len(model_name)))</span></pre></td></tr><tr><td class="code"><pre><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">8.0</span>, <span class="number">4.0</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i,each <span class="keyword">in</span> enumerate(preformance1):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.plot(xx,[each[i] <span class="keyword">for</span> each <span class="keyword">in</span> model_preformance])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.title(<span class="string">'model compare'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xlabel(<span class="string">'model name'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.ylabel(<span class="string">'performance'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xticks(xx,model_name)</span></pre></td></tr><tr><td class="code"><pre><span class="line">plt.legend(preformance1)</span></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.legend.Legend at 0x7ff754f29dd8&gt;</code></pre><p><img src="/images/classification-models/output_85_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i,each <span class="keyword">in</span> enumerate(preformance2):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.plot(xx,[each[i] <span class="keyword">for</span> each <span class="keyword">in</span> model_preformance])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.title(<span class="string">'model compare'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xlabel(<span class="string">'model name'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.ylabel(<span class="string">'performance'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xticks(xx,model_name)</span></pre></td></tr><tr><td class="code"><pre><span class="line">plt.legend(preformance2)</span></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.legend.Legend at 0x7ff754f6ecf8&gt;</code></pre><p><img src="/images/classification-models/output_86_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_name = RESULT.keys()</span></pre></td></tr><tr><td class="code"><pre><span class="line">model_preformance = RESULT.values()</span></pre></td></tr><tr><td class="code"><pre><span class="line">preformance1 = [<span class="string">'recall'</span>,<span class="string">'precision'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">preformance2 = [<span class="string">'acc'</span>,<span class="string">'f1'</span>,<span class="string">'auc'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">preformance = preformance1 + preformance2</span></pre></td></tr><tr><td class="code"><pre><span class="line">xx = list(range(len(model_name)))</span></pre></td></tr><tr><td class="code"><pre><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">10.0</span>, <span class="number">6.0</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i,each <span class="keyword">in</span> enumerate(preformance):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.plot(xx,[each[i] <span class="keyword">for</span> each <span class="keyword">in</span> model_preformance])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.title(<span class="string">'model compare'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xlabel(<span class="string">'model name'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.ylabel(<span class="string">'performance'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xticks(xx,model_name)</span></pre></td></tr><tr><td class="code"><pre><span class="line">plt.legend(preformance)</span></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.legend.Legend at 0x7ff754c2b0b8&gt;</code></pre><p><img src="/images/classification-models/output_87_1.png" alt="png"></p>
<p>可以着重展示一下SVM,LR,GBDT,GBDT+LR,XGBoost+LR  ,因为参数比较多,调参过程选择多一些<br>后面三个模型逻辑是这样的 :<br>LR 首先连续变量不做离散化直接拟合,之后选择一些连续变量做离散化再做拟合,发现模型表现没有提升,<br>于是想试试更多特征的表现,<br>于是通过GBDT（以每个叶子节点作为一个特征）进行特征转换，组合得到更多的特征，进行lr分类.<br>最后将GBDT换成ＸＧｂｏｏｓｔ做尝试</p>
<p>再使用GBDT或XGboost特征后，lr相比之前有了较大的改善  </p>
<p>但是相比GDBT本身却下降了,可能因为数据量的问题,出现了过拟合</p>
<p>XGboost，有小幅上升</p>
<p>进行探索，查看GBDT和XGBoost在训练集上的表现，发现GBDT已经完全拟合了训练集，确实存在过拟合现象，而XGBoost过拟合不严重</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions_lgbm_prob = XGB.predict(train_x.values)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_lgbm_01 = np.where(predictions_lgbm_prob &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) </span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_lgbm = accuracy_score(train_y,predictions_lgbm_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Light GBM model:'</span>, acc_lgbm)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(train_y, predictions_lgbm_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(train_y, predictions_lgbm_01)</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of Light GBM model: 0.9420289855072463</code></pre><p><img src="/images/classification-models/output_91_1.png" alt="png"></p>
<p><img src="/images/classification-models/output_91_2.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.95      0.95      0.95       270
     class 1       0.93      0.93      0.93       213

   micro avg       0.94      0.94      0.94       483
   macro avg       0.94      0.94      0.94       483
weighted avg       0.94      0.94      0.94       483</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions_lgbm_prob = lgbm.predict(train_x.values)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_lgbm_01 = np.where(predictions_lgbm_prob &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) </span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_lgbm = accuracy_score(train_y,predictions_lgbm_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Light GBM model:'</span>, acc_lgbm)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(train_y, predictions_lgbm_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(train_y, predictions_lgbm_01)</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of Light GBM model: 0.9979296066252588</code></pre><p><img src="/images/classification-models/output_92_1.png" alt="png"></p>
<p><img src="/images/classification-models/output_92_2.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       1.00      1.00      1.00       270
     class 1       1.00      1.00      1.00       213

   micro avg       1.00      1.00      1.00       483
   macro avg       1.00      1.00      1.00       483
weighted avg       1.00      1.00      1.00       483</code></pre>]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo 博客常用操作命令</title>
    <url>/hexo-command/</url>
    <content><![CDATA[<p>可参考官网详细教程</p>
<pre><code>https://hexo.io/zh-cn/docs/</code></pre><a id="more"></a>

<p>hexo安装以后，可以使用以下两种方式执行 Hexo：  </p>
<ol>
<li><code>npx hexo &lt;command&gt;</code>  </li>
<li>将 Hexo 所在的目录下的 node_modules 添加到环境变量之中即可直接使用 <code>hexo &lt;command&gt;</code>：<br> <code>echo &#39;PATH=&quot;$PATH:./node_modules/.bin&quot;&#39; &gt;&gt; ~/.profile</code></li>
</ol>
<h3 id="new"><a href="#new" class="headerlink" title="new"></a>new</h3><p>新建内容。如果没有设置 layout 的话，默认使用 _config.yml 中的 <code>default_layout</code> 参数代替。如果标题包含空格的话，请使用引号括起来。</p>
<pre><code>$ hexo new [layout] &lt;title&gt;
$ hexo new &quot;post title with whitespace&quot;</code></pre><h3 id="generate"><a href="#generate" class="headerlink" title="generate"></a>generate</h3><p>生成静态文件:</p>
<pre><code>$ hexo generate
$ hexo g</code></pre><h3 id="publish"><a href="#publish" class="headerlink" title="publish"></a>publish</h3><p>草稿发布:</p>
<pre><code>$ hexo publish [layout] &lt;filename&gt;</code></pre><h3 id="server"><a href="#server" class="headerlink" title="server"></a>server</h3><p>开启服务:</p>
<pre><code>$ hexo server
$ hexo s</code></pre><h3 id="deploy"><a href="#deploy" class="headerlink" title="deploy"></a>deploy</h3><p>部署网站:</p>
<pre><code>$ hexo deploy
$ hexo d</code></pre><h3 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h3><p>本地开启服务：</p>
<pre><code>$ npx hexo g &amp; npx hexo s</code></pre><p>上传部署：</p>
<pre><code>$ npx hexo g &amp; npx hexo s</code></pre>]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>NLTK + Stanford NLP 进行命名实体识别和词性标注</title>
    <url>/stanford-nltk/</url>
    <content><![CDATA[<p>NLTK 中使用 Stanford NLP 工具包进行NER和POS任务。</p>
<a id="more"></a>

<h1 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h1><ol>
<li>下载<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;nlp.stanford.edu&#x2F;software&#x2F;CRF-NER.html</span></pre></td></tr><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;nlp.stanford.edu&#x2F;software&#x2F;tagger.html</span></pre></td></tr></table></figure></li>
<li>解压  <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">unzip stanford-ner-2018-10-16.zip</span></pre></td></tr><tr><td class="code"><pre><span class="line">unzip stanford-postagger-full-2018-10-16.zip</span></pre></td></tr></table></figure></li>
<li>添加 <code>CLASSPATH</code> ，修改 <code>.bashrc</code> 文件:<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> STANFORD_NLTK_PATH=/home/haha/stanford_nltk  </span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> STANFORD_NER_PATH=<span class="variable">$STANFORD_NLTK_PATH</span>/stanford-ner-2018-10-16</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> STANFORD_POS_PATH=<span class="variable">$STANFORD_NLTK_PATH</span>/stanford-postagger-full-2018-10-16</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/home/haha/java/jdk-13.0.1  </span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span>  </span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.jar:<span class="variable">$STANFORD_NER_PATH</span>/stanford-ner.jar:<span class="variable">$STANFORD_POS_PATH</span>/stanford-postagger.jar</span></pre></td></tr></table></figure></li>
<li>添加 <code>STANFORD_MODELS</code> ，修改 <code>.bashrc</code> 文件:<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> STANFORD_MODELS=<span class="variable">$STANFORD_NER_PATH</span>/classifiers:<span class="variable">$STANFORD_POS_PATH</span>/models</span></pre></td></tr></table></figure>

</li>
</ol>
<h1 id="函数使用"><a href="#函数使用" class="headerlink" title="函数使用"></a>函数使用</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tag <span class="keyword">import</span> StanfordNERTagger</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tag <span class="keyword">import</span> StanfordPOSTagger</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">pos_tagger = StanfordPOSTagger(<span class="string">'english-bidirectional-distsim.tagger'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">ner_tagger = StanfordNERTagger(<span class="string">'english.all.3class.distsim.crf.ser.gz'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">tokens = [<span class="string">'The'</span>, <span class="string">'suspect'</span>, <span class="string">'dumped'</span>, <span class="string">'the'</span>, <span class="string">'dead'</span>, <span class="string">'body'</span>, <span class="string">'into'</span>, <span class="string">'a'</span>, <span class="string">'local'</span>, <span class="string">'reservoir'</span>, <span class="string">'.'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">pos = [each[<span class="number">1</span>] <span class="keyword">for</span> each <span class="keyword">in</span> pos_tagger.tag(tokens)]</span></pre></td></tr><tr><td class="code"><pre><span class="line">ner = [each[<span class="number">1</span>] <span class="keyword">for</span> each <span class="keyword">in</span> ner_tagger.tag(tokens)]</span></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>stanford</tag>
        <tag>nltk</tag>
        <tag>ner</tag>
        <tag>pos</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试知识点总结</title>
    <url>/ml-overview/</url>
    <content><![CDATA[<p>对面试中机器学习模型常碰到的问题进行总结对比。</p>
<a id="more"></a>

<h2 id="模型汇总"><a href="#模型汇总" class="headerlink" title="模型汇总"></a>模型汇总</h2><ol>
<li><h3 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h3><ol>
<li><h4 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h4> 多叉树：按选择的特征个数分叉;<br> 信息增益：max（A）：I(D,A) = H(D) - H(D|A);<br> 无法处理连续特征;<br> 信息增益倾向于选择特征个数多的特征;<br> 无法处理缺失值;<br> 没考虑过拟合;<br> 不支持缺失值处理;</li>
<li><h4 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h4> 多叉树，连续节点处为二叉树;<br> 信息增益比;<br> 剪枝：预剪枝，后剪枝;<br> 只能用于分类;<br> 支持缺失值处理;</li>
<li><h4 id="cart"><a href="#cart" class="headerlink" title="cart"></a>cart</h4> 二叉树，支持缺失值处理;<br> 分类：gini系数;<br> 回归：均方差;</li>
</ol>
</li>
<li><h3 id="Boost（改变样本分布）"><a href="#Boost（改变样本分布）" class="headerlink" title="Boost（改变样本分布）"></a>Boost（改变样本分布）</h3><ol>
<li><h4 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h4><ul>
<li>分类：<ul>
<li>指数损失函数 + 加法模型 + 前向分步算法</li>
<li>根据当前的学习误差率更新训练样本的权重</li>
<li>样本权重w-&gt;分类器样本误差率e-&gt;分类器权重α-&gt;样本权重w（和前一个总分类器有关）</li>
</ul>
</li>
<li>回归：<ul>
<li>平方损失函数，拟合残差</li>
<li>根据划分区域，搜索使得损失函数最小的取值 </li>
</ul>
</li>
<li>弱学习器不固定；对异常点敏感，样本权重较高；精度高  </li>
</ul>
</li>
<li><h4 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h4><p> 弱学习器固定为cart树;<br> 损失函数不固定;<br> 拟合损失函数的负梯度;<br> 分类：对数似然损失函数（指数损失变为adaboost）;<br> 回归：均方差，绝对损失;<br> 采样是不放回采样; </p>
<pre><code>Q：怎样设置单棵树的停止生长条件？  
A：节点分裂时的最小样本数，最大深度，最多叶子节点数，loss满足约束条件  

Q：评估特征的权重大小  
A：  
1. 通过计算每个特征在训练集下的信息增益，最后计算每个特征信息增益与所有特征信息增益之和的比例为权重值；  
2. 借鉴投票机制。用相同的gbdt参数对w每个特征训练出一个模型，然后在该模型下计算每个特征正确分类的个数，最后计算每个特征正确分类的个数与所有正确分类个数之和的比例为权重值。  

Q：增加样本数量时，训练时长是线性增加吗？  
A：不是。因为生成单棵决策树时，损失函数极小值与样本数量N不是线性相关  ？？？  

Q：增加树的棵树时，训练时长是线性增加吗？  
A：不是。因为每棵树的生成的时间复杂度不一样。   

Q：如何防止过拟合  
A：  
1. 增加样本（data bias or small data的缘故），移除噪声。  
2. 减少特征，保留重要的特征（可以用PCA等对特征进行降维）。  
3. 对样本进行采样（类似bagging）。就是建树的时候，不是把所有的样本都作为输入，而是选择一个子集。  
4. 对特征进行采样。类似样本采样一样,每次建树的时候，只对部分的特征进行切分。  
5. 加正则项，剪枝  

Q：gbdt在训练和预测的时候都用到了步长，这两个步长一样么？  
A：一样。 ？？？  

Q：gbdt中哪些部分可以并行？  
A：  
1. 计算每个样本的负梯度  
2. 分裂挑选最佳特征及其分割点时，对特征计算相应的误差及均值时  
3. 更新每个样本的负梯度时  
4. 最后预测过程中，每个样本将之前的所有树的结果累加的时候  </code></pre></li>
<li><h3 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h3></li>
</ol>
</li>
<li><h3 id="图模型"><a href="#图模型" class="headerlink" title="图模型"></a>图模型</h3><ol>
<li><h4 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h4><ul>
<li>两个假设：<ul>
<li>齐次马尔科夫链假设：<br>隐藏状态之和之前的一个有关</li>
<li>观测独立假设：<br>观测状态之和当前的隐藏状态有关</li>
</ul>
</li>
<li>三个问题：<ul>
<li>已知参数，观测序列，求观测序列概率，前向后向算法</li>
<li>已知观测序列，估计模型参数，EM算法</li>
<li>已知参数，观测序列，推测状态序列，维特比算法</li>
</ul>
</li>
</ul>
</li>
<li><h3 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h3><p> 参数估计，极大似然估计<br> 推测状态序列，维特比算法<br> 特征函数设计</p>
<p>HMM    生成模型，马尔可夫假设<br>CRF 判别模型，没有马尔可夫假设（所以容易更好的采纳上下文信息）</p>
</li>
</ol>
</li>
</ol>
<h2 id="模型对比"><a href="#模型对比" class="headerlink" title="模型对比"></a>模型对比</h2><ol>
<li><h3 id="LR-vs-SVM"><a href="#LR-vs-SVM" class="headerlink" title="LR vs SVM"></a>LR vs SVM</h3><ul>
<li><p>不同</p>
<ul>
<li>LR的损失函数是Cross entropy loss，SVM的损失函数是Hinge loss，自带L2正则</li>
<li>LR的解是受数据本身分布影响的，而SVM的解不受数据分布的影响（支持向量）</li>
<li>LR的输出具有自然的概率意义，而SVM的输出不具有概率意义</li>
<li>SVM依赖数据表达的距离测度，需要对数据normalization，LR则不需要</li>
<li>SVM受惩罚系数C的影响较大，实验中需要做Validation，LR则不需要</li>
<li>LR适合于大样本学习，SVM适合于小样本学习 ？？？</li>
<li>SVM可以处理大型特征空间，LR对此性能不好 ？？？</li>
<li><a href="https://shomy.top/2017/03/09/support-vector-regression/" target="_blank" rel="noopener">SVM 回归</a> </li>
</ul>
</li>
<li><p>相同</p>
<ul>
<li>线性决策</li>
<li>kernel trick，ker-SVM只需要计算支持向量的核函数；而<a href="https://shomy.top/2017/03/07/kernel-lr/" target="_blank" rel="noopener">ker-LR</a>需要所有的样本 ？？？</li>
<li>都会受到outlier的影响</li>
</ul>
<p>只有将最优解w表示为xi的线性组合，才能够利用核函数K<br>如果样本数量小于特征数，那么就没必要选择非线性核，简单的使用线性核就可以了<br>如果样本数量大于特征数目，这时可以使用非线性核，将样本映射到更高维度，一般可以得到更好的结果</p>
<p><a href="https://shomy.top/archives/page/2/" target="_blank" rel="noopener">https://shomy.top/archives/page/2/</a></p>
</li>
</ul>
</li>
<li><h3 id="LR-vs-决策树"><a href="#LR-vs-决策树" class="headerlink" title="LR vs 决策树"></a>LR vs 决策树</h3><ul>
<li>决策树可以处理缺失值，LR不可以</li>
<li>LR线性决策边界，可能欠拟合（但是可以核方法），决策树非线性决策边界，但是对线性拟合效果容易过拟合。（如；x+y=1）（但是可以剪枝，正则，bagging）</li>
<li>LR有概率值解释，决策树有决策过程直观解释</li>
<li>LR对数据整体结构的分析优于决策树，而决策树对局部结构的分析优于LR。</li>
<li>LR对极值比较敏感，容易受极端值的影响</li>
<li>决策树容易过拟合；</li>
<li>GDBT + LR 融合</li>
</ul>
</li>
<li><h3 id="RF-vs-GDBT"><a href="#RF-vs-GDBT" class="headerlink" title="RF vs GDBT"></a>RF vs GDBT</h3><ul>
<li>不同<ul>
<li>组成随机森林的树可以并行生成，而GBDT是串行生成</li>
<li>随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和</li>
<li>随机森林对异常值不敏感，而GBDT对异常值比较敏感</li>
<li>随机森林是通过减少模型的方差来提高性能，而GBDT是减少模型的偏差来提高性能的</li>
<li>随机森林不需要进行数据预处理，即特征归一化。GBDT则需要进行特征归一化 ？？？为什么要归一化</li>
<li>组成随机森林的树可以分类树也可以是回归树，GBDT由cart树组成</li>
</ul>
</li>
<li>相同<ul>
<li>集成算法</li>
</ul>
</li>
</ul>
</li>
<li><h3 id="XgBoost-vs-GBDT"><a href="#XgBoost-vs-GBDT" class="headerlink" title="XgBoost vs GBDT"></a>XgBoost vs GBDT</h3></li>
</ol>
<p><a href="https://zhuanlan.zhihu.com/p/46831267" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46831267</a><br><a href="https://zhuanlan.zhihu.com/p/34679467" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34679467</a></p>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer详解：各个特征维度分析推导</title>
    <url>/transformer/</url>
    <content><![CDATA[<p>谷歌在文章《Attention is all you need》中提出的transformer模型。如图主要架构：同样为encoder-decoder模式，左边部分是encoder，右边部分是decoder。<br>TensorFlow代码：<em><a href="https://www.github.com/kyubyong/transformer" target="_blank" rel="noopener">https://www.github.com/kyubyong/transformer</a></em><br><img src="/images/transformer1.png" alt=""></p>
<a id="more"></a>

<h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><p>用 sentencepiece 进行分词。</p>
<h1 id="Encoder-输入"><a href="#Encoder-输入" class="headerlink" title="Encoder 输入"></a>Encoder 输入</h1><p>初始输入为待翻译语句的embedding矩阵，由于句子长度不一致，需要做统一长度处理，长度取maxlength1，不够长的句子padding 0值，句尾加上 <code>&lt;/s&gt;</code> 。</p>
<pre><code>d = 512, [batchsize，maxlen1，d]</code></pre><p>考虑到词语间的相对位置信息，还要加上语句的position<br>encoding，由函数形式直接求出。</p>
<pre><code>PE(pos,2i) = sin(pos/10002i/d)
PE(pos,2i+1) = cos(pos/10002i/d)</code></pre><p>Padding的值不做position encoding。 <code>[batchsize，maxlen1，d]</code> ，最终:</p>
<pre><code>encoder input = position encoding + input embedding。
encoder input : [batchsize，maxlen1，d]</code></pre><h1 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h1><p>Encoder 由N = 6个相同的layer连接组成。每个layer中有两个sublayer，分别是multihead<br>self-attention以及FFN。</p>
<pre><code>Q = K = V = input
MultiHead(Q, K, V) = concat(head1, …, headh)Wo
headi = Attention(QW­iQ，KW­ik，VW­iV)
Attention(Q, K, V) = softmax(QKT/$$\sqrt{d}$$) V</code></pre><p><img src="/images/transformer1.png" alt=""><br><img src="/images/transformer3.png" alt=""><br><img src="/images/transformer4.png" alt=""></p>
<p>softmax前要做key_mask，把pad 0 的地方赋值为-inf，softmax后权重做query mask，赋值0。</p>
<pre><code>h = 8
W­iQ, W­ik, W­iV : [d, d/h]
Q : [maxlen_q, d]
K = V : [maxlen_k, d]
Maxlen_q = maxlen_k so: Q = K = V : [maxlen1, d]
QW­kQ，KW­ik，VW­iV : [maxlen1, d/h]
headi : [maxlen1, d/h] \* [d/h, maxlen1] \* [maxlen1, d/h] = [maxlen1, d/h]
Wo : [d, d]
MultiHead(Q,K,V): [maxlen, d]</code></pre><p><code>Softmax([maxlen_q, maxlen_k])</code> 在最后一个维度即 <code>maxlen_k</code> 上做 <code>softmax</code>。<br>position-wise是因为处理的attention输出是某一个位置i的attention输出。</p>
<pre><code>FFN(x) = ReLU ( xW1 + b1 ) \* W2 + b2
ReLU(x) = max( 0, x )
dff = 4 \* d = 2048
W1 : [d, dff]
W2 : [dff, d]</code></pre><p>流程：</p>
<pre><code>Input -&gt; dropout -&gt;
(
multihead self-attention -&gt; dropout -&gt; residual connection -&gt; LN -&gt;
FFN-&gt; dropout -&gt; residual connection -&gt; LN -&gt;
) * 6
-&gt; memory [batchsize，maxlen，d]</code></pre><p>代码中在multihead attention中对score做dropout，FFN后没有dropout，但文章说每个sublayer的output都有一个dropout。</p>
<h1 id="Decoder-输入"><a href="#Decoder-输入" class="headerlink" title="Decoder 输入"></a>Decoder 输入</h1><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>目标句子首尾分别加上 <code>&lt;s&gt;</code> , <code>&lt;/s&gt;</code>。</p>
<pre><code>Decoder input = Output embedding + position encoding
Decoder input : [batchsize，maxlen2，d]</code></pre><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>初始向量为<code>&lt;s&gt;</code>对应embedding，之后将前一步的输出拼接到当前的所有预测构成当前的decoder输入。</p>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>Decoder由N = 6 个相同的layer组成，每个layer中有三个sublayer，分别是multihead self-attention, mutihead attention以及FFN。</p>
<pre><code>decoder input -&gt; dropout -&gt;
(
   Masked multihead self-attention(dec, dec, dec) = dec-&gt; dropout -&gt;
   multihead attention(dec, memory, memory) -&gt; dropout -&gt; residual connection
   -&gt; LN -&gt; FFN -&gt; dropout -&gt; residual connection -&gt; LN -&gt;
) * 6
-&gt; dec -&gt; linear -&gt; softmax</code></pre><p>Self-attention 的mask为一个和dec相同维度的上三角全为-inf的矩阵。</p>
<pre><code>Linear( x ) = xW
Dec : [batchsize，maxlen2，d]
W : [d, vocabsize]</code></pre><p>W为词汇表embedding矩阵的转置, 输入输出的词汇表embedding矩阵为W。即三个参数共享。</p>
<pre><code>Linear( x ) : [batchsize，maxlen2，vocabsize]</code></pre><p>Softmax函数：</p>
<center>
$p\left( k\|x \right)=\frac{\exp({{z}_{k}})}{\sum\nolimits_{i=1}^{K}{\exp ({{z}_{i}})}}$
</center>
其中zi一般叫做 logits，即未被归一化的对数概率。

<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>损失函数：cross entropy。用p代表predicted probability，用q代表groundtruth。即：</p>
<center>$cross\_entropy\_loss=\sum\limits_{k=1}^{K}{q\left( k\|x\right)\log (p\left( k\|x \right))}$</center>


<p>groundtruth为one-hot，即每个样本只有惟一的类别，$q(k)={{\delta}_{k,y}}$，y是真实类别。</p>
<center>${{\delta }_{k,y}}\text{=}\left\{\begin{matrix} 1,k=y \\0,k\ne y \\\end{matrix} \right.$</center>


<p>对目标句子onehot 做labelmsmooth用$\tilde{q}(k|x)$代替$q(k|x)$。（为了正则化，防止过拟合）</p>
<center>$\tilde{q}(k\|x)=(1-\varepsilon ){{\delta }_{k,y}}+\varepsilon u(k)$</center>


<p>可以理解为，对于$q(k)={{\delta}_{k,y}}$函数分布的真实标签，将它变成以如下方式获得：首先从标注的真实标签的$\delta$分布中取定，然后以一定的概率$\varepsilon$，将其替换为在$u(k)$分布中的随机变量。$u(k)$为均匀分布，即$u(k)=1/K$</p>
<h1 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h1><p>Adam优化器：<br><img src="/images/transformer5.png" alt=""><br>学习率使用warm up learning rate:</p>
<pre><code>learningrate = dmodel-0.5 \* min ( step_num-0.5, step_num \* warmup_steps-1.5 )
warmup_steps ：4000</code></pre>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>transoformer</tag>
      </tags>
  </entry>
  <entry>
    <title>MarkDown 使用细节</title>
    <url>/markdown/</url>
    <content><![CDATA[<p>记录使用遇到的一些常用的MarkDown命令</p>
<a id="more"></a>

<p>行内标签</p>
<pre><code>`&lt;你的 GitHub 用户名&gt;.github.io`</code></pre><p>文字居中</p>
<pre><code>&lt;center&gt;文字    &lt;/center&gt;</code></pre><p>代码块插入</p>
<pre><code><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> *</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">''</span>)</span></pre></td></tr></table></figure></code></pre>]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>常用句法分析工具包使用说明：Hanlp、StanfordNLP等</title>
    <url>/parser/</url>
    <content><![CDATA[<p>对比各个常用的自然语言处理工具包中的句法分析模块。</p>
<a id="more"></a>

<h1 id="Hanlp"><a href="#Hanlp" class="headerlink" title="Hanlp"></a>Hanlp</h1><p>pip install pyhanlp 安装即可<br>项目地址：<a href="https://github.com/hankcs/pyhanlp" target="_blank" rel="noopener">https://github.com/hankcs/pyhanlp</a></p>
<p><strong>基于神经网络的高性能依存句法分析器</strong></p>
<p>输出为CONLL格式中，每个词语占一行，无值列用下划线代替，列的分隔符为制表符 <code>&#39;\t&#39;</code> ，行的分隔符为换行符 <code>&#39;\n&#39;</code>；句子与句子之间用空行分隔。<br>CONLL标注格式包含10列，分别为：  </p>
<table>
<thead>
<tr>
<th align="center">ID</th>
<th align="center">FORM</th>
<th align="center">LEMMA</th>
<th align="center">CPOSTAG</th>
<th align="center">POSTAG</th>
<th align="center">FEATS</th>
<th align="center">HEAD</th>
<th align="center">DEPREL</th>
<th align="center">PHEAD</th>
<th align="center">PDEPREL</th>
</tr>
</thead>
</table>
<p>只用到前８列，其含义分别为：  </p>
<table>
<thead>
<tr>
<th align="center">id</th>
<th align="center">name</th>
<th align="center">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="center">ID</td>
<td align="center">当前词在句子中的序号，１开始.</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">FORM</td>
<td align="center">当前词语或标点</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">LEMMA</td>
<td align="center">当前词语（或标点）的原型或词干，在中文中，此列与FORM相同</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">CPOSTAG</td>
<td align="center">当前词语的词性（粗粒度）</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">POSTAG</td>
<td align="center">当前词语的词性（细粒度）</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">FEATS</td>
<td align="center">句法特征，在本次评测中，此列未被使用，全部以下划线代替。</td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">HEAD</td>
<td align="center">当前词语的中心词</td>
</tr>
<tr>
<td align="center">8</td>
<td align="center">DEPREL</td>
<td align="center">当前词语与中心词的依存关系</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyhanlp <span class="keyword">import</span> *</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(HanLP.parseDependency(<span class="string">"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"</span>))</span></pre></td></tr></table></figure>

<pre><code>1    徐先生    徐先生    nh    nr    _    4    主谓关系    _    _
2    还    还    d    d    _    4    状中结构    _    _
3    具体    具体    a    ad    _    4    状中结构    _    _
4    帮助    帮助    v    v    _    0    核心关系    _    _
5    他    他    r    r    _    4    兼语    _    _
6    确定    确定    v    v    _    4    动宾关系    _    _
7    了    了    u    u    _    6    右附加关系    _    _
8    把    把    p    p    _    15    状中结构    _    _
9    画    画    v    v    _    8    介宾关系    _    _
10    雄鹰    雄鹰    n    n    _    9    动宾关系    _    _
11    、    、    wp    w    _    12    标点符号    _    _
12    松鼠    松鼠    n    n    _    10    并列关系    _    _
13    和    和    c    c    _    14    左附加关系    _    _
14    麻雀    麻雀    n    n    _    10    并列关系    _    _
15    作为    作为    v    v    _    6    动宾关系    _    _
16    主攻    主攻    v    vn    _    17    定中关系    _    _
17    目标    目标    n    n    _    15    动宾关系    _    _
18    。    。    wp    w    _    4    标点符号    _    _</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sentence = HanLP.parseDependency(<span class="string">"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> sentence.iterator():  <span class="comment"># 通过dir()可以查看sentence的方法</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(<span class="string">"%s --(%s)--&gt; %s"</span> % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))</span></pre></td></tr></table></figure>

<pre><code>徐先生 --(主谓关系)--&gt; 帮助
还 --(状中结构)--&gt; 帮助
具体 --(状中结构)--&gt; 帮助
帮助 --(核心关系)--&gt; ##核心##
他 --(兼语)--&gt; 帮助
确定 --(动宾关系)--&gt; 帮助
了 --(右附加关系)--&gt; 确定
把 --(状中结构)--&gt; 作为
画 --(介宾关系)--&gt; 把
雄鹰 --(动宾关系)--&gt; 画
、 --(标点符号)--&gt; 松鼠
松鼠 --(并列关系)--&gt; 雄鹰
和 --(左附加关系)--&gt; 麻雀
麻雀 --(并列关系)--&gt; 雄鹰
作为 --(动宾关系)--&gt; 确定
主攻 --(定中关系)--&gt; 目标
目标 --(动宾关系)--&gt; 作为
。 --(标点符号)--&gt; 帮助</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(dir(sentence))</span></pre></td></tr></table></figure>

<pre><code>[&apos;__class__&apos;, &apos;__delattr__&apos;, &apos;__dict__&apos;, &apos;__dir__&apos;, &apos;__doc__&apos;, &apos;__eq__&apos;, &apos;__format__&apos;, &apos;__ge__&apos;, &apos;__getattribute__&apos;, &apos;__gt__&apos;, &apos;__hash__&apos;, &apos;__init__&apos;, &apos;__init_subclass__&apos;, &apos;__javaclass__&apos;, &apos;__javaobject__&apos;, &apos;__le__&apos;, &apos;__lt__&apos;, &apos;__metaclass__&apos;, &apos;__module__&apos;, &apos;__ne__&apos;, &apos;__new__&apos;, &apos;__reduce__&apos;, &apos;__reduce_ex__&apos;, &apos;__repr__&apos;, &apos;__setattr__&apos;, &apos;__sizeof__&apos;, &apos;__str__&apos;, &apos;__subclasshook__&apos;, &apos;__weakref__&apos;, &apos;edgeArray&apos;, &apos;equals&apos;, &apos;findChildren&apos;, &apos;forEach&apos;, &apos;getClass&apos;, &apos;getEdgeArray&apos;, &apos;getWordArray&apos;, &apos;getWordArrayWithRoot&apos;, &apos;hashCode&apos;, &apos;iterator&apos;, &apos;notify&apos;, &apos;notifyAll&apos;, &apos;spliterator&apos;, &apos;toString&apos;, &apos;wait&apos;, &apos;word&apos;, &apos;wordArray&apos;, &apos;wordArrayWithRoot&apos;]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_array = sentence.getWordArray()</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># print(word_array[0])</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> word_array:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(<span class="string">"%s --(%s)--&gt; %s"</span> % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))</span></pre></td></tr></table></figure>

<pre><code>徐先生 --(主谓关系)--&gt; 帮助
还 --(状中结构)--&gt; 帮助
具体 --(状中结构)--&gt; 帮助
帮助 --(核心关系)--&gt; ##核心##
他 --(兼语)--&gt; 帮助
确定 --(动宾关系)--&gt; 帮助
了 --(右附加关系)--&gt; 确定
把 --(状中结构)--&gt; 作为
画 --(介宾关系)--&gt; 把
雄鹰 --(动宾关系)--&gt; 画
、 --(标点符号)--&gt; 松鼠
松鼠 --(并列关系)--&gt; 雄鹰
和 --(左附加关系)--&gt; 麻雀
麻雀 --(并列关系)--&gt; 雄鹰
作为 --(动宾关系)--&gt; 确定
主攻 --(定中关系)--&gt; 目标
目标 --(动宾关系)--&gt; 作为
。 --(标点符号)--&gt; 帮助</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 还可以直接遍历子树，从某棵子树的某个节点一路遍历到虚根</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">CoNLLWord = JClass(<span class="string">"com.hankcs.hanlp.corpus.dependency.CoNll.CoNLLWord"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">head = word_array[<span class="number">15</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> head.HEAD:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    head = head.HEAD</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">if</span> (head == CoNLLWord.ROOT):</span></pre></td></tr><tr><td class="code"><pre><span class="line">        print(head.LEMMA)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">else</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        print(<span class="string">"%s --(%s)--&gt; "</span> % (head.LEMMA, head.DEPREL))</span></pre></td></tr></table></figure>

<pre><code>目标 --(动宾关系)--&gt; 
作为 --(动宾关系)--&gt; 
确定 --(动宾关系)--&gt; 
帮助 --(核心关系)--&gt; 
##核心##</code></pre><h1 id="StanfordNLP"><a href="#StanfordNLP" class="headerlink" title="StanfordNLP"></a>StanfordNLP</h1><p>pip install stanfordnlp 安装即可<br>项目地址：<a href="https://github.com/stanfordnlp/stanfordnlp" target="_blank" rel="noopener">https://github.com/stanfordnlp/stanfordnlp</a><br>依存句法关系符号解释：<a href="https://www.cnblogs.com/sherry-yang/p/9061341.html" target="_blank" rel="noopener">https://www.cnblogs.com/sherry-yang/p/9061341.html</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> stanfordnlp</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nlp = stanfordnlp.Pipeline(lang=<span class="string">'zh'</span>)</span></pre></td></tr></table></figure>

<pre><code>Use device: gpu
---
Loading: tokenize
With settings: 
{&apos;model_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd_tokenizer.pt&apos;, &apos;lang&apos;: &apos;zh&apos;, &apos;shorthand&apos;: &apos;zh_gsd&apos;, &apos;mode&apos;: &apos;predict&apos;}
---
Loading: pos
With settings: 
{&apos;model_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd_tagger.pt&apos;, &apos;pretrain_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt&apos;, &apos;lang&apos;: &apos;zh&apos;, &apos;shorthand&apos;: &apos;zh_gsd&apos;, &apos;mode&apos;: &apos;predict&apos;}
---
Loading: lemma
With settings: 
{&apos;model_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd_lemmatizer.pt&apos;, &apos;lang&apos;: &apos;zh&apos;, &apos;shorthand&apos;: &apos;zh_gsd&apos;, &apos;mode&apos;: &apos;predict&apos;}
Building an attentional Seq2Seq model...
Using a Bi-LSTM encoder
Using soft attention for LSTM.
Finetune all embeddings.
[Running seq2seq lemmatizer with edit classifier]
---
Loading: depparse
With settings: 
{&apos;model_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd_parser.pt&apos;, &apos;pretrain_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt&apos;, &apos;lang&apos;: &apos;zh&apos;, &apos;shorthand&apos;: &apos;zh_gsd&apos;, &apos;mode&apos;: &apos;predict&apos;}
Done loading processors!
---</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">doc = nlp(<span class="string">"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">doc.sentences[<span class="number">0</span>].print_dependencies()</span></pre></td></tr></table></figure>

<pre><code>(&apos;徐&apos;, &apos;2&apos;, &apos;nmod&apos;)
(&apos;先生&apos;, &apos;4&apos;, &apos;nsubj&apos;)
(&apos;还&apos;, &apos;4&apos;, &apos;mark&apos;)
(&apos;具体&apos;, &apos;0&apos;, &apos;root&apos;)
(&apos;帮助&apos;, &apos;4&apos;, &apos;obj&apos;)
(&apos;他&apos;, &apos;7&apos;, &apos;nsubj&apos;)
(&apos;确定&apos;, &apos;4&apos;, &apos;ccomp&apos;)
(&apos;了&apos;, &apos;7&apos;, &apos;case:aspect&apos;)
(&apos;把&apos;, &apos;15&apos;, &apos;aux:caus&apos;)
(&apos;画雄鹰&apos;, &apos;15&apos;, &apos;obj&apos;)
(&apos;、&apos;, &apos;12&apos;, &apos;punct&apos;)
(&apos;松鼠&apos;, &apos;10&apos;, &apos;conj&apos;)
(&apos;和&apos;, &apos;14&apos;, &apos;cc&apos;)
(&apos;麻雀&apos;, &apos;10&apos;, &apos;conj&apos;)
(&apos;作&apos;, &apos;7&apos;, &apos;ccomp&apos;)
(&apos;为&apos;, &apos;15&apos;, &apos;mark&apos;)
(&apos;主攻&apos;, &apos;18&apos;, &apos;nmod&apos;)
(&apos;目标&apos;, &apos;16&apos;, &apos;obj&apos;)
(&apos;。&apos;, &apos;4&apos;, &apos;punct&apos;)</code></pre><h1 id="HIT-LTP"><a href="#HIT-LTP" class="headerlink" title="HIT LTP"></a>HIT LTP</h1><p>项目地址：  </p>
<ul>
<li><a href="https://github.com/HIT-SCIR/pyltp" target="_blank" rel="noopener">https://github.com/HIT-SCIR/pyltp</a>  </li>
<li><a href="https://pyltp.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener">https://pyltp.readthedocs.io/zh_CN/latest/</a>  </li>
</ul>
<p>安装步骤：</p>
<ul>
<li>pip install pyltp</li>
<li>下载模型文件：<a href="http://ltp.ai/download.html" target="_blank" rel="noopener">七牛云</a>，当前模型版本 3.4.0</li>
</ul>
<p>输出：</p>
<ul>
<li>arc.head 表示依存弧的父节点词的索引。ROOT节点的索引是0，第一个词开始的索引依次为1、2、3…  </li>
<li>arc.relation 表示依存弧的关系。  </li>
</ul>
<p>标注集请参考: <a href="https://ltp.readthedocs.io/zh_CN/latest/appendix.html#id5" target="_blank" rel="noopener">https://ltp.readthedocs.io/zh_CN/latest/appendix.html#id5</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyltp <span class="keyword">import</span> Parser</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyltp <span class="keyword">import</span> Segmentor</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyltp <span class="keyword">import</span> Postagger</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dependency_parser</span><span class="params">(sentences)</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    output = []</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        parser = Parser()</span></pre></td></tr><tr><td class="code"><pre><span class="line">        parser.load(<span class="string">'./ltp_data_v3.4.0/parser.model'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        segmentor = Segmentor() </span></pre></td></tr><tr><td class="code"><pre><span class="line">        segmentor.load(<span class="string">'./ltp_data_v3.4.0/cws.model'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        postagger = Postagger() </span></pre></td></tr><tr><td class="code"><pre><span class="line">        postagger.load(<span class="string">'./ltp_data_v3.4.0/pos.model'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">        words = segmentor.segment(sentence)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        postags = postagger.postag(words)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        arcs = parser.parse(words, postags)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        output.append(&#123;<span class="string">'words'</span>:words,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                       <span class="string">'postags'</span>:postags,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                       <span class="string">'arcs'</span>:arcs</span></pre></td></tr><tr><td class="code"><pre><span class="line">                      &#125;)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#         print(' '.join(output[0]['words']))</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#         print(' '.join(output[0]['postags']))</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    segmentor.release() </span></pre></td></tr><tr><td class="code"><pre><span class="line">    postagger.release()</span></pre></td></tr><tr><td class="code"><pre><span class="line">    parser.release()</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">return</span> output</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sentences = [<span class="string">'徐先生还具体帮助他确定了把画雄鹰，松鼠和麻雀作为主攻目标。'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">output = dependency_parser(sentences)</span></pre></td></tr><tr><td class="code"><pre><span class="line">Arcs = [each[<span class="string">'arcs'</span>] <span class="keyword">for</span> each <span class="keyword">in</span> output]</span></pre></td></tr><tr><td class="code"><pre><span class="line">[<span class="string">" "</span>.join(<span class="string">"%d:%s"</span> % (arc.head, arc.relation) <span class="keyword">for</span> arc <span class="keyword">in</span> arcs) <span class="keyword">for</span> arcs <span class="keyword">in</span> Arcs]</span></pre></td></tr></table></figure>




<pre><code>[&apos;2:ATT 5:SBV 4:ADV 5:ADV 0:HED 5:DBL 5:VOB 7:RAD 10:ADV 7:VOB 10:VOB 5:WP 16:SBV 15:LAD 13:COO 5:COO 18:ATT 16:VOB 5:WP&apos;]</code></pre><h1 id="FudanNLP-FNLP"><a href="#FudanNLP-FNLP" class="headerlink" title="FudanNLP (FNLP)"></a>FudanNLP (FNLP)</h1><p><a href="https://github.com/FudanNLP/fnlp" target="_blank" rel="noopener">https://github.com/FudanNLP/fnlp</a><br>java 接口，且不再更新，现在已经推出FastNLP</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>parser</tag>
      </tags>
  </entry>
  <entry>
    <title>SQuAD2.0 刷榜top3模型分析</title>
    <url>/SQuAD2-0/</url>
    <content><![CDATA[<p>由于<a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" rel="noopener">SQuAD2.0榜单</a>一直在更新，所以top3模型也在更新。</p>
<a id="more"></a>
<h2 id="top1-BERT-DAE-AoA"><a href="#top1-BERT-DAE-AoA" class="headerlink" title="top1: BERT + DAE + AoA"></a>top1: BERT + DAE + AoA</h2><ul>
<li>AoA: attention over attention [1]</li>
<li>DAE: DA Enhanced<ul>
<li>Data Augmentation</li>
<li>Domain Adaptation<br><img src="/images/aoa.png" alt="AoA"></li>
</ul>
</li>
</ul>
<p><a href="https://www.infoq.cn/article/M7NpCAAMrPzRo-RViOKs" target="_blank" rel="noopener">https://www.infoq.cn/article/M7NpCAAMrPzRo-RViOKs</a><br><a href="https://zhuanlan.zhihu.com/p/27361305" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27361305</a></p>
<pre><code>[1] Cui Y, Chen Z, Wei S, et al. Attention-over-attention neural networks for reading comprehension[J]. arXiv preprint arXiv:1607.04423, 2016.</code></pre><h2 id="top2-BERT-ConvLSTM-MTL-Verifier"><a href="#top2-BERT-ConvLSTM-MTL-Verifier" class="headerlink" title="top2: BERT + ConvLSTM + MTL + Verifier"></a>top2: BERT + ConvLSTM + MTL + Verifier</h2><ul>
<li>MTL: 多任务学习<ul>
<li><del>预测一个问题是否可答</del></li>
<li><del>预测该问题在篇章中的答案</del></li>
</ul>
</li>
<li>Verifier: 验证器 [1]</li>
<li>convLSTM [2]</li>
</ul>
<p><a href="https://msd.misuland.com/pd/12136984602514128" target="_blank" rel="noopener">https://msd.misuland.com/pd/12136984602514128</a><br><a href="https://blog.csdn.net/maka_uir/article/details/83650978" target="_blank" rel="noopener">https://blog.csdn.net/maka_uir/article/details/83650978</a></p>
<pre><code>[1] Hu M, Peng Y, Huang Z, et al. Read+ verify: Machine reading comprehension with unanswerable questions[J]. arXiv preprint arXiv:1808.05759, 2018.
[2] Shi X , Chen Z , Wang H , et al. Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting[J]. 2015.</code></pre><h2 id="top3-BERT-N-Gram-Masking-Synthetic-Self-Training"><a href="#top3-BERT-N-Gram-Masking-Synthetic-Self-Training" class="headerlink" title="top3: BERT + N-Gram Masking + Synthetic Self-Training"></a>top3: BERT + N-Gram Masking + Synthetic Self-Training</h2><ul>
<li>N-Gram Masking: 类似百度的ERNIE模型</li>
<li>Synthetic Self-Training: BERT官方PPT  </li>
</ul>
<p>(这个方法全部在预训练上做改进，没有对bert上层模型做什么改进)<br>Unclear if adding things on top of BERT really helps by very much.  </p>
<p><a href="https://zhuanlan.zhihu.com/p/63126803" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/63126803</a><br><a href="https://nlp.stanford.edu/seminar/details/jdevlin.pdf" target="_blank" rel="noopener">https://nlp.stanford.edu/seminar/details/jdevlin.pdf</a></p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p><a href="http://web.stanford.edu/class/cs224n/posters/15845024.pdf" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/posters/15845024.pdf</a></p>
<pre><code>[1] Ensemble BERT with Data Augmentation and Linguistic Knowledge on SQuAD2.0</code></pre>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>SQuAD2.0</tag>
      </tags>
  </entry>
  <entry>
    <title>GitHub Pages + Hexo + Next：博客搭建、功能设置及美化</title>
    <url>/next/</url>
    <content><![CDATA[<p>使用Hexo博客框架搭建自己的个人博客，并部署到个人的GitHub上，选用NexT主题，添加一些使用小功能并进行界面的美化。</p>
<a id="more"></a>

<h1 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h1><p>Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。</p>
<h2 id="搭建博客框架"><a href="#搭建博客框架" class="headerlink" title="搭建博客框架"></a>搭建博客框架</h2><p>搭建流程参照官网中文说明文档：</p>
<pre><code>https://hexo.io/zh-cn/docs/</code></pre><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>将搭建在本地的博客通过GitHub Pages部署在个人的GitHub中。从而能通过 <code>&lt;你的 GitHub 用户名&gt;.github.io</code> 域名访问博客</p>
<p>参考教程：</p>
<pre><code>https://www.jianshu.com/p/05289a4bc8b2
https://www.cnblogs.com/jackyroc/p/7681938.html</code></pre><h1 id="Next-主题"><a href="#Next-主题" class="headerlink" title="Next 主题"></a>Next 主题</h1><p>部署完成后，即可对博客做一些细节上的优化，增添小工具以及美化界面</p>
<p>参考教程：</p>
<pre><code>https://zhuanlan.zhihu.com/p/30836436
https://io-oi.me/tech/hexo-next-optimization/</code></pre><h2 id="字体调节"><a href="#字体调节" class="headerlink" title="字体调节"></a>字体调节</h2><p>Next主题默认字体为 <code>font-size-medium = 1em</code> ，有点大。通常来讲，Next主题控制字体大小的文件是在主题文件夹中的 <code>source\css_variables</code> 目录下的 <code>base.styl</code> 文件中，修改如下文件 ：</p>
<pre><code>// Font size
$font-size-base           = (hexo-config(&apos;font.enable&apos;) and hexo-config(&apos;font.global.size&apos;) is a &apos;unit&apos;) ? unit(hexo-config(&apos;font.global.size&apos;), em) : 1em;
$font-size-smallest       = .75em;
$font-size-smaller        = .8125em;
$font-size-small          = .875em;
$font-size-medium         = 1em;
$font-size-large          = 1.125em;
$font-size-larger         = 1.25em;
$font-size-largest        = 1.375em;</code></pre><p>把 <code>1em</code> 改为 <code>0.875em</code> 即可:</p>
<pre><code>// Font size
$font-size-base           = (hexo-config(&apos;font.enable&apos;) and hexo-config(&apos;font.global.size&apos;) is a &apos;unit&apos;) ? unit(hexo-config(&apos;font.global.size&apos;), em) : .875em;
$font-size-smallest       = .75em;
$font-size-smaller        = .8125em;
$font-size-small          = .875em;
$font-size-medium         = 1em;
$font-size-large          = 1.125em;
$font-size-larger         = 1.25em;
$font-size-largest        = 1.375em;</code></pre><h2 id="插入本地图片"><a href="#插入本地图片" class="headerlink" title="插入本地图片"></a>插入本地图片</h2><p>资源（Asset）代表 source 文件夹中除了文章以外的所有文件，例如图片、CSS、JS 文件等。比方说，如果你的Hexo项目中只有少量图片，那最简单的方法就是将它们放在 <code>source/images</code> 文件夹中。然后通过类似于 <code>![](/images/image.jpg)</code> 的方法访问它们。</p>
<h2 id="添加❤脚注"><a href="#添加❤脚注" class="headerlink" title="添加❤脚注"></a>添加❤脚注</h2><p>更改主题配置文件：</p>
<pre><code>icon:
  # Icon name in Font Awesome. See: https://fontawesome.com/v4.7.0/icons/
  # `heart` is recommended with animation in red (#ff0000).
  name: heart #user
  # If you want to animate the icon, set it to true.
  animated: true
  # Change the color of icon, using Hex Code.
  # color: &quot;#808080&quot;
  color: &quot;#ff0000&quot;</code></pre><h2 id="搜索引擎优化"><a href="#搜索引擎优化" class="headerlink" title="搜索引擎优化"></a>搜索引擎优化</h2><h3 id="标题优化"><a href="#标题优化" class="headerlink" title="标题优化"></a>标题优化</h3><p>给标题增加详细信息,更改 <code>index.swig</code> 文件 <code>your-hexo-site\themes\next\layout</code>: </p>
<pre><code>{% block title %}{{ title }}{%- if theme.index_with_subtitle and subtitle %} - {{ subtitle }}{%- endif %}{% endblock %}

	{% block title %}{{ title }}{%- if theme.index_with_subtitle and subtitle %} - {{ subtitle }}{%- endif %} - {{ theme.keywords }} - {{ config.title }}{{ theme.description }}{% endblock %}</code></pre><h3 id="修改链接"><a href="#修改链接" class="headerlink" title="修改链接"></a>修改链接</h3><p>HEXO默认的文章链接形式为 <code>domain/year/month/day/postname</code> ，默认就是一个四级url，并且可能造成url过长，对搜索引擎是十分不友好的，我们可以改成 <code>domain/postname</code> 的形式。编辑站点 <code>_config.yml</code> 文件，修改其中的 <code>permalink</code> 字段改为 <code>permalink: :title.html</code> 即可。</p>
<pre><code>permalink: :year/:month/:day/:title/
permalink: :title/</code></pre><p>参考:</p>
<pre><code>http://www.ehcoo.com/seo.html</code></pre><h3 id="SOE配置"><a href="#SOE配置" class="headerlink" title="SOE配置"></a>SOE配置</h3><p>参考：</p>
<pre><code>https://www.jianshu.com/p/86557c34b671
https://blog.junyu.io/posts/0008-blog-seo.html</code></pre><h2 id="首页阅读全文设置"><a href="#首页阅读全文设置" class="headerlink" title="首页阅读全文设置"></a>首页阅读全文设置</h2><ol>
<li>在文章中手动加入 <code>&lt;!--more--&gt;</code> 进行截断</li>
<li>通过在配置文件中加入代码，自动截断（但实验失败）。</li>
</ol>
<h2 id="字数统计及阅读时长"><a href="#字数统计及阅读时长" class="headerlink" title="字数统计及阅读时长"></a>字数统计及阅读时长</h2><p>安装 <code>hexo-wordcount</code> 失败后采用下面方法：</p>
<pre><code>npm install hexo-symbols-count-time --save</code></pre><p>在配置文件加入设置：</p>
<pre><code>symbols_count_time:  
  symbols: true  
  time: true  
  total_symbols: true  
  total_time: true</code></pre><h2 id="阅读次数统计"><a href="#阅读次数统计" class="headerlink" title="阅读次数统计"></a>阅读次数统计</h2><p>主题配置文件设置：</p>
<pre><code>busuanzi_count:
  enable: true
  total_visitors: true
  total_visitors_icon: user
  total_views: true
  total_views_icon: eye
  post_views: true
  post_views_icon: eye</code></pre><h2 id="评论功能"><a href="#评论功能" class="headerlink" title="评论功能"></a>评论功能</h2><p>Next主题已经集成了GitTalk,直接使用即可。首先，注册GitHub OAuth Apps，生成<code>client_id</code> 和 <code>client_secret</code> 。再修改主题配置文件：</p>
<pre><code>gitalk:
  enable: true
  github_id: troublemeeter # GitHub repo owner
  repo: troublemeeter.github.io # Repository name to store issues
  client_id: # GitHub Application Client ID
  client_secret: # GitHub Application Client Secret
  admin_user: troublemeeter # GitHub repo owner and collaborators, only these guys can initialize gitHub issues
  distraction_free_mode: true # Facebook-like distraction free mode
  # Gitalk&apos;s display language depends on user&apos;s browser or system environment
  # If you want everyone visiting your site to see a uniform language, you can set a force language value
  # Available values: en | es-ES | fr | ru | zh-CN | zh-TW
  language:</code></pre><p>指定页面不添加评论功能，在文章头部设置 <code>comments: false</code>：</p>
<pre><code>---
title: 标签
date: 2019-12-10 00:21:09
type: &quot;tags&quot;
comments: false
---</code></pre><p>参考教程：</p>
<pre><code>https://mrluyc.github.io/2019/07/30/HexoNexT%E9%9B%86%E6%88%90Gitalk/</code></pre><h2 id="数学公式支持"><a href="#数学公式支持" class="headerlink" title="数学公式支持"></a>数学公式支持</h2><p>安装 <code>hexo-math</code>：</p>
<pre><code>$ npm install hexo-math --save</code></pre><p>在配置文件加入设置：</p>
<pre><code>math:
  engine: &apos;mathjax&apos; # or &apos;katex&apos;
  mathjax:
    # src: custom_mathjax_source
    config:
      # MathJax config</code></pre><p>在Next主题配置文件更改设置为：</p>
<pre><code># Math Formulas Render Support
math:
  # Default (true) will load mathjax / katex script on demand.
  # That is it only render those page which has `mathjax: true` in Front-matter.
  # If you set it to false, it will load mathjax / katex srcipt EVERY PAGE.
  per_page: true
  # hexo-renderer-pandoc (or hexo-renderer-kramed) required for full MathJax support.
  mathjax:
    enable: true
    # See: https://mhchem.github.io/MathJax-mhchem/
    mhchem: false</code></pre><p>在需要加载mathjax的文件的头部加入<code>mathjax: true</code>：</p>
<pre><code>---
title: transformer
date: 2019-12-10 17:50:42
tags:
mathjax: true
---</code></pre><h2 id="代码高亮"><a href="#代码高亮" class="headerlink" title="代码高亮"></a>代码高亮</h2><p>不喜欢五颜六色，所以暂时未处理。</p>
<h2 id="迁移"><a href="#迁移" class="headerlink" title="迁移"></a>迁移</h2><pre><code>https://blog.csdn.net/White_Idiot/article/details/80685990</code></pre><p>迁移完后，相关命令：新建Markdown文章，编辑文章</p>
<pre><code>hexo new &quot;post-name&quot;</code></pre><p>将相关更改推送到hexo分支</p>
<pre><code>git add
git commit -m &quot;...&quot;
git push origin hexo</code></pre><p>将静态文件推送到master分支</p>
<pre><code>hexo clean # 如果配置文件没有更改，忽略该命令
hexo g -d</code></pre><h2 id="打赏功能"><a href="#打赏功能" class="headerlink" title="打赏功能"></a>打赏功能</h2><p>制作好微信收款码和支付宝收款码，保存至<code>themes/next/source/images</code>。并修改主题配置文件如下：</p>
<pre><code># Reward (Donate)
# Front-matter variable (unsupport animation).
reward_settings:
  # If true, reward will be displayed in every article by default.
  enable: true
  animation: true
  comment: 🤣~疯狂暗示~🤣</code></pre><p>效果如下，不信试一试~</p>
]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
</search>
