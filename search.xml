<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>sublime中配置sqltools连接数据库</title>
    <url>/sublime-sql/</url>
    <content><![CDATA[<p>利用sublime插件sqltools，实现sublime连接数据库，在sublime中优雅的写SQL。</p>
<a id="more"></a>

<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>在package control中安装SQLTOOLS</p>
<h1 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h1><p>配置connection设置文件：首选项 - Package Settings - SQLTools - Connecttions:</p>
<pre><code>{
    &quot;connections&quot;: {
        &quot;my_local&quot;: {
            &quot;type&quot;    : &quot;mysql&quot;,
            &quot;host&quot;    : &quot;127.0.0.1&quot;,
            &quot;port&quot;    : &quot;3306&quot;,
            &quot;username&quot;: &quot;root&quot;,
            &quot;password&quot;: &quot;0823&quot;,
            &quot;database&quot;: &quot;myemployees&quot;,
            &quot;encoding&quot;: &quot;utf-8&quot;
        },
    },
    &quot;default&quot;: &quot;my_local&quot;
}</code></pre><p>配置setting设置文件：首选项 - Package Settings - SQLTools - Settings:</p>
<pre><code>{
    &quot;cli&quot;:
    {
        &quot;mysql&quot;: &quot;D:/Program/MySQL/bin/mysql.exe&quot;
    }
}</code></pre><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><ol>
<li><p>快捷键<code>ctrl+alt+e</code>进行数据库选择</p>
</li>
<li><p>选择数据库后，键入sql语句，光标定位在sql语句上，快捷键<code>ctrl+e+e</code>，运行语句</p>
</li>
</ol>
<h1 id="美化"><a href="#美化" class="headerlink" title="美化"></a>美化</h1><p>安装SqlBeautifier插件美化SQL语句，命令：<code>ctrl+k</code> + <code>ctrl+f</code> </p>
]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>SQL学习教程</title>
    <url>/learn-sql/</url>
    <content><![CDATA[<p><em>来自尚硅谷MySQL课程学习笔记</em></p>
<p>B站课程链接</p>
<pre><code>https://www.bilibili.com/video/av49181542?from=search&amp;seid=2930455405068245320</code></pre><a id="more"></a>

<h2 id="数据库的好处"><a href="#数据库的好处" class="headerlink" title="数据库的好处"></a>数据库的好处</h2><pre><code>1.持久化数据到本地
2.可以实现结构化查询，方便管理</code></pre><h2 id="数据库相关概念"><a href="#数据库相关概念" class="headerlink" title="数据库相关概念"></a>数据库相关概念</h2><pre><code>1、DB：数据库，保存一组有组织的数据的容器
2、DBMS：数据库管理系统，又称为数据库软件（产品），用于管理DB中的数据
3、SQL:结构化查询语言，用于和DBMS通信的语言</code></pre><h2 id="数据库存储数据的特点"><a href="#数据库存储数据的特点" class="headerlink" title="数据库存储数据的特点"></a>数据库存储数据的特点</h2><pre><code>1、将数据放到表中，表再放到库中
2、一个数据库中可以有多个表，每个表都有一个的名字，用来标识自己。表名具有唯一性。
3、表具有一些特性，这些特性定义了数据在表中如何存储，类似java中 “类”的设计。
4、表由列组成，我们也称为字段。所有表都是由一个或多个列组成的，每一列类似java 中的”属性”
5、表中的数据是按行存储的，每一行类似于java中的“对象”。</code></pre><h2 id="MySQL介绍"><a href="#MySQL介绍" class="headerlink" title="MySQL介绍"></a>MySQL介绍</h2><h3 id="MySQL服务的启动和停止"><a href="#MySQL服务的启动和停止" class="headerlink" title="MySQL服务的启动和停止"></a>MySQL服务的启动和停止</h3><ol>
<li>计算机——右击管理——服务</li>
<li>通过管理员身份运行<figure class="highlight cmd"><table><tr><td class="code"><pre><span class="line"><span class="built_in">net</span> <span class="built_in">start</span> 服务名（启动服务）</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="built_in">net</span> stop 服务名（停止服务）</span></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="MySQL服务的登录和退出"><a href="#MySQL服务的登录和退出" class="headerlink" title="MySQL服务的登录和退出"></a>MySQL服务的登录和退出</h3><ol>
<li>通过mysql自带的客户端，只限于root用户。</li>
<li>通过windows自带的客户端：<figure class="highlight cmd"><table><tr><td class="code"><pre><span class="line">登录：	mysql 【-h主机名 -P端口号 】-u用户名 -p密码</span></pre></td></tr><tr><td class="code"><pre><span class="line">退出：	<span class="keyword">exit</span>或ctrl+C</span></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="MySQL的常见命令"><a href="#MySQL的常见命令" class="headerlink" title="MySQL的常见命令"></a>MySQL的常见命令</h3><ol>
<li>查看当前所有的数据库：<code>show databases;</code></li>
<li>打开指定的库：    <code>use 库名</code></li>
<li>查看当前库的所有表：<code>show tables;</code></li>
<li>查看其它库的所有表：<code>show tables from 库名;</code></li>
<li>创建表：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create table 表名(</span></pre></td></tr><tr><td class="code"><pre><span class="line">	列名 列类型,</span></pre></td></tr><tr><td class="code"><pre><span class="line">	列名 列类型，</span></pre></td></tr><tr><td class="code"><pre><span class="line">);</span></pre></td></tr></table></figure></li>
<li>查看表结构：<code>desc 表名;</code></li>
<li>查看服务器的版本：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">方式一：登录到mysql服务端</span></pre></td></tr><tr><td class="code"><pre><span class="line">select version();</span></pre></td></tr><tr><td class="code"><pre><span class="line">方式二：没有登录到mysql服务端</span></pre></td></tr><tr><td class="code"><pre><span class="line">mysql --version</span></pre></td></tr><tr><td class="code"><pre><span class="line">或</span></pre></td></tr><tr><td class="code"><pre><span class="line">mysql --V</span></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="MySQL的语法规范"><a href="#MySQL的语法规范" class="headerlink" title="MySQL的语法规范"></a>MySQL的语法规范</h3><ol>
<li>不区分大小写,但建议关键字大写，表名、列名小写；</li>
<li>每条命令最好用分号结尾；</li>
<li>每条命令根据需要，可以进行缩进 或换行；</li>
<li>注释：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">单行注释：#注释文字</span></pre></td></tr><tr><td class="code"><pre><span class="line">单行注释：-- 注释文字</span></pre></td></tr><tr><td class="code"><pre><span class="line">多行注释：&#x2F;* 注释文字  *&#x2F;</span></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="SQL的语言分类"><a href="#SQL的语言分类" class="headerlink" title="SQL的语言分类"></a>SQL的语言分类</h3><pre><code>DQL（Data Query Language）：数据查询语言
    select 
DML（Data Manipulate Language）:数据操作语言
    insert 、update、delete
DDL（Data Define Languge）：数据定义语言
    create、drop、alter
TCL（Transaction Control Language）：事务控制语言
    commit、rollback</code></pre><h3 id="SQL的常见命令"><a href="#SQL的常见命令" class="headerlink" title="SQL的常见命令"></a>SQL的常见命令</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>; <span class="comment">#查看所有的数据库</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">use</span> 库名; <span class="comment">#打开指定 的库</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span>; <span class="comment">#显示库中的所有表</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span> <span class="keyword">from</span> 库名; <span class="comment">#显示指定库中的所有表</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> 表名(</span></pre></td></tr><tr><td class="code"><pre><span class="line">	字段名 字段类型,	</span></pre></td></tr><tr><td class="code"><pre><span class="line">	字段名 字段类型</span></pre></td></tr><tr><td class="code"><pre><span class="line">); <span class="comment">#创建表</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">desc 表名; <span class="comment">#查看指定表的结构</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> 表名; <span class="comment">#显示表中的所有数据</span></span></pre></td></tr></table></figure>

<h2 id="DQL语言"><a href="#DQL语言" class="headerlink" title="DQL语言"></a>DQL语言</h2><h3 id="基础查询"><a href="#基础查询" class="headerlink" title="基础查询"></a>基础查询</h3><p>语法：</p>
<pre><code>SELECT 要查询的东西
【FROM 表名】;</code></pre><p>类似于Java中 :System.out.println(要打印的东西);特点：</p>
<pre><code>1. 通过select查询完的结果 ，是一个虚拟的表格，不是真实存在
2. 要查询的东西 可以是常量值、可以是表达式、可以是字段、可以是函数</code></pre><h3 id="条件查询"><a href="#条件查询" class="headerlink" title="条件查询"></a>条件查询</h3><p>条件查询：根据条件过滤原始表的数据，查询到想要的数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> 要查询的字段|表达式|常量值|函数</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> 表</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">where</span> 条件 ;</span></pre></td></tr></table></figure>
<p>条件表达式，示例：<code>salary&gt;10000</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">条件运算符：	&gt; &lt; &gt;&#x3D; &lt;&#x3D; &#x3D; !&#x3D; &lt;&gt;</span></pre></td></tr></table></figure>
<p>逻辑表达式，示例：<code>salary&gt;10000 &amp;&amp; salary&lt;20000</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">逻辑运算符：</span></pre></td></tr><tr><td class="code"><pre><span class="line">and（&amp;&amp;）:两个条件如果同时成立，结果为true，否则为false</span></pre></td></tr><tr><td class="code"><pre><span class="line">or(||)：两个条件只要有一个成立，结果为true，否则为false</span></pre></td></tr><tr><td class="code"><pre><span class="line">not(!)：如果条件成立，则not后为false，否则为true</span></pre></td></tr></table></figure>
<p>模糊查询，示例：<code>last_name like &#39;a%&#39;</code></p>
<h3 id="排序查询"><a href="#排序查询" class="headerlink" title="排序查询"></a>排序查询</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> 要查询的东西</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> 表</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">where</span> 条件</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> 排序的字段|表达式|函数|别名 [<span class="keyword">asc</span>|<span class="keyword">desc</span>]</span></pre></td></tr></table></figure>

<h3 id="常见函数"><a href="#常见函数" class="headerlink" title="常见函数"></a>常见函数</h3><h4 id="单行函数"><a href="#单行函数" class="headerlink" title="单行函数"></a>单行函数</h4><ol>
<li><p>字符函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">concat 拼接</span></pre></td></tr><tr><td class="code"><pre><span class="line">substr 截取子串</span></pre></td></tr><tr><td class="code"><pre><span class="line">upper 转换成大写</span></pre></td></tr><tr><td class="code"><pre><span class="line">lower 转换成小写</span></pre></td></tr><tr><td class="code"><pre><span class="line">trim 去前后指定的空格和字符</span></pre></td></tr><tr><td class="code"><pre><span class="line">ltrim 去左边空格</span></pre></td></tr><tr><td class="code"><pre><span class="line">rtrim 去右边空格</span></pre></td></tr><tr><td class="code"><pre><span class="line">replace 替换</span></pre></td></tr><tr><td class="code"><pre><span class="line">lpad 左填充</span></pre></td></tr><tr><td class="code"><pre><span class="line">rpad 右填充</span></pre></td></tr><tr><td class="code"><pre><span class="line">instr 返回子串第一次出现的索引</span></pre></td></tr><tr><td class="code"><pre><span class="line">length 获取字节个数</span></pre></td></tr></table></figure></li>
<li><p>数学函数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">round 四舍五入</span></pre></td></tr><tr><td class="code"><pre><span class="line">rand 随机数</span></pre></td></tr><tr><td class="code"><pre><span class="line">floor 向下取整</span></pre></td></tr><tr><td class="code"><pre><span class="line">ceil 向上取整</span></pre></td></tr><tr><td class="code"><pre><span class="line">mod 取余</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">truncate</span> 截断</span></pre></td></tr></table></figure></li>
<li><p>日期函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">now 当前系统日期+时间</span></pre></td></tr><tr><td class="code"><pre><span class="line">curdate 当前系统日期</span></pre></td></tr><tr><td class="code"><pre><span class="line">curtime 当前系统时间</span></pre></td></tr><tr><td class="code"><pre><span class="line">str_to_date 将字符转换成日期</span></pre></td></tr><tr><td class="code"><pre><span class="line">date_format 将日期转换成字符</span></pre></td></tr></table></figure>
</li>
<li><p>流程控制函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if 处理双分支</span></pre></td></tr><tr><td class="code"><pre><span class="line">case 处理多分支</span></pre></td></tr><tr><td class="code"><pre><span class="line">	情况1：处理等值判断</span></pre></td></tr><tr><td class="code"><pre><span class="line">	情况2：处理条件判断</span></pre></td></tr></table></figure>
</li>
<li><p>其他函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">version 版本</span></pre></td></tr><tr><td class="code"><pre><span class="line">database 当前库</span></pre></td></tr><tr><td class="code"><pre><span class="line">user 当前连接用户</span></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="分组函数"><a href="#分组函数" class="headerlink" title="分组函数"></a>分组函数</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sum 求和</span></pre></td></tr><tr><td class="code"><pre><span class="line">max 最大值</span></pre></td></tr><tr><td class="code"><pre><span class="line">min 最小值</span></pre></td></tr><tr><td class="code"><pre><span class="line">avg 平均值</span></pre></td></tr><tr><td class="code"><pre><span class="line">count 计数</span></pre></td></tr></table></figure>
<p>特点：</p>
<pre><code>1. 以上五个分组函数都忽略null值，除了count(*)
2. sum和avg一般用于处理数值型，max、min、count可以处理任何数据类型
3. 都可以搭配distinct使用，用于统计去重后的结果
4. count的参数可以支持：字段，\*，常量值：一般放1。建议使用 count(*)</code></pre><h3 id="分组查询"><a href="#分组查询" class="headerlink" title="分组查询"></a>分组查询</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> 查询的字段，分组函数</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> 表</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> 分组的字段</span></pre></td></tr></table></figure>

<pre><code>特点：
1、可以按单个字段分组
2、和分组函数一同查询的字段最好是分组后的字段
3、分组筛选
            针对的表 位置            关键字
分组前筛选：    原始表            group by的前面        where
分组后筛选：    分组后的结果集    group by的后面        having
4、可以按多个字段分组，字段之间用逗号隔开
5、可以支持排序
6、having后可以支持别名</code></pre><h3 id="多表连接查询"><a href="#多表连接查询" class="headerlink" title="多表连接查询"></a>多表连接查询</h3><pre><code>笛卡尔乘积：如果连接条件省略或无效则会出现
解决办法：添加上连接条件</code></pre><p>一、传统模式下的连接：等值连接——非等值连接</p>
<pre><code>1.等值连接的结果 = 多个表的交集
2.n表连接，至少需要n-1个连接条件
3.多个表不分主次，没有顺序要求
4.一般为表起别名，提高阅读性和性能</code></pre><p>二、sql99语法：通过join关键字实现连接</p>
<pre><code>含义：1999年推出的sql语法
支持：
等值连接、非等值连接 （内连接）、外连接、    交叉连接

select 字段，...
from 表1
【inner|left outer|right outer|cross】join 表2 on  连接条件
【inner|left outer|right outer|cross】join 表3 on  连接条件
【where 筛选条件】
【group by 分组字段】
【having 分组后的筛选条件】
【order by 排序的字段或表达式】</code></pre><p>好处：语句上，连接条件和筛选条件实现了分离，简洁明了！</p>
<p>三、自连接<br>案例：查询员工名和直接上级的名称</p>
<pre><code>SELECT e.last_name,m.last_name
FROM employees e
JOIN employees m ON e.`manager_id`=m.`employee_id`;

SELECT e.last_name,m.last_name
FROM employees e,employees m 
WHERE e.`manager_id`=m.`employee_id`;</code></pre><h3 id="子查询"><a href="#子查询" class="headerlink" title="子查询"></a>子查询</h3><p>含义：</p>
<pre><code>一条查询语句中又嵌套了另一条完整的select语句，其中被嵌套的select语句，称为子查询或内查询
在外面的查询语句，称为主查询或外查询</code></pre><p>特点：</p>
<pre><code>1、子查询都放在小括号内
2、子查询可以放在from后面、select后面、where后面、having后面，但一般放在条件的右侧
3、子查询优先于主查询执行，主查询使用了子查询的执行结果
4、子查询根据查询结果的行数不同分为以下两类：
  单行子查询
    结果集只有一行
    一般搭配单行操作符使用：&gt; &lt; = &lt;&gt; &gt;= &lt;= 
    非法使用子查询的情况：
    a、子查询的结果为一组值
    b、子查询的结果为空

  多行子查询
    结果集有多行
    一般搭配多行操作符使用：any、all、in、not in
    in： 属于子查询结果中的任意一个就行
    any和all往往可以用其他查询代替</code></pre><h3 id="分页查询"><a href="#分页查询" class="headerlink" title="分页查询"></a>分页查询</h3><p>应用场景：实际的web项目中需要根据用户的需求提交对应的分页查询的sql语句</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> 字段|表达式,...</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> 表</span></pre></td></tr><tr><td class="code"><pre><span class="line">【<span class="keyword">where</span> 条件】</span></pre></td></tr><tr><td class="code"><pre><span class="line">【<span class="keyword">group</span> <span class="keyword">by</span> 分组字段】</span></pre></td></tr><tr><td class="code"><pre><span class="line">【<span class="keyword">having</span> 条件】</span></pre></td></tr><tr><td class="code"><pre><span class="line">【<span class="keyword">order</span> <span class="keyword">by</span> 排序的字段】</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">limit</span> 【起始的条目索引，】条目数;</span></pre></td></tr></table></figure>
<p>特点：</p>
<pre><code>1.起始条目索引从0开始
2.limit子句放在查询语句的最后
3.公式：select * from  表 limit （page-1）*sizePerPage,sizePerPage
假如:
每页显示条目数sizePerPage
要显示的页数 page</code></pre><h3 id="联合查询"><a href="#联合查询" class="headerlink" title="联合查询"></a>联合查询</h3><p>引入：union 联合、合并</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> 字段|常量|表达式|函数 【<span class="keyword">from</span> 表】 【<span class="keyword">where</span> 条件】 <span class="keyword">union</span> 【<span class="keyword">all</span>】</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> 字段|常量|表达式|函数 【<span class="keyword">from</span> 表】 【<span class="keyword">where</span> 条件】 <span class="keyword">union</span> 【<span class="keyword">all</span>】</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> 字段|常量|表达式|函数 【<span class="keyword">from</span> 表】 【<span class="keyword">where</span> 条件】 <span class="keyword">union</span>  【<span class="keyword">all</span>】</span></pre></td></tr><tr><td class="code"><pre><span class="line">.....</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> 字段|常量|表达式|函数 【<span class="keyword">from</span> 表】 【<span class="keyword">where</span> 条件】</span></pre></td></tr></table></figure>
<p>特点：</p>
<pre><code>1、多条查询语句的查询的列数必须是一致的
2、多条查询语句的查询的列的类型几乎相同
3、union代表去重，union all代表不去重</code></pre><h2 id="DML语言"><a href="#DML语言" class="headerlink" title="DML语言"></a>DML语言</h2><h3 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h3><p>语法：<br>    insert into 表名(字段名，…)<br>    values(值1，…);</p>
<p>特点：</p>
<pre><code>1、字段类型和值类型一致或兼容，而且一一对应
2、可以为空的字段，可以不用插入值，或用null填充
3、不可以为空的字段，必须插入值
4、字段个数和值的个数必须一致
5、字段可以省略，但默认所有字段，并且顺序和表中的存储顺序一致</code></pre><h3 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h3><p>修改单表语法：</p>
<pre><code>update 表名 set 字段=新值,字段=新值
【where 条件】</code></pre><p>修改多表语法：</p>
<pre><code>update 表1 别名1,表2 别名2
set 字段=新值，字段=新值
where 连接条件
and 筛选条件</code></pre><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><p>方式1：delete语句 </p>
<p>单表的删除： ★<br>    delete from 表名 【where 筛选条件】</p>
<p>多表的删除：<br>    delete 别名1，别名2<br>    from 表1 别名1，表2 别名2<br>    where 连接条件<br>    and 筛选条件;</p>
<p>方式2：truncate语句</p>
<pre><code>truncate table 表名</code></pre><p>两种方式的区别【面试题】</p>
<pre><code>#1.truncate不能加where条件，而delete可以加where条件

#2.truncate的效率高一丢丢

#3.truncate 删除带自增长的列的表后，如果再插入数据，数据从1开始
#delete 删除带自增长列的表后，如果再插入数据，数据从上一次的断点处开始

#4.truncate删除不能回滚，delete删除可以回滚</code></pre><h2 id="DDL语言"><a href="#DDL语言" class="headerlink" title="DDL语言"></a>DDL语言</h2><h3 id="库和表的管理"><a href="#库和表的管理" class="headerlink" title="库和表的管理"></a>库和表的管理</h3><p>库的管理：</p>
<pre><code>一、创建库
create database 库名
二、删除库
drop database 库名</code></pre><p>表的管理：<br>    #1.创建表</p>
<pre><code>CREATE TABLE IF NOT EXISTS stuinfo(
    stuId INT,
    stuName VARCHAR(20),
    gender CHAR,
    bornDate DATETIME


);

DESC studentinfo;
#2.修改表 alter
语法：ALTER TABLE 表名 ADD|MODIFY|DROP|CHANGE COLUMN 字段名 【字段类型】;

#①修改字段名
ALTER TABLE studentinfo CHANGE  COLUMN sex gender CHAR;

#②修改表名
ALTER TABLE stuinfo RENAME [TO]  studentinfo;
#③修改字段类型和列级约束
ALTER TABLE studentinfo MODIFY COLUMN borndate DATE ;

#④添加字段

ALTER TABLE studentinfo ADD COLUMN email VARCHAR(20) first;
#⑤删除字段
ALTER TABLE studentinfo DROP COLUMN email;


#3.删除表

DROP TABLE [IF EXISTS] studentinfo;</code></pre><h3 id="常见类型"><a href="#常见类型" class="headerlink" title="常见类型"></a>常见类型</h3><pre><code>整型：

小数：
    浮点型
    定点型
字符型：
日期型：
Blob类型：</code></pre><h3 id="常见约束"><a href="#常见约束" class="headerlink" title="常见约束"></a>常见约束</h3><pre><code>NOT NULL
DEFAULT
UNIQUE
CHECK
PRIMARY KEY
FOREIGN KEY</code></pre><h2 id="数据库事务"><a href="#数据库事务" class="headerlink" title="数据库事务"></a>数据库事务</h2><h3 id="含义"><a href="#含义" class="headerlink" title="含义"></a>含义</h3><pre><code>通过一组逻辑操作单元（一组DML——sql语句），将数据从一种状态切换到另外一种状态</code></pre><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><pre><code>（ACID）
原子性：要么都执行，要么都回滚
一致性：保证数据的状态操作前和操作后保持一致
隔离性：多个事务同时操作相同数据库的同一个数据时，一个事务的执行不受另外一个事务的干扰
持久性：一个事务一旦提交，则数据将持久化到本地，除非其他事务对其进行修改</code></pre><p>相关步骤：</p>
<pre><code>1、开启事务
2、编写事务的一组逻辑操作单元（多条sql语句）
3、提交事务或回滚事务</code></pre><h3 id="事务的分类："><a href="#事务的分类：" class="headerlink" title="事务的分类："></a>事务的分类：</h3><p>隐式事务，没有明显的开启和结束事务的标志</p>
<pre><code>比如
insert、update、delete语句本身就是一个事务</code></pre><p>显式事务，具有明显的开启和结束事务的标志</p>
<pre><code>1、开启事务
取消自动提交事务的功能

2、编写事务的一组逻辑操作单元（多条sql语句）
insert
update
delete

3、提交事务或回滚事务</code></pre><h3 id="使用到的关键字"><a href="#使用到的关键字" class="headerlink" title="使用到的关键字"></a>使用到的关键字</h3><pre><code>set autocommit=0;
start transaction;
commit;
rollback;

savepoint  断点
commit to 断点
rollback to 断点</code></pre><h3 id="事务的隔离级别"><a href="#事务的隔离级别" class="headerlink" title="事务的隔离级别:"></a>事务的隔离级别:</h3><p>事务并发问题如何发生？</p>
<pre><code>当多个事务同时操作同一个数据库的相同数据时</code></pre><p>事务的并发问题有哪些？</p>
<pre><code>脏读：一个事务读取到了另外一个事务未提交的数据
不可重复读：同一个事务中，多次读取到的数据不一致
幻读：一个事务读取数据时，另外一个事务进行更新，导致第一个事务读取到了没有更新的数据</code></pre><p>如何避免事务的并发问题？</p>
<pre><code>通过设置事务的隔离级别
1、READ UNCOMMITTED
2、READ COMMITTED 可以避免脏读
3、REPEATABLE READ 可以避免脏读、不可重复读和一部分幻读
4、SERIALIZABLE可以避免脏读、不可重复读和幻读</code></pre><p>设置隔离级别：</p>
<pre><code>set session|global  transaction isolation level 隔离级别名;</code></pre><p>查看隔离级别：</p>
<pre><code>select @@tx_isolation;</code></pre><h2 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h2><p>含义：理解成一张虚拟的表</p>
<p>视图和表的区别：</p>
<pre><code>    使用方式    占用物理空间

视图    完全相同    不占用，仅仅保存的是sql逻辑

表    完全相同    占用</code></pre><p>视图的好处：</p>
<pre><code>1、sql语句提高重用性，效率高
2、和表实现了分离，提高了安全性</code></pre><h3 id="视图的创建"><a href="#视图的创建" class="headerlink" title="视图的创建"></a>视图的创建</h3><pre><code>语法：
CREATE VIEW  视图名
AS
查询语句;</code></pre><h3 id="视图的增删改查"><a href="#视图的增删改查" class="headerlink" title="视图的增删改查"></a>视图的增删改查</h3><pre><code>1、查看视图的数据 ★

SELECT * FROM my_v4;
SELECT * FROM my_v1 WHERE last_name=&apos;Partners&apos;;

2、插入视图的数据
INSERT INTO my_v4(last_name,department_id) VALUES(&apos;虚竹&apos;,90);

3、修改视图的数据

UPDATE my_v4 SET last_name =&apos;梦姑&apos; WHERE last_name=&apos;虚竹&apos;;


4、删除视图的数据
DELETE FROM my_v4;</code></pre><h3 id="某些视图不能更新"><a href="#某些视图不能更新" class="headerlink" title="某些视图不能更新"></a>某些视图不能更新</h3><pre><code>包含以下关键字的sql语句：分组函数、distinct、group  by、having、union或者union all
常量视图
Select中包含子查询
join
from一个不能更新的视图
where子句的子查询引用了from子句中的表</code></pre><h3 id="视图逻辑的更新"><a href="#视图逻辑的更新" class="headerlink" title="视图逻辑的更新"></a>视图逻辑的更新</h3><pre><code>#方式一：
CREATE OR REPLACE VIEW test_v7
AS
SELECT last_name FROM employees
WHERE employee_id&gt;100;

#方式二:
ALTER VIEW test_v7
AS
SELECT employee_id FROM employees;

SELECT * FROM test_v7;</code></pre><h3 id="视图的删除"><a href="#视图的删除" class="headerlink" title="视图的删除"></a>视图的删除</h3><pre><code>DROP VIEW test_v1,test_v2,test_v3;</code></pre><h3 id="视图结构的查看"><a href="#视图结构的查看" class="headerlink" title="视图结构的查看"></a>视图结构的查看</h3><pre><code>DESC test_v7;
SHOW CREATE VIEW test_v7;</code></pre><h2 id="存储过程"><a href="#存储过程" class="headerlink" title="存储过程"></a>存储过程</h2><p>含义：一组经过预先编译的sql语句的集合<br>好处：</p>
<pre><code>1、提高了sql语句的重用性，减少了开发程序员的压力
2、提高了效率
3、减少了传输次数</code></pre><p>分类：</p>
<pre><code>1、无返回无参
2、仅仅带in类型，无返回有参
3、仅仅带out类型，有返回无参
4、既带in又带out，有返回有参
5、带inout，有返回有参
注意：in、out、inout都可以在一个存储过程中带多个</code></pre><h3 id="创建存储过程"><a href="#创建存储过程" class="headerlink" title="创建存储过程"></a>创建存储过程</h3><p>语法：</p>
<pre><code>create procedure 存储过程名(in|out|inout 参数名  参数类型,...)
begin
    存储过程体

end</code></pre><p>类似于方法：</p>
<pre><code>修饰符 返回类型 方法名(参数类型 参数名,...){

    方法体;
}</code></pre><p>注意</p>
<pre><code>1、需要设置新的结束标记
delimiter 新的结束标记
示例：
delimiter $

CREATE PROCEDURE 存储过程名(IN|OUT|INOUT 参数名  参数类型,...)
BEGIN
    sql语句1;
    sql语句2;

END $

2、存储过程体中可以有多条sql语句，如果仅仅一条sql语句，则可以省略begin end

3、参数前面的符号的意思
in:该参数只能作为输入 （该参数不能做返回值）
out：该参数只能作为输出（该参数只能做返回值）
inout：既能做输入又能做输出</code></pre><h3 id="调用存储过程"><a href="#调用存储过程" class="headerlink" title="调用存储过程"></a>调用存储过程</h3><pre><code>call 存储过程名(实参列表)</code></pre><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><h3 id="创建函数"><a href="#创建函数" class="headerlink" title="创建函数"></a>创建函数</h3><p>学过的函数：LENGTH、SUBSTR、CONCAT等<br>语法：</p>
<pre><code>CREATE FUNCTION 函数名(参数名 参数类型,...) RETURNS 返回类型
BEGIN
    函数体

END</code></pre><h3 id="调用函数"><a href="#调用函数" class="headerlink" title="调用函数"></a>调用函数</h3><pre><code>SELECT 函数名（实参列表）</code></pre><h3 id="函数和存储过程的区别"><a href="#函数和存储过程的区别" class="headerlink" title="函数和存储过程的区别"></a>函数和存储过程的区别</h3><pre><code>        关键字        调用语法    返回值            应用场景
函数        FUNCTION    SELECT 函数()    只能是一个        一般用于查询结果为一个值并返回时，当有返回值而且仅仅一个
存储过程    PROCEDURE    CALL 存储过程()    可以有0个或多个        一般用于更新</code></pre><h2 id="流程控制结构"><a href="#流程控制结构" class="headerlink" title="流程控制结构"></a>流程控制结构</h2><h3 id="系统变量"><a href="#系统变量" class="headerlink" title="系统变量"></a>系统变量</h3><p>一、全局变量</p>
<p>作用域：针对于所有会话（连接）有效，但不能跨重启</p>
<pre><code>查看所有全局变量
SHOW GLOBAL VARIABLES;
查看满足条件的部分系统变量
SHOW GLOBAL VARIABLES LIKE &apos;%char%&apos;;
查看指定的系统变量的值
SELECT @@global.autocommit;
为某个系统变量赋值
SET @@global.autocommit=0;
SET GLOBAL autocommit=0;</code></pre><p>二、会话变量</p>
<p>作用域：针对于当前会话（连接）有效</p>
<pre><code>查看所有会话变量
SHOW SESSION VARIABLES;
查看满足条件的部分会话变量
SHOW SESSION VARIABLES LIKE &apos;%char%&apos;;
查看指定的会话变量的值
SELECT @@autocommit;
SELECT @@session.tx_isolation;
为某个会话变量赋值
SET @@session.tx_isolation=&apos;read-uncommitted&apos;;
SET SESSION tx_isolation=&apos;read-committed&apos;;</code></pre><h3 id="自定义变量"><a href="#自定义变量" class="headerlink" title="自定义变量"></a>自定义变量</h3><p>一、用户变量</p>
<p>声明并初始化：</p>
<pre><code>SET @变量名=值;
SET @变量名:=值;
SELECT @变量名:=值;</code></pre><p>赋值：</p>
<pre><code>方式一：一般用于赋简单的值
SET 变量名=值;
SET 变量名:=值;
SELECT 变量名:=值;


方式二：一般用于赋表 中的字段值
SELECT 字段名或表达式 INTO 变量
FROM 表;</code></pre><p>使用：</p>
<pre><code>select @变量名;</code></pre><p>二、局部变量</p>
<p>声明：</p>
<pre><code>declare 变量名 类型 【default 值】;</code></pre><p>赋值：</p>
<pre><code>方式一：一般用于赋简单的值
SET 变量名=值;
SET 变量名:=值;
SELECT 变量名:=值;


方式二：一般用于赋表 中的字段值
SELECT 字段名或表达式 INTO 变量
FROM 表;</code></pre><p>使用：</p>
<pre><code>select 变量名</code></pre><p>二者的区别：</p>
<pre><code>作用域            定义位置        语法</code></pre><p>用户变量    当前会话        会话的任何地方        加@符号，不用指定类型<br>局部变量    定义它的BEGIN END中     BEGIN END的第一句话    一般不用加@,需要指定类型</p>
<h3 id="分支"><a href="#分支" class="headerlink" title="分支"></a>分支</h3><p>一、if函数<br>    语法：if(条件，值1，值2)<br>    特点：可以用在任何位置</p>
<p>二、case语句</p>
<p>语法：</p>
<pre><code>情况一：类似于switch
case 表达式
when 值1 then 结果1或语句1(如果是语句，需要加分号) 
when 值2 then 结果2或语句2(如果是语句，需要加分号)
...
else 结果n或语句n(如果是语句，需要加分号)
end 【case】（如果是放在begin end中需要加上case，如果放在select后面不需要）

情况二：类似于多重if
case 
when 条件1 then 结果1或语句1(如果是语句，需要加分号) 
when 条件2 then 结果2或语句2(如果是语句，需要加分号)
...
else 结果n或语句n(如果是语句，需要加分号)
end 【case】（如果是放在begin end中需要加上case，如果放在select后面不需要）</code></pre><p>特点：<br>    可以用在任何位置</p>
<p>三、if elseif语句</p>
<p>语法：</p>
<pre><code>if 情况1 then 语句1;
elseif 情况2 then 语句2;
...
else 语句n;
end if;</code></pre><p>特点：<br>    只能用在begin end中！！！！！！！！！！！！！！！</p>
<p>三者比较：<br>            应用场合<br>    if函数        简单双分支<br>    case结构    等值判断 的多分支<br>    if结构        区间判断 的多分支</p>
<h3 id="循环"><a href="#循环" class="headerlink" title="循环"></a>循环</h3><p>语法：</p>
<pre><code>【标签：】WHILE 循环条件  DO
    循环体
END WHILE 【标签】;</code></pre><p>特点：</p>
<pre><code>只能放在BEGIN END里面

如果要搭配leave跳转语句，需要使用标签，否则可以不用标签

leave类似于java中的break语句，跳出所在循环！！！</code></pre>]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL 8.0.19 Windows以及Navicat15破解版安装教程</title>
    <url>/install-mysql/</url>
    <content><![CDATA[<p>Windows10系统<br>MySQL安装版本：8.0.19<br>Navicat安装版本：premium 15 for mysql</p>
<p>Hint:</p>
<pre><code>SQL为结构化查询语句
MySQL, SQL Server, Oracle等为数据库管理系统
Navicat, SQLyog等为数据库管理工具</code></pre><a id="more"></a>
<h1 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h1><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>官网下载地址，选择64位安装包：mysql-8.0.19-winx64.zip。</p>
<pre><code>https://dev.mysql.com/downloads/mysql/</code></pre><p>然而下载速度很慢，可以切换国内镜像。搜狐镜像也比较慢，网易镜像速度飞快。</p>
<pre><code>http://mirrors.163.com/mysql/Downloads/MySQL-8.0/
http://mirrors.sohu.com/mysql/MySQL-8.0/</code></pre><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><p>解压压缩包至指定的安装目录如：<code>D:\Program\MySQL</code>。</p>
<h3 id="添加配置文件"><a href="#添加配置文件" class="headerlink" title="添加配置文件"></a>添加配置文件</h3><p>手动新建配置文件<code>my.ini</code>，写入内容（无需手动创建data文件夹）：</p>
<pre><code>[mysqld]
# 设置3306端口
port=3306
# 设置mysql的安装目录
basedir=D:\Program\MySQL
# 设置mysql数据库的数据的存放目录
datadir=D:\Program\MySQL\data
# 允许最大连接数
max_connections=200
# 允许连接失败的次数。
max_connect_errors=10
# 服务端使用的字符集默认为utf8mb4
character-set-server=utf8mb4
# 创建新表时将使用的默认存储引擎
default-storage-engine=INNODB
# 默认使用“mysql_native_password”插件认证
#mysql_native_password
default_authentication_plugin=mysql_native_password
[mysql]
# 设置mysql客户端默认字符集
default-character-set=utf8mb4
[client]
# 设置mysql客户端连接服务端时默认使用的端口
port=3306
default-character-set=utf8mb4</code></pre><h3 id="初始化数据库"><a href="#初始化数据库" class="headerlink" title="初始化数据库"></a>初始化数据库</h3><p>管理员身份打开cmd窗口，执行如下命令，将生成root用户的初始密码，用作后续登录。</p>
<pre><code>mysqld --initialize --user=mysql --console</code></pre><h2 id="安装数据库"><a href="#安装数据库" class="headerlink" title="安装数据库"></a>安装数据库</h2><ol>
<li>执行命令：<code>mysqld -install</code></li>
<li>启动服务：<code>net start mysql</code></li>
<li>登录数据库：<code>mysql -u root -p</code>，并输入刚刚得到的密码</li>
<li>修改密码：<code>alter user &#39;root&#39;@&#39;localhost&#39; identified by &#39;yourpassword&#39;</code></li>
<li>退出：<code>exit</code></li>
</ol>
<h3 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h3><p>若安装过程中报错，<code>VCRUNTIME140_1.dll</code>问题，微软官网下载适用于Visual Studio 2015、2017 和 2019 的 Microsoft Visual C++ 可再发行软件包：</p>
<pre><code>https://support.microsoft.com/zh-cn/help/2977003/the-latest-supported-visual-c-downloads</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><pre><code>https://blog.csdn.net/qq_37350706/article/details/81707862</code></pre><h1 id="Navicat"><a href="#Navicat" class="headerlink" title="Navicat"></a>Navicat</h1><p>参考如下，玄学破解，需要多试几次。</p>
<pre><code>https://www.cnblogs.com/runw/p/12255962.html</code></pre><p>官方使用手册</p>
<pre><code>https://www.navicat.com.cn/support/online-manual</code></pre>]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP 汇总链接合集</title>
    <url>/nlp-url-collection/</url>
    <content><![CDATA[<a id="more"></a>

<ol>
<li>中文自然语言处理 Chinese NLP<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;chinesenlp.xyz&#x2F;#&#x2F;zh&#x2F;docs&#x2F;</span></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>常见机器学习分类模型Python实现</title>
    <url>/classification-models/</url>
    <content><![CDATA[<p>机器学习分类模型代码实现：KNN, LDA, NB, LR, CART, SVM, RF, XGBOOST, LightGBDT</p>
<a id="more"></a>

<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score, GridSearchCV</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, accuracy_score, roc_curve, auc</span></pre></td></tr><tr><td class="code"><pre><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>]=[<span class="string">'SimHei'</span>]  </span></pre></td></tr><tr><td class="code"><pre><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>]=<span class="literal">False</span>  </span></pre></td></tr><tr><td class="code"><pre><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.read_table(<span class="string">'australian.dat'</span>,sep=<span class="string">' '</span>, header=<span class="literal">None</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">df.columns = [<span class="string">'A1'</span>, <span class="string">'A2'</span>, <span class="string">'A3'</span>, <span class="string">'A4'</span>, <span class="string">'A5'</span>, <span class="string">'A6'</span>, <span class="string">'A7'</span>, <span class="string">'A8'</span>, <span class="string">'A9'</span>, <span class="string">'A10'</span>, <span class="string">'A11'</span>, <span class="string">'A12'</span>, <span class="string">'A13'</span>, <span class="string">'A14'</span>, <span class="string">'Default'</span>]</span></pre></td></tr></table></figure>

<ol>
<li><p>分类变量处理：哑变量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">categorical_columns = [<span class="string">'A1'</span>, <span class="string">'A4'</span>, <span class="string">'A5'</span>, <span class="string">'A6'</span>, <span class="string">'A8'</span>, <span class="string">'A9'</span>, <span class="string">'A11'</span>, <span class="string">'A12'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">df = pd.get_dummies(df, columns = categorical_columns)</span></pre></td></tr><tr><td class="code"><pre><span class="line">df.describe()</span></pre></td></tr></table></figure>
<div>
<style scoped>
 .dataframe tbody tr th:only-of-type {
     vertical-align: middle;
 }

<p> .dataframe tbody tr th {</p>
<pre><code>vertical-align: top;</code></pre><p> }</p>
<p> .dataframe thead th {</p>
<pre><code>text-align: right;</code></pre><p> }<br></style></p>
<table border="1" class="dataframe">
<thead>
 <tr style="text-align: right;">
   <th></th>
   <th>A2</th>
   <th>A3</th>
   <th>A7</th>
   <th>A10</th>
   <th>A13</th>
   <th>A14</th>
   <th>Default</th>
   <th>A1_0</th>
   <th>A1_1</th>
   <th>A4_1</th>
   <th>...</th>
   <th>A6_9</th>
   <th>A8_0</th>
   <th>A8_1</th>
   <th>A9_0</th>
   <th>A9_1</th>
   <th>A11_0</th>
   <th>A11_1</th>
   <th>A12_1</th>
   <th>A12_2</th>
   <th>A12_3</th>
 </tr>
</thead>
<tbody>
 <tr>
   <th>count</th>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.00000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>...</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
   <td>690.000000</td>
 </tr>
 <tr>
   <th>mean</th>
   <td>31.568203</td>
   <td>4.758725</td>
   <td>2.223406</td>
   <td>2.40000</td>
   <td>184.014493</td>
   <td>1018.385507</td>
   <td>0.444928</td>
   <td>0.321739</td>
   <td>0.678261</td>
   <td>0.236232</td>
   <td>...</td>
   <td>0.011594</td>
   <td>0.476812</td>
   <td>0.523188</td>
   <td>0.572464</td>
   <td>0.427536</td>
   <td>0.542029</td>
   <td>0.457971</td>
   <td>0.082609</td>
   <td>0.905797</td>
   <td>0.011594</td>
 </tr>
 <tr>
   <th>std</th>
   <td>11.853273</td>
   <td>4.978163</td>
   <td>3.346513</td>
   <td>4.86294</td>
   <td>172.159274</td>
   <td>5210.102598</td>
   <td>0.497318</td>
   <td>0.467482</td>
   <td>0.467482</td>
   <td>0.425074</td>
   <td>...</td>
   <td>0.107128</td>
   <td>0.499824</td>
   <td>0.499824</td>
   <td>0.495080</td>
   <td>0.495080</td>
   <td>0.498592</td>
   <td>0.498592</td>
   <td>0.275490</td>
   <td>0.292323</td>
   <td>0.107128</td>
 </tr>
 <tr>
   <th>min</th>
   <td>13.750000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.00000</td>
   <td>0.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>...</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
 </tr>
 <tr>
   <th>25%</th>
   <td>22.670000</td>
   <td>1.000000</td>
   <td>0.165000</td>
   <td>0.00000</td>
   <td>80.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>...</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
 </tr>
 <tr>
   <th>50%</th>
   <td>28.625000</td>
   <td>2.750000</td>
   <td>1.000000</td>
   <td>0.00000</td>
   <td>160.000000</td>
   <td>6.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
   <td>...</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
   <td>0.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
 </tr>
 <tr>
   <th>75%</th>
   <td>37.707500</td>
   <td>7.207500</td>
   <td>2.625000</td>
   <td>3.00000</td>
   <td>272.000000</td>
   <td>396.500000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
   <td>...</td>
   <td>0.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
   <td>1.000000</td>
   <td>0.000000</td>
 </tr>
 <tr>
   <th>max</th>
   <td>80.250000</td>
   <td>28.000000</td>
   <td>28.500000</td>
   <td>67.00000</td>
   <td>2000.000000</td>
   <td>100001.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>...</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
   <td>1.000000</td>
 </tr>
</tbody>
</table>
<p>8 rows × 43 columns</p>
</div>
</li>
<li><p>连续变量处理：标准化<br>先划分训练集和测试集，在求训练集的连续变量每一列的方差和均值，用得到的训练集的方差和均值分别对训练集和测试集做标准化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df, test_df = train_test_split(df, test_size = <span class="number">0.3</span>, random_state= <span class="number">2019</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">train_y = train_df.Default</span></pre></td></tr><tr><td class="code"><pre><span class="line">test_y = test_df.Default</span></pre></td></tr><tr><td class="code"><pre><span class="line">train_x = train_df.drop([<span class="string">'Default'</span>], axis = <span class="number">1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">test_x = test_df.drop([<span class="string">'Default'</span>], axis = <span class="number">1</span>)</span></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">continus_keys = train_x.keys()[:<span class="number">6</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">continus_keys</span></pre></td></tr></table></figure>
<p> Index([‘A2’, ‘A3’, ‘A7’, ‘A10’, ‘A13’, ‘A14’], dtype=’object’)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> continus_keys:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    std,mean = [np.std(train_x[each]),np.mean(train_x[each])]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    f = <span class="keyword">lambda</span> x: (x-mean)/(std)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    train_x[each] = train_x[each].apply(f)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    test_x[each] = test_x[each].apply(f)</span></pre></td></tr></table></figure>
<p>训练集和测试集的维度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_x.shape,train_x.shape</span></pre></td></tr><tr><td class="code"><pre><span class="line">((<span class="number">207</span>, <span class="number">42</span>), (<span class="number">483</span>, <span class="number">42</span>))</span></pre></td></tr></table></figure></li>
<li><p>roc曲线函数和混淆矩阵函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_roc_curve</span><span class="params">(test_y, predictions)</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    false_positive_rate, recall, thresholds = roc_curve(test_y, predictions)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    roc_auc = auc(false_positive_rate, recall)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.title(<span class="string">'Receiver Operating Characteristic (ROC)'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.plot(false_positive_rate, recall, <span class="string">'b'</span>, label = <span class="string">'AUC = %0.3f'</span> %roc_auc)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.legend(loc=<span class="string">'lower right'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.plot([<span class="number">0</span>,<span class="number">1</span>], [<span class="number">0</span>,<span class="number">1</span>], <span class="string">'r--'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xlim([<span class="number">0.0</span>,<span class="number">1.0</span>])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.ylim([<span class="number">0.0</span>,<span class="number">1.0</span>])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.ylabel(<span class="string">'Recall'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xlabel(<span class="string">'Fall-out (1-Specificity)'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.show()</span></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_confusion_matrix</span><span class="params">(test_y, predictions)</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    cm = confusion_matrix(test_y, predictions)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    labels = [<span class="string">'No Default'</span>, <span class="string">'Default'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span></pre></td></tr><tr><td class="code"><pre><span class="line">    sns.heatmap(cm,xticklabels=labels, yticklabels=labels, annot=<span class="literal">True</span>, fmt=<span class="string">'d'</span>, cmap=<span class="string">"Blues"</span>, vmin = <span class="number">0.2</span>);</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.title(<span class="string">'Confusion Matrix'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.ylabel(<span class="string">'True Class'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xlabel(<span class="string">'Predicted Class'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.show()</span></pre></td></tr><tr><td class="code"><pre><span class="line">    target_names = [<span class="string">'class 0'</span>, <span class="string">'class 1'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(classification_report(test_y, predictions, target_names=target_names))</span></pre></td></tr><tr><td class="code"><pre><span class="line">    recall = metrics.recall_score(test_y, predictions)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    f1 = metrics.f1_score(test_y, predictions)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    acc = metrics.accuracy_score(test_y, predictions)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    pre = metrics.precision_score(test_y, predictions)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    auc = metrics.roc_auc_score(test_y,predictions)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">return</span> [recall,pre,acc,f1,auc]</span></pre></td></tr></table></figure>


</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">RESULT = &#123;&#125;</span></pre></td></tr></table></figure>

<h1 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span></pre></td></tr></table></figure>

<p>GridSearchCV 为网格搜索， 在训练集上进行10折交叉验证，目的找到最优参数<br>搜索参数：<br>k_range：领域大小<br>p: 距离定义,p=2:欧式距离;p=1：曼哈顿距离<br>这里距离定义都是用于连续变量，但是数据集中还有分类变量，存在一点不合理，ppt还是别展示，避免被问到</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knn = KNeighborsClassifier()</span></pre></td></tr><tr><td class="code"><pre><span class="line">k_range = list(range(<span class="number">5</span>,<span class="number">15</span>))</span></pre></td></tr><tr><td class="code"><pre><span class="line">p = [<span class="number">1</span>,<span class="number">2</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">param_gridknn = dict(n_neighbors = k_range,p=p)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridKNN = GridSearchCV(knn, param_gridknn, cv=<span class="number">10</span>, scoring=<span class="string">'roc_auc'</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridKNN.fit(train_x,train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'best params are:'</span>,str(gridKNN.best_params_))</span></pre></td></tr></table></figure>

<pre><code>Fitting 10 folds for each of 20 candidates, totalling 200 fits


[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    7.4s


best params are: {&apos;n_neighbors&apos;: 10, &apos;p&apos;: 1}


[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    8.6s finished</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">10</span>, p=<span class="number">1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">knn.fit(train_x, train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_Knn_prob = knn.predict(test_x)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_Knn_01 = np.where(predictions_Knn_prob &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) </span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_knn = accuracy_score(test_y, predictions_Knn_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of KNN model:'</span>, acc_knn)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y, predictions_Knn_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_Knn_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'KNN'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of KNN model: 0.8647342995169082


/usr/local/lib/python3.6/dist-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family [&apos;sans-serif&apos;] not found. Falling back to DejaVu Sans.
  (prop.get_family(), self.defaultFamily[fontext]))
/usr/local/lib/python3.6/dist-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family [&apos;sans-serif&apos;] not found. Falling back to DejaVu Sans.
  (prop.get_family(), self.defaultFamily[fontext]))
/usr/local/lib/python3.6/dist-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family [&apos;sans-serif&apos;] not found. Falling back to DejaVu Sans.
  (prop.get_family(), self.defaultFamily[fontext]))</code></pre><p><img src="/images/classification-models/output_19_2.png" alt="png"></p>
<p><img src="/images/classification-models/output_19_3.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.86      0.89      0.88       113
     class 1       0.87      0.83      0.85        94

   micro avg       0.86      0.86      0.86       207
   macro avg       0.86      0.86      0.86       207
weighted avg       0.86      0.86      0.86       207</code></pre><h1 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lda = LinearDiscriminantAnalysis()</span></pre></td></tr><tr><td class="code"><pre><span class="line">lda.fit(train_x, train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LDA_01 = lda.predict(test_x)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LDA_prob = lda.predict_proba(test_x)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LDA_prob = predictions_LDA_prob[:,<span class="number">1</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_lda = accuracy_score(test_y, predictions_LDA_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of LDA model:'</span>, acc_lda)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y, predictions_LDA_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_LDA_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'LDA'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of LDA model: 0.8357487922705314


/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn(&quot;Variables are collinear.&quot;)</code></pre><p><img src="/images/classification-models/output_22_2.png" alt="png"></p>
<p><img src="/images/classification-models/output_22_3.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.93      0.75      0.83       113
     class 1       0.76      0.94      0.84        94

   micro avg       0.84      0.84      0.84       207
   macro avg       0.85      0.84      0.84       207
weighted avg       0.85      0.84      0.84       207</code></pre><h1 id="NB"><a href="#NB" class="headerlink" title="NB"></a>NB</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> BernoulliNB, GaussianNB, MultinomialNB</span></pre></td></tr></table></figure>

<p>GaussianNB就是先验为高斯分布的朴素贝叶斯，<br>MultinomialNB就是先验为多项式分布的朴素贝叶斯，<br>而BernoulliNB就是先验为伯努利分布的朴素贝叶斯。<br>贝叶斯方法数据使用未经标准化的数据，保持原有数据的分布</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df1, test_df1 = train_test_split(df, test_size = <span class="number">0.3</span>, random_state= <span class="number">2019</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">train_y1 = train_df1.Default</span></pre></td></tr><tr><td class="code"><pre><span class="line">test_y1 = test_df1.Default</span></pre></td></tr><tr><td class="code"><pre><span class="line">train_x1 = train_df1.drop([<span class="string">'Default'</span>], axis = <span class="number">1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">test_x1 = test_df1.drop([<span class="string">'Default'</span>], axis = <span class="number">1</span>)</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classifiers = &#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'BNB'</span>: BernoulliNB(),</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'GNB'</span>: GaussianNB(),</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'MNB'</span>: MultinomialNB()&#125;</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> name, clf <span class="keyword">in</span> classifiers.items():</span></pre></td></tr><tr><td class="code"><pre><span class="line">    scores = cross_val_score(clf, train_x1, train_y1, cv=<span class="number">10</span>, scoring=<span class="string">'roc_auc'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(name,<span class="string">'\t--&gt; '</span>,scores.mean())</span></pre></td></tr></table></figure>

<pre><code>BNB     --&gt;  0.9022927689594358
GNB     --&gt;  0.9014189514189515
MNB     --&gt;  0.6811768478435146</code></pre><p>可见BernoulliNB在NB大类中相对较优</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf = BernoulliNB() </span></pre></td></tr><tr><td class="code"><pre><span class="line">clf.fit(train_x1, train_y1)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#Predict on test set</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_Naive_Bayes_prob = clf.predict_proba(test_x1)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_Naive_Bayes_prob = predictions_Naive_Bayes_prob[:,<span class="number">1</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_Naive_Bayes_01 = clf.predict(test_x1)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#Print accuracy</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_Naive = accuracy_score(test_y, predictions_Naive_Bayes_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Naive Bayes model:'</span>, acc_Naive)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y1, predictions_Naive_Bayes_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_Naive_Bayes_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'MNB'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of Naive Bayes model: 0.8357487922705314</code></pre><p><img src="/images/classification-models/output_30_1.png" alt="png"></p>
<p><img src="/images/classification-models/output_30_2.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.88      0.81      0.84       113
     class 1       0.79      0.87      0.83        94

   micro avg       0.84      0.84      0.84       207
   macro avg       0.84      0.84      0.84       207
weighted avg       0.84      0.84      0.84       207</code></pre><h1 id="LR"><a href="#LR" class="headerlink" title="LR"></a>LR</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span></pre></td></tr></table></figure>

<p>先不做特征处理，不做标准化，直接进行拟合 </p>
<p>参数选择：<br>正则化：l1,l2<br>正则化参数：C<br>样本权重：平衡权重，均匀权重</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = LogisticRegression()</span></pre></td></tr><tr><td class="code"><pre><span class="line">penalty  = [<span class="string">'l1'</span>,<span class="string">'l2'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">C = [<span class="number">1</span>,<span class="number">0.3</span>,<span class="number">0.1</span>,<span class="number">0.03</span>,<span class="number">0.01</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">class_weight = [<span class="string">'balanced'</span>,<span class="literal">None</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">param_gridknn = dict(penalty=penalty,C=C,class_weight=class_weight)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridLR = GridSearchCV(lr, param_gridknn, cv=<span class="number">10</span>, scoring=<span class="string">'roc_auc'</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridLR.fit(train_x1,train_y1)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'best params are:'</span>,str(gridLR.best_params_))</span></pre></td></tr></table></figure>

<pre><code>Fitting 10 folds for each of 20 candidates, totalling 200 fits


[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.
[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    5.0s


best params are: {&apos;C&apos;: 1, &apos;class_weight&apos;: None, &apos;penalty&apos;: &apos;l1&apos;}


[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    5.3s finished
/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logreg = LogisticRegression(class_weight=<span class="literal">None</span>,C=<span class="number">1</span>,penalty=<span class="string">'l1'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">logreg.fit(train_x1, train_y1)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_prob = logreg.predict_proba(test_x1)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_prob = predictions_LogReg_prob[:,<span class="number">1</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_01 = logreg.predict(test_x1)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_logit = accuracy_score(test_y1, predictions_LogReg_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Logistic Regression model:'</span>, acc_logit)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y1, predictions_LogReg_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y1, predictions_LogReg_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'lr'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)


Overall accuracy of Logistic Regression model: 0.8647342995169082</code></pre><p><img src="/images/classification-models/output_35_2.png" alt="png"></p>
<p><img src="/images/classification-models/output_35_3.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.93      0.81      0.87       113
     class 1       0.81      0.93      0.86        94

   micro avg       0.86      0.86      0.86       207
   macro avg       0.87      0.87      0.86       207
weighted avg       0.87      0.86      0.86       207</code></pre><p>对’A3’, ‘A7’, ‘A10’, ‘A13’, ‘A14’连续变量离散化后(全部等宽离散化为4组)，进行拟合<br>之所以选择’A3’, ‘A7’, ‘A10’, ‘A13’, ‘A14’使因为这些列的数据分布很不均匀，而A2较均匀。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.read_table(<span class="string">'australian.dat'</span>,sep=<span class="string">' '</span>, header=<span class="literal">None</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">df.columns = [<span class="string">'A1'</span>, <span class="string">'A2'</span>, <span class="string">'A3'</span>, <span class="string">'A4'</span>, <span class="string">'A5'</span>, <span class="string">'A6'</span>, <span class="string">'A7'</span>, <span class="string">'A8'</span>, <span class="string">'A9'</span>, <span class="string">'A10'</span>, <span class="string">'A11'</span>, <span class="string">'A12'</span>, <span class="string">'A13'</span>, <span class="string">'A14'</span>, <span class="string">'Default'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">df1 = df</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> [<span class="string">'A3'</span>, <span class="string">'A7'</span>, <span class="string">'A10'</span>, <span class="string">'A13'</span>, <span class="string">'A14'</span>]:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    df1[each] = pd.cut(df1[each],bins=<span class="number">4</span>,labels=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span></pre></td></tr><tr><td class="code"><pre><span class="line">df1 = pd.get_dummies(df1)</span></pre></td></tr><tr><td class="code"><pre><span class="line">train_df1, test_df1 = train_test_split(df1, test_size = <span class="number">0.3</span>, random_state= <span class="number">2019</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">train_y1 = train_df1.Default</span></pre></td></tr><tr><td class="code"><pre><span class="line">test_y1 = test_df1.Default</span></pre></td></tr><tr><td class="code"><pre><span class="line">train_x1 = train_df1.drop([<span class="string">'Default'</span>], axis = <span class="number">1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">test_x1 = test_df1.drop([<span class="string">'Default'</span>], axis = <span class="number">1</span>)</span></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead.
  &quot;&quot;&quot;Entry point for launching an IPython kernel.</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = LogisticRegression()</span></pre></td></tr><tr><td class="code"><pre><span class="line">penalty  = [<span class="string">'l1'</span>,<span class="string">'l2'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">C = [<span class="number">1</span>,<span class="number">0.3</span>,<span class="number">0.1</span>,<span class="number">0.03</span>,<span class="number">0.01</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">class_weight = [<span class="string">'balanced'</span>,<span class="literal">None</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">param_gridknn = dict(penalty=penalty,C=C,class_weight=class_weight)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridLR = GridSearchCV(lr, param_gridknn, cv=<span class="number">10</span>, scoring=<span class="string">'roc_auc'</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridLR.fit(train_x1,train_y1)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'best params are:'</span>,str(gridLR.best_params_))</span></pre></td></tr></table></figure>

<pre><code>Fitting 10 folds for each of 20 candidates, totalling 200 fits


[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    6.1s
[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    6.7s finished


best params are: {&apos;C&apos;: 1, &apos;class_weight&apos;: None, &apos;penalty&apos;: &apos;l1&apos;}</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logreg = LogisticRegression(class_weight=<span class="literal">None</span>,C=<span class="number">1</span>,penalty=<span class="string">'l1'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">logreg.fit(train_x1, train_y1)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_prob = logreg.predict_proba(test_x1)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_prob = predictions_LogReg_prob[:,<span class="number">1</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_01 = logreg.predict(test_x1)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_logit = accuracy_score(test_y1, predictions_LogReg_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Logistic Regression model:'</span>, acc_logit)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y1, predictions_LogReg_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y1, predictions_LogReg_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'lr1'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)


Overall accuracy of Logistic Regression model: 0.855072463768116</code></pre><p><img src="/images/classification-models/output_39_2.png" alt="png"></p>
<p><img src="/images/classification-models/output_39_3.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.93      0.80      0.86       113
     class 1       0.79      0.93      0.85        94

   micro avg       0.86      0.86      0.86       207
   macro avg       0.86      0.86      0.86       207
weighted avg       0.87      0.86      0.86       207</code></pre><p>特征离散化和不离散化结果几乎一致，没有特别的差距，应该是数据集的特点</p>
<h1 id="Decision-Tree-：Cart"><a href="#Decision-Tree-：Cart" class="headerlink" title="Decision Tree ：Cart"></a>Decision Tree ：Cart</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span></pre></td></tr></table></figure>

<p>参数选择：<br>树深：max_depth  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dt = DecisionTreeClassifier()</span></pre></td></tr><tr><td class="code"><pre><span class="line">max_depth = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>, <span class="literal">None</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">param_griddt = dict(max_depth = max_depth)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridKNN = GridSearchCV(dt, param_griddt, cv=<span class="number">10</span>, scoring=<span class="string">'roc_auc'</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridKNN.fit(train_x,train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'best params are:'</span>,str(gridKNN.best_params_))</span></pre></td></tr></table></figure>

<pre><code>Fitting 10 folds for each of 5 candidates, totalling 50 fits
best params are: {&apos;max_depth&apos;: 2}


[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    5.7s finished</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dt = DecisionTreeClassifier(max_depth=<span class="number">2</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">dt.fit(train_x, train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_dt_prob = dt.predict(test_x)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_dt_01 = np.where(predictions_dt_prob &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) </span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_dt = accuracy_score(test_y, predictions_dt_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Decision Tree model:'</span>, acc_dt)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y, predictions_dt_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_dt_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'DT'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of Decision Tree model: 0.8405797101449275</code></pre><p><img src="/images/classification-models/output_45_1.png" alt="png"></p>
<p><img src="/images/classification-models/output_45_2.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.94      0.75      0.84       113
     class 1       0.76      0.95      0.84        94

   micro avg       0.84      0.84      0.84       207
   macro avg       0.85      0.85      0.84       207
weighted avg       0.86      0.84      0.84       207</code></pre><h1 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span></pre></td></tr></table></figure>

<p>参数选择：<br>核函数：多项式，sigmoid，高斯核函数，线性核函数，线性核函数<br>gamma:核函数系数<br>C：正则项系数 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">svm = SVC()</span></pre></td></tr><tr><td class="code"><pre><span class="line">param_gridsvm = [&#123;<span class="string">'kernel'</span>: [<span class="string">'poly'</span>, <span class="string">'rbf'</span>, <span class="string">'sigmoid'</span>], <span class="string">'gamma'</span>: [<span class="number">0.001</span>,<span class="number">0.003</span>,<span class="number">0.01</span>,<span class="number">0.03</span>,<span class="number">0.1</span>,<span class="number">1</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">                     <span class="string">'C'</span>: [<span class="number">0.01</span>,<span class="number">0.03</span>,<span class="number">0.1</span>,<span class="number">1</span>, <span class="number">3</span>,<span class="number">10</span>,<span class="number">30</span>, <span class="number">100</span>,<span class="number">300</span>, <span class="number">1000</span>]&#125;,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                    &#123;<span class="string">'kernel'</span>: [<span class="string">'linear'</span>], <span class="string">'C'</span>: [<span class="number">0.01</span>,<span class="number">0.03</span>,<span class="number">0.1</span>,<span class="number">1</span>, <span class="number">3</span>,<span class="number">10</span>,<span class="number">30</span>, <span class="number">100</span>,<span class="number">300</span>, <span class="number">1000</span>]&#125;]</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridsvm = GridSearchCV(svm, param_gridsvm, cv=<span class="number">10</span>, scoring=<span class="string">'roc_auc'</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridsvm.fit(train_x,train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'best params are:'</span>,str(gridsvm.best_params_))</span></pre></td></tr></table></figure>

<pre><code>Fitting 10 folds for each of 190 candidates, totalling 1900 fits


[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.
[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:    6.3s
[Parallel(n_jobs=-1)]: Done 1156 tasks      | elapsed:   25.0s


best params are: {&apos;C&apos;: 0.1, &apos;gamma&apos;: 0.1, &apos;kernel&apos;: &apos;poly&apos;}


[Parallel(n_jobs=-1)]: Done 1900 out of 1900 | elapsed:  1.9min finished
/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.
  DeprecationWarning)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">svm = SVC(C=<span class="number">0.1</span>, gamma=<span class="number">0.1</span>, kernel=<span class="string">'poly'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">svm.fit(train_x, train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_svm_prob = svm.predict(test_x)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_svm_01 = np.where(predictions_svm_prob &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) </span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_svm = accuracy_score(test_y, predictions_svm_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of SVM model:'</span>, acc_knn)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y, predictions_svm_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_svm_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'SVM'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of SVM model: 0.8647342995169082</code></pre><p><img src="/images/classification-models/output_50_1.png" alt="png"></p>
<p><img src="/images/classification-models/output_50_2.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.87      0.86      0.87       113
     class 1       0.83      0.85      0.84        94

   micro avg       0.86      0.86      0.86       207
   macro avg       0.85      0.85      0.85       207
weighted avg       0.86      0.86      0.86       207</code></pre><h1 id="RF"><a href="#RF" class="headerlink" title="RF"></a>RF</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span></pre></td></tr></table></figure>

<p>参数选择：<br>n_estimaors: 森林中树的个数<br>max_depth: 最大树深  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rf = RandomForestClassifier()</span></pre></td></tr><tr><td class="code"><pre><span class="line">n_estimators = list(range(<span class="number">5</span>,<span class="number">101</span>,<span class="number">10</span>))</span></pre></td></tr><tr><td class="code"><pre><span class="line">max_depth = list(range(<span class="number">2</span>,<span class="number">20</span>,<span class="number">2</span>))</span></pre></td></tr><tr><td class="code"><pre><span class="line">param_gridrf = dict(n_estimators = n_estimators, max_depth=max_depth)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridRF = GridSearchCV(rf, param_gridrf, cv=<span class="number">10</span>, scoring=<span class="string">'roc_auc'</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridRF.fit(train_x,train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'best params are:'</span>,str(gridRF.best_params_))</span></pre></td></tr></table></figure>

<pre><code>Fitting 10 folds for each of 90 candidates, totalling 900 fits


[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    7.2s
[Parallel(n_jobs=-1)]: Done 348 tasks      | elapsed:   18.9s
[Parallel(n_jobs=-1)]: Done 848 tasks      | elapsed:   37.3s


best params are: {&apos;max_depth&apos;: 10, &apos;n_estimators&apos;: 95}


[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed:   40.0s finished</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rf = RandomForestClassifier(n_estimators=<span class="number">95</span>, max_depth=<span class="number">10</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">rf.fit(train_x, train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_rf_prob = rf.predict(test_x)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_rf_01 = np.where(predictions_rf_prob &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) </span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_rf = accuracy_score(test_y, predictions_rf_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Random Forest model:'</span>, acc_rf)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y, predictions_rf_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_rf_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'RF'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of Random Forest model: 0.855072463768116</code></pre><p><img src="/images/classification-models/output_55_1.png" alt="png"></p>
<p><img src="/images/classification-models/output_55_2.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.90      0.82      0.86       113
     class 1       0.81      0.89      0.85        94

   micro avg       0.86      0.86      0.86       207
   macro avg       0.86      0.86      0.85       207
weighted avg       0.86      0.86      0.86       207</code></pre><h1 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span></pre></td></tr></table></figure>

<p>参数选择：<br>学习率<br>弱学习器的个数<br>最大树深<br>样本采样比列<br>叶子权重正则系数<br>叶子个数正则系数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">XGB = xgb.XGBClassifier()</span></pre></td></tr><tr><td class="code"><pre><span class="line">param_gridxgb = dict(</span></pre></td></tr><tr><td class="code"><pre><span class="line">    learning_rate = [<span class="number">1</span>,<span class="number">0.1</span>,<span class="number">0.01</span>], </span></pre></td></tr><tr><td class="code"><pre><span class="line">    n_estimators = [<span class="number">8</span>,<span class="number">16</span>,<span class="number">32</span>,<span class="number">64</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    max_depth = [<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    subsample = [<span class="number">1</span>,<span class="number">0.8</span>,<span class="number">0.6</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    reg_alpha = [<span class="number">1</span>,<span class="number">0.1</span>,<span class="number">0</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    reg_lambda = [<span class="number">1</span>,<span class="number">0.1</span>,<span class="number">0</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridxgb = GridSearchCV(XGB, param_gridxgb, cv=<span class="number">5</span>, scoring=<span class="string">'roc_auc'</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridxgb.fit(train_x,train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'best params are:'</span>,str(gridxgb.best_params_))</span></pre></td></tr></table></figure>

<pre><code>Fitting 5 folds for each of 1620 candidates, totalling 8100 fits


[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.
[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:    4.6s
[Parallel(n_jobs=-1)]: Done 1481 tasks      | elapsed:   37.6s
[Parallel(n_jobs=-1)]: Done 3481 tasks      | elapsed:  1.6min
[Parallel(n_jobs=-1)]: Done 5277 tasks      | elapsed:  2.7min
[Parallel(n_jobs=-1)]: Done 7077 tasks      | elapsed:  3.8min


best params are: {&apos;learning_rate&apos;: 0.1, &apos;max_depth&apos;: 4, &apos;n_estimators&apos;: 32, &apos;reg_alpha&apos;: 0, &apos;reg_lambda&apos;: 0.1, &apos;subsample&apos;: 0.6}


[Parallel(n_jobs=-1)]: Done 8100 out of 8100 | elapsed:  4.5min finished</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gridxgb.best_params_</span></pre></td></tr></table></figure>




<pre><code>{&apos;learning_rate&apos;: 0.1,
 &apos;max_depth&apos;: 4,
 &apos;n_estimators&apos;: 32,
 &apos;reg_alpha&apos;: 0,
 &apos;reg_lambda&apos;: 0.1,
 &apos;subsample&apos;: 0.6}</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">% matplotlib inline</span></pre></td></tr><tr><td class="code"><pre><span class="line">xgdmat = xgb.DMatrix(train_x, train_y) <span class="comment"># Create our DMatrix to make XGBoost more efficient</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">testdmat = xgb.DMatrix(test_x)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># final_gb = xgb.train(gridxgb.best_params_, xgdmat)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># predictions_xgb_prob = final_gb.predict(testdmat)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># predictions_xgb_01 = np.where(predictions_xgb_prob &gt; 0.5, 1, 0) #Turn probability to 0-1 binary output</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># xgb.plot_importance(final_gb)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">trainxdmat = xgb.DMatrix(train_x) <span class="comment"># Create our DMatrix to make XGBoost more efficient</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">trainydmat = xgb.DMatrix(train_y) <span class="comment"># Create our DMatrix to make XGBoost more efficient</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">XGB = xgb.XGBClassifier(learning_rate=<span class="number">0.1</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                        max_depth=<span class="number">4</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                        n_estimators= <span class="number">32</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                        reg_alpha= <span class="number">0</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                        reg_lambda= <span class="number">0.1</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                        subsample= <span class="number">0.6</span>,                    </span></pre></td></tr><tr><td class="code"><pre><span class="line">                       )</span></pre></td></tr><tr><td class="code"><pre><span class="line">XGB.fit(train_x.values,train_y.values)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'1'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_xgb_prob = XGB.predict(test_x.values)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_xgb_01 = np.where(predictions_xgb_prob &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) <span class="comment">#Turn probability to 0-1 binary output</span></span></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version
  if getattr(data, &apos;base&apos;, None) is not None and \
/usr/local/lib/python3.6/dist-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version
  data.base is not None and isinstance(data, np.ndarray) \


1</code></pre><p><img src="/images/classification-models/output_61_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">acc_lgbm = accuracy_score(test_y,predictions_xgb_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Light GBM model:'</span>, acc_lgbm)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y, predictions_rf_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_rf_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'XGB'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of Light GBM model: 0.8647342995169082</code></pre><p><img src="/images/classification-models/output_62_1.png" alt="png"></p>
<p><img src="/images/classification-models/output_62_2.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.90      0.82      0.86       113
     class 1       0.81      0.89      0.85        94

   micro avg       0.86      0.86      0.86       207
   macro avg       0.86      0.86      0.85       207
weighted avg       0.86      0.86      0.86       207</code></pre><h1 id="LightGBM（GBDT）"><a href="#LightGBM（GBDT）" class="headerlink" title="LightGBM（GBDT）"></a>LightGBM（GBDT）</h1><p>先单纯使用GBDT</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data=lgb.Dataset(train_x,label=train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">params = &#123;<span class="string">'boosting_type'</span>: <span class="string">'gbdt'</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'max_depth'</span> : <span class="number">-1</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'objective'</span>: <span class="string">'binary'</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'nthread'</span>: <span class="number">5</span>, <span class="comment"># Updated from nthread</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'num_leaves'</span>: <span class="number">64</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'learning_rate'</span>: <span class="number">0.05</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'max_bin'</span>: <span class="number">512</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'subsample_for_bin'</span>: <span class="number">200</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'subsample'</span>: <span class="number">1</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'subsample_freq'</span>: <span class="number">1</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'colsample_bytree'</span>: <span class="number">0.8</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'reg_alpha'</span>: <span class="number">5</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'reg_lambda'</span>: <span class="number">10</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'min_split_gain'</span>: <span class="number">0.5</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'min_child_weight'</span>: <span class="number">1</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'min_child_samples'</span>: <span class="number">5</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'scale_pos_weight'</span>: <span class="number">1</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'num_class'</span> : <span class="number">1</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          <span class="string">'metric'</span> : <span class="string">'binary_error'</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">          &#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">gridParams = &#123;</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'learning_rate'</span>: [<span class="number">1</span>,<span class="number">0.1</span>,<span class="number">0.01</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'n_estimators'</span>: [<span class="number">8</span>,<span class="number">16</span>,<span class="number">32</span>,<span class="number">64</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'num_leaves'</span>: [<span class="number">50</span>,<span class="number">100</span>,<span class="number">150</span>,<span class="number">200</span>,<span class="number">250</span>,<span class="number">300</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'boosting_type'</span> : [<span class="string">'gbdt'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'objective'</span> : [<span class="string">'binary'</span>], </span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'colsample_bytree'</span> : [<span class="number">1</span>,<span class="number">0.8</span>,<span class="number">0.6</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'subsample'</span> : [<span class="number">1</span>,<span class="number">0.8</span>,<span class="number">0.6</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'reg_alpha'</span> : [<span class="number">10</span>,<span class="number">3</span>,<span class="number">1</span>, <span class="number">0.1</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="string">'reg_lambda'</span> : [<span class="number">10</span>,<span class="number">3</span>,<span class="number">1</span>, <span class="number">0.1</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">mdl = lgb.LGBMClassifier(boosting_type= <span class="string">'gbdt'</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          objective = <span class="string">'binary'</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          n_jobs = <span class="number">5</span>, <span class="comment"># Updated from 'nthread'</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">          silent = <span class="literal">True</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">          max_depth = params[<span class="string">'max_depth'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">          max_bin = params[<span class="string">'max_bin'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">          subsample_for_bin = params[<span class="string">'subsample_for_bin'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">          subsample = params[<span class="string">'subsample'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">          subsample_freq = params[<span class="string">'subsample_freq'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">          min_split_gain = params[<span class="string">'min_split_gain'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">          min_child_weight = params[<span class="string">'min_child_weight'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">          min_child_samples = params[<span class="string">'min_child_samples'</span>],</span></pre></td></tr><tr><td class="code"><pre><span class="line">          scale_pos_weight = params[<span class="string">'scale_pos_weight'</span>])</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">mdl.get_params().keys()</span></pre></td></tr></table></figure>




<pre><code>dict_keys([&apos;boosting_type&apos;, &apos;class_weight&apos;, &apos;colsample_bytree&apos;, &apos;importance_type&apos;, &apos;learning_rate&apos;, &apos;max_depth&apos;, &apos;min_child_samples&apos;, &apos;min_child_weight&apos;, &apos;min_split_gain&apos;, &apos;n_estimators&apos;, &apos;n_jobs&apos;, &apos;num_leaves&apos;, &apos;objective&apos;, &apos;random_state&apos;, &apos;reg_alpha&apos;, &apos;reg_lambda&apos;, &apos;silent&apos;, &apos;subsample&apos;, &apos;subsample_for_bin&apos;, &apos;subsample_freq&apos;, &apos;max_bin&apos;, &apos;scale_pos_weight&apos;])</code></pre><p>这里用的4折交叉验证，因为参数实在太多，跑不动，太慢了，其实用10折比较好<br>后来把正则项系数改大了，因为后面发现过拟合了，但是还没跑，就用原来的参数吧；或者你有时间就直接运行一下就可以</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grid = GridSearchCV(mdl, gridParams, verbose=<span class="number">1</span>, cv=<span class="number">4</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">grid.fit(train_x, train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(grid.best_params_)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(grid.best_score_)</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">% matplotlib inline</span></pre></td></tr><tr><td class="code"><pre><span class="line">best_p = &#123;<span class="string">'boosting_type'</span>: <span class="string">'gbdt'</span>, <span class="string">'colsample_bytree'</span>: <span class="number">0.6</span>, <span class="string">'learning_rate'</span>: <span class="number">1</span>, <span class="string">'n_estimators'</span>: <span class="number">16</span>, <span class="string">'num_leaves'</span>: <span class="number">50</span>, <span class="string">'objective'</span>: <span class="string">'binary'</span>, <span class="string">'reg_alpha'</span>: <span class="number">0.01</span>, <span class="string">'reg_lambda'</span>: <span class="number">1</span>, <span class="string">'subsample'</span>: <span class="number">1</span>&#125;</span></pre></td></tr><tr><td class="code"><pre><span class="line">lgbm = lgb.train(best_p,<span class="comment">#grid.best_params_,</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">                 train_data,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                 <span class="number">2500</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                 verbose_eval= <span class="number">4</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">                 )</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_lgbm_prob = lgbm.predict(test_x)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_lgbm_01 = np.where(predictions_lgbm_prob &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) </span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">lgb.plot_importance(lgbm, max_num_features=<span class="number">21</span>, importance_type=<span class="string">'split'</span>)</span></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument
  warnings.warn(&quot;Found `{}` in params. Will use it instead of argument&quot;.format(alias))





&lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff7884feb00&gt;</code></pre><p><img src="/images/classification-models/output_69_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">acc_lgbm = accuracy_score(test_y,predictions_lgbm_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Light GBM model:'</span>, acc_lgbm)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y, predictions_lgbm_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_lgbm_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'GBDT'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of Light GBM model: 0.8695652173913043</code></pre><p><img src="/images/classification-models/output_70_1.png" alt="png"></p>
<p><img src="/images/classification-models/output_70_2.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.92      0.83      0.87       113
     class 1       0.82      0.91      0.86        94

   micro avg       0.87      0.87      0.87       207
   macro avg       0.87      0.87      0.87       207
weighted avg       0.88      0.87      0.87       207</code></pre><h1 id="GBDT＋LR"><a href="#GBDT＋LR" class="headerlink" title="GBDT＋LR"></a>GBDT＋LR</h1><p>1.通过GBDT进行特征转换(为了发现有效的特征和特征组合): 判断样本落在哪个叶子节点上,每个叶子节点作为lr的一维特征,通过遍历树，就得到了该样本对应的所有LR特征。构造的新特征向量是取值onehot化的<br>2.转换得到新的特征后，用lr分类  </p>
<p>继续使用上面GBDT模型，得到新的特征</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_pred = lgbm.predict(train_x, pred_leaf=<span class="literal">True</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># y_pred = final_gb.predict(testdmat,pred_leaf=True)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">y_pred.shape</span></pre></td></tr><tr><td class="code"><pre><span class="line">num_leaf = max([max(each) <span class="keyword">for</span> each <span class="keyword">in</span> y_pred])</span></pre></td></tr><tr><td class="code"><pre><span class="line">num_leaf</span></pre></td></tr></table></figure>




<pre><code>17</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'Writing transformed training data'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># num_leaf = grid.best_params_['num_leaves']</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">transformed_training_matrix = np.zeros([len(y_pred), len(y_pred[<span class="number">0</span>]) * num_leaf],</span></pre></td></tr><tr><td class="code"><pre><span class="line">                                       dtype=np.int64)  <span class="comment"># N * num_tress * num_leafs</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(y_pred)):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    temp = np.arange(len(y_pred[<span class="number">0</span>])) * num_leaf + np.array(y_pred[i])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    transformed_training_matrix[i][temp] += <span class="number">1</span></span></pre></td></tr></table></figure>

<pre><code>Writing transformed training data</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_pred = lgbm.predict(test_x, pred_leaf=<span class="literal">True</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Writing transformed testing data'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">transformed_testing_matrix = np.zeros([len(y_pred), len(y_pred[<span class="number">0</span>]) * num_leaf], dtype=np.int64)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(y_pred)):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    temp = np.arange(len(y_pred[<span class="number">0</span>])) * num_leaf + np.array(y_pred[i])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    transformed_testing_matrix[i][temp] += <span class="number">1</span></span></pre></td></tr></table></figure>

<pre><code>Writing transformed testing data</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transformed_testing_matrix.shape,transformed_training_matrix.shape</span></pre></td></tr></table></figure>




<pre><code>((207, 272), (483, 272))</code></pre><p>训练集和测试集新的特征的维度</p>
<p>将GBDT换成XGoost，通过XGboost进行特征转换</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_pred = XGB.apply(train_x.values)</span></pre></td></tr><tr><td class="code"><pre><span class="line">num_leaf = max([max(each) <span class="keyword">for</span> each <span class="keyword">in</span> y_pred])</span></pre></td></tr><tr><td class="code"><pre><span class="line">y_pred[<span class="number">13</span>]</span></pre></td></tr></table></figure>




<pre><code>array([23, 25, 23, 17, 21, 24, 25, 22, 25,  6, 21, 22, 21, 19, 14, 12, 23,
       12, 12, 22,  7, 14, 22, 11, 13, 14, 19, 11, 16, 12, 21, 16],
      dtype=int32)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Writing transformed training data'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># num_leaf = grid.best_params_['num_leaves']</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">transformed_training_matrix = np.zeros([len(y_pred), len(y_pred[<span class="number">0</span>]) * num_leaf],</span></pre></td></tr><tr><td class="code"><pre><span class="line">                                       dtype=np.int64)  <span class="comment"># N * num_tress * num_leafs</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(y_pred)):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    temp = np.arange(len(y_pred[<span class="number">0</span>])) * num_leaf + np.array(y_pred[i])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    transformed_training_matrix[i][temp] += <span class="number">1</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="code"><pre><span class="line">y_pred = XGB.apply(test_x.values)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Writing transformed testing data'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">transformed_testing_matrix = np.zeros([len(y_pred), len(y_pred[<span class="number">0</span>]) * num_leaf], dtype=np.int64)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(y_pred)):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    temp = np.arange(len(y_pred[<span class="number">0</span>])) * num_leaf + np.array(y_pred[i])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    transformed_testing_matrix[i][temp] += <span class="number">1</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">transformed_testing_matrix.shape,transformed_training_matrix.shape</span></pre></td></tr></table></figure>

<pre><code>Writing transformed training data
Writing transformed testing data





((207, 832), (483, 832))</code></pre><p>用新的特征进行lr分类,先做参数选择</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = LogisticRegression()</span></pre></td></tr><tr><td class="code"><pre><span class="line">penalty  = [<span class="string">'l1'</span>,<span class="string">'l2'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">C = [<span class="number">10</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">0.3</span>,<span class="number">0.1</span>,<span class="number">0.03</span>,<span class="number">0.01</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">class_weight = [<span class="string">'balanced'</span>,<span class="literal">None</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">param_gridknn = dict(penalty=penalty,C=C,class_weight=class_weight)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridLR = GridSearchCV(lr, param_gridknn, cv=<span class="number">10</span>, scoring=<span class="string">'roc_auc'</span>,verbose=<span class="number">1</span>, n_jobs=<span class="number">-1</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">gridLR.fit(transformed_training_matrix,train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'best params are:'</span>,str(gridLR.best_params_))</span></pre></td></tr></table></figure>

<pre><code>Fitting 10 folds for each of 28 candidates, totalling 280 fits


[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.


best params are: {&apos;C&apos;: 0.3, &apos;class_weight&apos;: &apos;balanced&apos;, &apos;penalty&apos;: &apos;l2&apos;}


[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed:    5.3s finished
/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logreg = LogisticRegression(class_weight=<span class="string">'balanced'</span>,C=<span class="number">0.3</span>,penalty=<span class="string">'l2'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">logreg.fit(transformed_training_matrix,train_y)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_prob = logreg.predict_proba(transformed_testing_matrix)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_prob = predictions_LogReg_prob[:,<span class="number">1</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_LogReg_01 = logreg.predict(transformed_testing_matrix)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_logit = accuracy_score(test_y, predictions_LogReg_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Logistic Regression model:'</span>, acc_logit)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(test_y, predictions_LogReg_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(test_y, predictions_LogReg_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">RESULT[<span class="string">'XGB_lr'</span>] = r</span></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)


Overall accuracy of Logistic Regression model: 0.8599033816425121</code></pre><p><img src="/images/classification-models/output_83_2.png" alt="png"></p>
<p><img src="/images/classification-models/output_83_3.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.92      0.81      0.86       113
     class 1       0.80      0.91      0.86        94

   micro avg       0.86      0.86      0.86       207
   macro avg       0.86      0.86      0.86       207
weighted avg       0.87      0.86      0.86       207</code></pre><h1 id="结果汇总"><a href="#结果汇总" class="headerlink" title="结果汇总"></a>结果汇总</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_name = RESULT.keys()</span></pre></td></tr><tr><td class="code"><pre><span class="line">model_preformance = RESULT.values()</span></pre></td></tr><tr><td class="code"><pre><span class="line">preformance1 = [<span class="string">'recall'</span>,<span class="string">'precision'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">preformance2 = [<span class="string">'acc'</span>,<span class="string">'f1'</span>,<span class="string">'auc'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">xx = list(range(len(model_name)))</span></pre></td></tr><tr><td class="code"><pre><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">8.0</span>, <span class="number">4.0</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i,each <span class="keyword">in</span> enumerate(preformance1):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.plot(xx,[each[i] <span class="keyword">for</span> each <span class="keyword">in</span> model_preformance])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.title(<span class="string">'model compare'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xlabel(<span class="string">'model name'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.ylabel(<span class="string">'performance'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xticks(xx,model_name)</span></pre></td></tr><tr><td class="code"><pre><span class="line">plt.legend(preformance1)</span></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.legend.Legend at 0x7ff754f29dd8&gt;</code></pre><p><img src="/images/classification-models/output_85_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i,each <span class="keyword">in</span> enumerate(preformance2):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.plot(xx,[each[i] <span class="keyword">for</span> each <span class="keyword">in</span> model_preformance])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.title(<span class="string">'model compare'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xlabel(<span class="string">'model name'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.ylabel(<span class="string">'performance'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xticks(xx,model_name)</span></pre></td></tr><tr><td class="code"><pre><span class="line">plt.legend(preformance2)</span></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.legend.Legend at 0x7ff754f6ecf8&gt;</code></pre><p><img src="/images/classification-models/output_86_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_name = RESULT.keys()</span></pre></td></tr><tr><td class="code"><pre><span class="line">model_preformance = RESULT.values()</span></pre></td></tr><tr><td class="code"><pre><span class="line">preformance1 = [<span class="string">'recall'</span>,<span class="string">'precision'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">preformance2 = [<span class="string">'acc'</span>,<span class="string">'f1'</span>,<span class="string">'auc'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">preformance = preformance1 + preformance2</span></pre></td></tr><tr><td class="code"><pre><span class="line">xx = list(range(len(model_name)))</span></pre></td></tr><tr><td class="code"><pre><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">10.0</span>, <span class="number">6.0</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i,each <span class="keyword">in</span> enumerate(preformance):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.plot(xx,[each[i] <span class="keyword">for</span> each <span class="keyword">in</span> model_preformance])</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.title(<span class="string">'model compare'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xlabel(<span class="string">'model name'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.ylabel(<span class="string">'performance'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    plt.xticks(xx,model_name)</span></pre></td></tr><tr><td class="code"><pre><span class="line">plt.legend(preformance)</span></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.legend.Legend at 0x7ff754c2b0b8&gt;</code></pre><p><img src="/images/classification-models/output_87_1.png" alt="png"></p>
<p>可以着重展示一下SVM,LR,GBDT,GBDT+LR,XGBoost+LR  ,因为参数比较多,调参过程选择多一些<br>后面三个模型逻辑是这样的 :<br>LR 首先连续变量不做离散化直接拟合,之后选择一些连续变量做离散化再做拟合,发现模型表现没有提升,<br>于是想试试更多特征的表现,<br>于是通过GBDT（以每个叶子节点作为一个特征）进行特征转换，组合得到更多的特征，进行lr分类.<br>最后将GBDT换成ＸＧｂｏｏｓｔ做尝试</p>
<p>再使用GBDT或XGboost特征后，lr相比之前有了较大的改善  </p>
<p>但是相比GDBT本身却下降了,可能因为数据量的问题,出现了过拟合</p>
<p>XGboost，有小幅上升</p>
<p>进行探索，查看GBDT和XGBoost在训练集上的表现，发现GBDT已经完全拟合了训练集，确实存在过拟合现象，而XGBoost过拟合不严重</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions_lgbm_prob = XGB.predict(train_x.values)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_lgbm_01 = np.where(predictions_lgbm_prob &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) </span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_lgbm = accuracy_score(train_y,predictions_lgbm_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Light GBM model:'</span>, acc_lgbm)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(train_y, predictions_lgbm_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(train_y, predictions_lgbm_01)</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of Light GBM model: 0.9420289855072463</code></pre><p><img src="/images/classification-models/output_91_1.png" alt="png"></p>
<p><img src="/images/classification-models/output_91_2.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       0.95      0.95      0.95       270
     class 1       0.93      0.93      0.93       213

   micro avg       0.94      0.94      0.94       483
   macro avg       0.94      0.94      0.94       483
weighted avg       0.94      0.94      0.94       483</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions_lgbm_prob = lgbm.predict(train_x.values)</span></pre></td></tr><tr><td class="code"><pre><span class="line">predictions_lgbm_01 = np.where(predictions_lgbm_prob &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>) </span></pre></td></tr><tr><td class="code"><pre><span class="line">acc_lgbm = accuracy_score(train_y,predictions_lgbm_01)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">'Overall accuracy of Light GBM model:'</span>, acc_lgbm)</span></pre></td></tr><tr><td class="code"><pre><span class="line">print_roc_curve(train_y, predictions_lgbm_prob)</span></pre></td></tr><tr><td class="code"><pre><span class="line">r = print_confusion_matrix(train_y, predictions_lgbm_01)</span></pre></td></tr></table></figure>

<pre><code>Overall accuracy of Light GBM model: 0.9979296066252588</code></pre><p><img src="/images/classification-models/output_92_1.png" alt="png"></p>
<p><img src="/images/classification-models/output_92_2.png" alt="png"></p>
<pre><code>              precision    recall  f1-score   support

     class 0       1.00      1.00      1.00       270
     class 1       1.00      1.00      1.00       213

   micro avg       1.00      1.00      1.00       483
   macro avg       1.00      1.00      1.00       483
weighted avg       1.00      1.00      1.00       483</code></pre>]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo 博客常用操作命令</title>
    <url>/hexo-command/</url>
    <content><![CDATA[<p>可参考官网详细教程</p>
<pre><code>https://hexo.io/zh-cn/docs/</code></pre><a id="more"></a>

<p>hexo安装以后，可以使用以下两种方式执行 Hexo：  </p>
<ol>
<li><code>npx hexo &lt;command&gt;</code>  </li>
<li>将 Hexo 所在的目录下的 node_modules 添加到环境变量之中即可直接使用 <code>hexo &lt;command&gt;</code>：<br> <code>echo &#39;PATH=&quot;$PATH:./node_modules/.bin&quot;&#39; &gt;&gt; ~/.profile</code></li>
</ol>
<h3 id="new"><a href="#new" class="headerlink" title="new"></a>new</h3><p>新建内容。如果没有设置 layout 的话，默认使用 _config.yml 中的 <code>default_layout</code> 参数代替。如果标题包含空格的话，请使用引号括起来。</p>
<pre><code>$ hexo new [layout] &lt;title&gt;
$ hexo new &quot;post title with whitespace&quot;</code></pre><h3 id="generate"><a href="#generate" class="headerlink" title="generate"></a>generate</h3><p>生成静态文件:</p>
<pre><code>$ hexo generate
$ hexo g</code></pre><h3 id="publish"><a href="#publish" class="headerlink" title="publish"></a>publish</h3><p>草稿发布:</p>
<pre><code>$ hexo publish [layout] &lt;filename&gt;</code></pre><h3 id="server"><a href="#server" class="headerlink" title="server"></a>server</h3><p>开启服务:</p>
<pre><code>$ hexo server
$ hexo s</code></pre><h3 id="deploy"><a href="#deploy" class="headerlink" title="deploy"></a>deploy</h3><p>部署网站:</p>
<pre><code>$ hexo deploy
$ hexo d</code></pre><h3 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h3><p>本地开启服务：</p>
<pre><code>$ npx hexo g &amp; npx hexo s</code></pre><p>上传部署：</p>
<pre><code>$ npx hexo g &amp; npx hexo s</code></pre>]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>NLTK + Stanford NLP 进行命名实体识别和词性标注</title>
    <url>/stanford-nltk/</url>
    <content><![CDATA[<p>NLTK 中使用 Stanford NLP 工具包进行NER和POS任务。</p>
<a id="more"></a>

<h1 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h1><ol>
<li>下载<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;nlp.stanford.edu&#x2F;software&#x2F;CRF-NER.html</span></pre></td></tr><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;nlp.stanford.edu&#x2F;software&#x2F;tagger.html</span></pre></td></tr></table></figure></li>
<li>解压  <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">unzip stanford-ner-2018-10-16.zip</span></pre></td></tr><tr><td class="code"><pre><span class="line">unzip stanford-postagger-full-2018-10-16.zip</span></pre></td></tr></table></figure></li>
<li>添加 <code>CLASSPATH</code> ，修改 <code>.bashrc</code> 文件:<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> STANFORD_NLTK_PATH=/home/haha/stanford_nltk  </span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> STANFORD_NER_PATH=<span class="variable">$STANFORD_NLTK_PATH</span>/stanford-ner-2018-10-16</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> STANFORD_POS_PATH=<span class="variable">$STANFORD_NLTK_PATH</span>/stanford-postagger-full-2018-10-16</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/home/haha/java/jdk-13.0.1  </span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span>  </span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.jar:<span class="variable">$STANFORD_NER_PATH</span>/stanford-ner.jar:<span class="variable">$STANFORD_POS_PATH</span>/stanford-postagger.jar</span></pre></td></tr></table></figure></li>
<li>添加 <code>STANFORD_MODELS</code> ，修改 <code>.bashrc</code> 文件:<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> STANFORD_MODELS=<span class="variable">$STANFORD_NER_PATH</span>/classifiers:<span class="variable">$STANFORD_POS_PATH</span>/models</span></pre></td></tr></table></figure>

</li>
</ol>
<h1 id="函数使用"><a href="#函数使用" class="headerlink" title="函数使用"></a>函数使用</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tag <span class="keyword">import</span> StanfordNERTagger</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tag <span class="keyword">import</span> StanfordPOSTagger</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">pos_tagger = StanfordPOSTagger(<span class="string">'english-bidirectional-distsim.tagger'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">ner_tagger = StanfordNERTagger(<span class="string">'english.all.3class.distsim.crf.ser.gz'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">tokens = [<span class="string">'The'</span>, <span class="string">'suspect'</span>, <span class="string">'dumped'</span>, <span class="string">'the'</span>, <span class="string">'dead'</span>, <span class="string">'body'</span>, <span class="string">'into'</span>, <span class="string">'a'</span>, <span class="string">'local'</span>, <span class="string">'reservoir'</span>, <span class="string">'.'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">pos = [each[<span class="number">1</span>] <span class="keyword">for</span> each <span class="keyword">in</span> pos_tagger.tag(tokens)]</span></pre></td></tr><tr><td class="code"><pre><span class="line">ner = [each[<span class="number">1</span>] <span class="keyword">for</span> each <span class="keyword">in</span> ner_tagger.tag(tokens)]</span></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>stanford</tag>
        <tag>nltk</tag>
        <tag>ner</tag>
        <tag>pos</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试知识点总结</title>
    <url>/ml-overview/</url>
    <content><![CDATA[<p>对面试中机器学习模型常碰到的问题进行总结对比。</p>
<a id="more"></a>

<h2 id="模型汇总"><a href="#模型汇总" class="headerlink" title="模型汇总"></a>模型汇总</h2><ol>
<li><h3 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h3><ol>
<li><h4 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h4> 多叉树：按选择的特征个数分叉;<br> 信息增益：max（A）：I(D,A) = H(D) - H(D|A);<br> 无法处理连续特征;<br> 信息增益倾向于选择特征个数多的特征;<br> 无法处理缺失值;<br> 没考虑过拟合;<br> 不支持缺失值处理;</li>
<li><h4 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h4> 多叉树，连续节点处为二叉树;<br> 信息增益比;<br> 剪枝：预剪枝，后剪枝;<br> 只能用于分类;<br> 支持缺失值处理;</li>
<li><h4 id="cart"><a href="#cart" class="headerlink" title="cart"></a>cart</h4> 二叉树，支持缺失值处理;<br> 分类：gini系数;<br> 回归：均方差;</li>
</ol>
</li>
<li><h3 id="Boost（改变样本分布）"><a href="#Boost（改变样本分布）" class="headerlink" title="Boost（改变样本分布）"></a>Boost（改变样本分布）</h3><ol>
<li><h4 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h4><ul>
<li>分类：<ul>
<li>指数损失函数 + 加法模型 + 前向分步算法</li>
<li>根据当前的学习误差率更新训练样本的权重</li>
<li>样本权重w-&gt;分类器样本误差率e-&gt;分类器权重α-&gt;样本权重w（和前一个总分类器有关）</li>
</ul>
</li>
<li>回归：<ul>
<li>平方损失函数，拟合残差</li>
<li>根据划分区域，搜索使得损失函数最小的取值 </li>
</ul>
</li>
<li>弱学习器不固定；对异常点敏感，样本权重较高；精度高  </li>
</ul>
</li>
<li><h4 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h4><p> 弱学习器固定为cart树;<br> 损失函数不固定;<br> 拟合损失函数的负梯度;<br> 分类：对数似然损失函数（指数损失变为adaboost）;<br> 回归：均方差，绝对损失;<br> 采样是不放回采样; </p>
<pre><code>Q：怎样设置单棵树的停止生长条件？  
A：节点分裂时的最小样本数，最大深度，最多叶子节点数，loss满足约束条件  

Q：评估特征的权重大小  
A：  
1. 通过计算每个特征在训练集下的信息增益，最后计算每个特征信息增益与所有特征信息增益之和的比例为权重值；  
2. 借鉴投票机制。用相同的gbdt参数对w每个特征训练出一个模型，然后在该模型下计算每个特征正确分类的个数，最后计算每个特征正确分类的个数与所有正确分类个数之和的比例为权重值。  

Q：增加样本数量时，训练时长是线性增加吗？  
A：不是。因为生成单棵决策树时，损失函数极小值与样本数量N不是线性相关  ？？？  

Q：增加树的棵树时，训练时长是线性增加吗？  
A：不是。因为每棵树的生成的时间复杂度不一样。   

Q：如何防止过拟合  
A：  
1. 增加样本（data bias or small data的缘故），移除噪声。  
2. 减少特征，保留重要的特征（可以用PCA等对特征进行降维）。  
3. 对样本进行采样（类似bagging）。就是建树的时候，不是把所有的样本都作为输入，而是选择一个子集。  
4. 对特征进行采样。类似样本采样一样,每次建树的时候，只对部分的特征进行切分。  
5. 加正则项，剪枝  

Q：gbdt在训练和预测的时候都用到了步长，这两个步长一样么？  
A：一样。 ？？？  

Q：gbdt中哪些部分可以并行？  
A：  
1. 计算每个样本的负梯度  
2. 分裂挑选最佳特征及其分割点时，对特征计算相应的误差及均值时  
3. 更新每个样本的负梯度时  
4. 最后预测过程中，每个样本将之前的所有树的结果累加的时候  </code></pre></li>
<li><h3 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h3></li>
</ol>
</li>
<li><h3 id="图模型"><a href="#图模型" class="headerlink" title="图模型"></a>图模型</h3><ol>
<li><h4 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h4><ul>
<li>两个假设：<ul>
<li>齐次马尔科夫链假设：<br>隐藏状态之和之前的一个有关</li>
<li>观测独立假设：<br>观测状态之和当前的隐藏状态有关</li>
</ul>
</li>
<li>三个问题：<ul>
<li>已知参数，观测序列，求观测序列概率，前向后向算法</li>
<li>已知观测序列，估计模型参数，EM算法</li>
<li>已知参数，观测序列，推测状态序列，维特比算法</li>
</ul>
</li>
</ul>
</li>
<li><h3 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h3><p> 参数估计，极大似然估计<br> 推测状态序列，维特比算法<br> 特征函数设计</p>
<p>HMM    生成模型，马尔可夫假设<br>CRF 判别模型，没有马尔可夫假设（所以容易更好的采纳上下文信息）</p>
</li>
</ol>
</li>
</ol>
<h2 id="模型对比"><a href="#模型对比" class="headerlink" title="模型对比"></a>模型对比</h2><ol>
<li><h3 id="LR-vs-SVM"><a href="#LR-vs-SVM" class="headerlink" title="LR vs SVM"></a>LR vs SVM</h3><ul>
<li><p>不同</p>
<ul>
<li>LR的损失函数是Cross entropy loss，SVM的损失函数是Hinge loss，自带L2正则</li>
<li>LR的解是受数据本身分布影响的，而SVM的解不受数据分布的影响（支持向量）</li>
<li>LR的输出具有自然的概率意义，而SVM的输出不具有概率意义</li>
<li>SVM依赖数据表达的距离测度，需要对数据normalization，LR则不需要</li>
<li>SVM受惩罚系数C的影响较大，实验中需要做Validation，LR则不需要</li>
<li>LR适合于大样本学习，SVM适合于小样本学习 ？？？</li>
<li>SVM可以处理大型特征空间，LR对此性能不好 ？？？</li>
<li><a href="https://shomy.top/2017/03/09/support-vector-regression/" target="_blank" rel="noopener">SVM 回归</a> </li>
</ul>
</li>
<li><p>相同</p>
<ul>
<li>线性决策</li>
<li>kernel trick，ker-SVM只需要计算支持向量的核函数；而<a href="https://shomy.top/2017/03/07/kernel-lr/" target="_blank" rel="noopener">ker-LR</a>需要所有的样本 ？？？</li>
<li>都会受到outlier的影响</li>
</ul>
<p>只有将最优解w表示为xi的线性组合，才能够利用核函数K<br>如果样本数量小于特征数，那么就没必要选择非线性核，简单的使用线性核就可以了<br>如果样本数量大于特征数目，这时可以使用非线性核，将样本映射到更高维度，一般可以得到更好的结果</p>
<p><a href="https://shomy.top/archives/page/2/" target="_blank" rel="noopener">https://shomy.top/archives/page/2/</a></p>
</li>
</ul>
</li>
<li><h3 id="LR-vs-决策树"><a href="#LR-vs-决策树" class="headerlink" title="LR vs 决策树"></a>LR vs 决策树</h3><ul>
<li>决策树可以处理缺失值，LR不可以</li>
<li>LR线性决策边界，可能欠拟合（但是可以核方法），决策树非线性决策边界，但是对线性拟合效果容易过拟合。（如；x+y=1）（但是可以剪枝，正则，bagging）</li>
<li>LR有概率值解释，决策树有决策过程直观解释</li>
<li>LR对数据整体结构的分析优于决策树，而决策树对局部结构的分析优于LR。</li>
<li>LR对极值比较敏感，容易受极端值的影响</li>
<li>决策树容易过拟合；</li>
<li>GDBT + LR 融合</li>
</ul>
</li>
<li><h3 id="RF-vs-GDBT"><a href="#RF-vs-GDBT" class="headerlink" title="RF vs GDBT"></a>RF vs GDBT</h3><ul>
<li>不同<ul>
<li>组成随机森林的树可以并行生成，而GBDT是串行生成</li>
<li>随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和</li>
<li>随机森林对异常值不敏感，而GBDT对异常值比较敏感</li>
<li>随机森林是通过减少模型的方差来提高性能，而GBDT是减少模型的偏差来提高性能的</li>
<li>随机森林不需要进行数据预处理，即特征归一化。GBDT则需要进行特征归一化 ？？？为什么要归一化</li>
<li>组成随机森林的树可以分类树也可以是回归树，GBDT由cart树组成</li>
</ul>
</li>
<li>相同<ul>
<li>集成算法</li>
</ul>
</li>
</ul>
</li>
<li><h3 id="XgBoost-vs-GBDT"><a href="#XgBoost-vs-GBDT" class="headerlink" title="XgBoost vs GBDT"></a>XgBoost vs GBDT</h3></li>
</ol>
<p><a href="https://zhuanlan.zhihu.com/p/46831267" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46831267</a><br><a href="https://zhuanlan.zhihu.com/p/34679467" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34679467</a></p>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer详解：各个特征维度分析推导</title>
    <url>/transformer/</url>
    <content><![CDATA[<p>谷歌在文章《Attention is all you need》中提出的transformer模型。如图主要架构：同样为encoder-decoder模式，左边部分是encoder，右边部分是decoder。<br>TensorFlow代码：<em><a href="https://www.github.com/kyubyong/transformer" target="_blank" rel="noopener">https://www.github.com/kyubyong/transformer</a></em><br><img src="/images/transformer1.png" alt=""></p>
<a id="more"></a>

<h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><p>用 sentencepiece 进行分词。</p>
<h1 id="Encoder-输入"><a href="#Encoder-输入" class="headerlink" title="Encoder 输入"></a>Encoder 输入</h1><p>初始输入为待翻译语句的embedding矩阵，由于句子长度不一致，需要做统一长度处理，长度取maxlength1，不够长的句子padding 0值，句尾加上 <code>&lt;/s&gt;</code> 。</p>
<pre><code>d = 512, [batchsize，maxlen1，d]</code></pre><p>考虑到词语间的相对位置信息，还要加上语句的position<br>encoding，由函数形式直接求出。</p>
<pre><code>PE(pos,2i) = sin(pos/10002i/d)
PE(pos,2i+1) = cos(pos/10002i/d)</code></pre><p>Padding的值不做position encoding。 <code>[batchsize，maxlen1，d]</code> ，最终:</p>
<pre><code>encoder input = position encoding + input embedding。
encoder input : [batchsize，maxlen1，d]</code></pre><h1 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h1><p>Encoder 由N = 6个相同的layer连接组成。每个layer中有两个sublayer，分别是multihead<br>self-attention以及FFN。</p>
<pre><code>Q = K = V = input
MultiHead(Q, K, V) = concat(head1, …, headh)Wo
headi = Attention(QW­iQ，KW­ik，VW­iV)
Attention(Q, K, V) = softmax(QKT/$$\sqrt{d}$$) V</code></pre><p><img src="/images/transformer1.png" alt=""><br><img src="/images/transformer3.png" alt=""><br><img src="/images/transformer4.png" alt=""></p>
<p>softmax前要做key_mask，把pad 0 的地方赋值为-inf，softmax后权重做query mask，赋值0。</p>
<pre><code>h = 8
W­iQ, W­ik, W­iV : [d, d/h]
Q : [maxlen_q, d]
K = V : [maxlen_k, d]
Maxlen_q = maxlen_k so: Q = K = V : [maxlen1, d]
QW­kQ，KW­ik，VW­iV : [maxlen1, d/h]
headi : [maxlen1, d/h] \* [d/h, maxlen1] \* [maxlen1, d/h] = [maxlen1, d/h]
Wo : [d, d]
MultiHead(Q,K,V): [maxlen, d]</code></pre><p><code>Softmax([maxlen_q, maxlen_k])</code> 在最后一个维度即 <code>maxlen_k</code> 上做 <code>softmax</code>。<br>position-wise是因为处理的attention输出是某一个位置i的attention输出。</p>
<pre><code>FFN(x) = ReLU ( xW1 + b1 ) \* W2 + b2
ReLU(x) = max( 0, x )
dff = 4 \* d = 2048
W1 : [d, dff]
W2 : [dff, d]</code></pre><p>流程：</p>
<pre><code>Input -&gt; dropout -&gt;
(
multihead self-attention -&gt; dropout -&gt; residual connection -&gt; LN -&gt;
FFN-&gt; dropout -&gt; residual connection -&gt; LN -&gt;
) * 6
-&gt; memory [batchsize，maxlen，d]</code></pre><p>代码中在multihead attention中对score做dropout，FFN后没有dropout，但文章说每个sublayer的output都有一个dropout。</p>
<h1 id="Decoder-输入"><a href="#Decoder-输入" class="headerlink" title="Decoder 输入"></a>Decoder 输入</h1><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>目标句子首尾分别加上 <code>&lt;s&gt;</code> , <code>&lt;/s&gt;</code>。</p>
<pre><code>Decoder input = Output embedding + position encoding
Decoder input : [batchsize，maxlen2，d]</code></pre><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>初始向量为<code>&lt;s&gt;</code>对应embedding，之后将前一步的输出拼接到当前的所有预测构成当前的decoder输入。</p>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>Decoder由N = 6 个相同的layer组成，每个layer中有三个sublayer，分别是multihead self-attention, mutihead attention以及FFN。</p>
<pre><code>decoder input -&gt; dropout -&gt;
(
   Masked multihead self-attention(dec, dec, dec) = dec-&gt; dropout -&gt;
   multihead attention(dec, memory, memory) -&gt; dropout -&gt; residual connection
   -&gt; LN -&gt; FFN -&gt; dropout -&gt; residual connection -&gt; LN -&gt;
) * 6
-&gt; dec -&gt; linear -&gt; softmax</code></pre><p>Self-attention 的mask为一个和dec相同维度的上三角全为-inf的矩阵。</p>
<pre><code>Linear( x ) = xW
Dec : [batchsize，maxlen2，d]
W : [d, vocabsize]</code></pre><p>W为词汇表embedding矩阵的转置, 输入输出的词汇表embedding矩阵为W。即三个参数共享。</p>
<pre><code>Linear( x ) : [batchsize，maxlen2，vocabsize]</code></pre><p>Softmax函数：</p>
<center>
$p\left( k\|x \right)=\frac{\exp({{z}_{k}})}{\sum\nolimits_{i=1}^{K}{\exp ({{z}_{i}})}}$
</center>
其中zi一般叫做 logits，即未被归一化的对数概率。

<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>损失函数：cross entropy。用p代表predicted probability，用q代表groundtruth。即：</p>
<center>$cross\_entropy\_loss=\sum\limits_{k=1}^{K}{q\left( k\|x\right)\log (p\left( k\|x \right))}$</center>


<p>groundtruth为one-hot，即每个样本只有惟一的类别，$q(k)={{\delta}_{k,y}}$，y是真实类别。</p>
<center>${{\delta }_{k,y}}\text{=}\left\{\begin{matrix} 1,k=y \\0,k\ne y \\\end{matrix} \right.$</center>


<p>对目标句子onehot 做labelmsmooth用$\tilde{q}(k|x)$代替$q(k|x)$。（为了正则化，防止过拟合）</p>
<center>$\tilde{q}(k\|x)=(1-\varepsilon ){{\delta }_{k,y}}+\varepsilon u(k)$</center>


<p>可以理解为，对于$q(k)={{\delta}_{k,y}}$函数分布的真实标签，将它变成以如下方式获得：首先从标注的真实标签的$\delta$分布中取定，然后以一定的概率$\varepsilon$，将其替换为在$u(k)$分布中的随机变量。$u(k)$为均匀分布，即$u(k)=1/K$</p>
<h1 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h1><p>Adam优化器：<br><img src="/images/transformer5.png" alt=""><br>学习率使用warm up learning rate:</p>
<pre><code>learningrate = dmodel-0.5 \* min ( step_num-0.5, step_num \* warmup_steps-1.5 )
warmup_steps ：4000</code></pre>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>transoformer</tag>
      </tags>
  </entry>
  <entry>
    <title>MarkDown 使用细节</title>
    <url>/markdown/</url>
    <content><![CDATA[<p>记录使用遇到的一些常用的MarkDown命令</p>
<a id="more"></a>

<p>行内标签</p>
<pre><code>`&lt;你的 GitHub 用户名&gt;.github.io`</code></pre><p>文字居中</p>
<pre><code>&lt;center&gt;文字    &lt;/center&gt;</code></pre><p>代码块插入</p>
<pre><code><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> *</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="string">''</span>)</span></pre></td></tr></table></figure></code></pre>]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>常用句法分析工具包使用说明：Hanlp、StanfordNLP等</title>
    <url>/parser/</url>
    <content><![CDATA[<p>对比各个常用的自然语言处理工具包中的句法分析模块。</p>
<a id="more"></a>

<h1 id="Hanlp"><a href="#Hanlp" class="headerlink" title="Hanlp"></a>Hanlp</h1><p>pip install pyhanlp 安装即可<br>项目地址：<a href="https://github.com/hankcs/pyhanlp" target="_blank" rel="noopener">https://github.com/hankcs/pyhanlp</a></p>
<p><strong>基于神经网络的高性能依存句法分析器</strong></p>
<p>输出为CONLL格式中，每个词语占一行，无值列用下划线代替，列的分隔符为制表符 <code>&#39;\t&#39;</code> ，行的分隔符为换行符 <code>&#39;\n&#39;</code>；句子与句子之间用空行分隔。<br>CONLL标注格式包含10列，分别为：  </p>
<table>
<thead>
<tr>
<th align="center">ID</th>
<th align="center">FORM</th>
<th align="center">LEMMA</th>
<th align="center">CPOSTAG</th>
<th align="center">POSTAG</th>
<th align="center">FEATS</th>
<th align="center">HEAD</th>
<th align="center">DEPREL</th>
<th align="center">PHEAD</th>
<th align="center">PDEPREL</th>
</tr>
</thead>
</table>
<p>只用到前８列，其含义分别为：  </p>
<table>
<thead>
<tr>
<th align="center">id</th>
<th align="center">name</th>
<th align="center">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="center">ID</td>
<td align="center">当前词在句子中的序号，１开始.</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">FORM</td>
<td align="center">当前词语或标点</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">LEMMA</td>
<td align="center">当前词语（或标点）的原型或词干，在中文中，此列与FORM相同</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">CPOSTAG</td>
<td align="center">当前词语的词性（粗粒度）</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">POSTAG</td>
<td align="center">当前词语的词性（细粒度）</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">FEATS</td>
<td align="center">句法特征，在本次评测中，此列未被使用，全部以下划线代替。</td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">HEAD</td>
<td align="center">当前词语的中心词</td>
</tr>
<tr>
<td align="center">8</td>
<td align="center">DEPREL</td>
<td align="center">当前词语与中心词的依存关系</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyhanlp <span class="keyword">import</span> *</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(HanLP.parseDependency(<span class="string">"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"</span>))</span></pre></td></tr></table></figure>

<pre><code>1    徐先生    徐先生    nh    nr    _    4    主谓关系    _    _
2    还    还    d    d    _    4    状中结构    _    _
3    具体    具体    a    ad    _    4    状中结构    _    _
4    帮助    帮助    v    v    _    0    核心关系    _    _
5    他    他    r    r    _    4    兼语    _    _
6    确定    确定    v    v    _    4    动宾关系    _    _
7    了    了    u    u    _    6    右附加关系    _    _
8    把    把    p    p    _    15    状中结构    _    _
9    画    画    v    v    _    8    介宾关系    _    _
10    雄鹰    雄鹰    n    n    _    9    动宾关系    _    _
11    、    、    wp    w    _    12    标点符号    _    _
12    松鼠    松鼠    n    n    _    10    并列关系    _    _
13    和    和    c    c    _    14    左附加关系    _    _
14    麻雀    麻雀    n    n    _    10    并列关系    _    _
15    作为    作为    v    v    _    6    动宾关系    _    _
16    主攻    主攻    v    vn    _    17    定中关系    _    _
17    目标    目标    n    n    _    15    动宾关系    _    _
18    。    。    wp    w    _    4    标点符号    _    _</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sentence = HanLP.parseDependency(<span class="string">"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> sentence.iterator():  <span class="comment"># 通过dir()可以查看sentence的方法</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(<span class="string">"%s --(%s)--&gt; %s"</span> % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))</span></pre></td></tr></table></figure>

<pre><code>徐先生 --(主谓关系)--&gt; 帮助
还 --(状中结构)--&gt; 帮助
具体 --(状中结构)--&gt; 帮助
帮助 --(核心关系)--&gt; ##核心##
他 --(兼语)--&gt; 帮助
确定 --(动宾关系)--&gt; 帮助
了 --(右附加关系)--&gt; 确定
把 --(状中结构)--&gt; 作为
画 --(介宾关系)--&gt; 把
雄鹰 --(动宾关系)--&gt; 画
、 --(标点符号)--&gt; 松鼠
松鼠 --(并列关系)--&gt; 雄鹰
和 --(左附加关系)--&gt; 麻雀
麻雀 --(并列关系)--&gt; 雄鹰
作为 --(动宾关系)--&gt; 确定
主攻 --(定中关系)--&gt; 目标
目标 --(动宾关系)--&gt; 作为
。 --(标点符号)--&gt; 帮助</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(dir(sentence))</span></pre></td></tr></table></figure>

<pre><code>[&apos;__class__&apos;, &apos;__delattr__&apos;, &apos;__dict__&apos;, &apos;__dir__&apos;, &apos;__doc__&apos;, &apos;__eq__&apos;, &apos;__format__&apos;, &apos;__ge__&apos;, &apos;__getattribute__&apos;, &apos;__gt__&apos;, &apos;__hash__&apos;, &apos;__init__&apos;, &apos;__init_subclass__&apos;, &apos;__javaclass__&apos;, &apos;__javaobject__&apos;, &apos;__le__&apos;, &apos;__lt__&apos;, &apos;__metaclass__&apos;, &apos;__module__&apos;, &apos;__ne__&apos;, &apos;__new__&apos;, &apos;__reduce__&apos;, &apos;__reduce_ex__&apos;, &apos;__repr__&apos;, &apos;__setattr__&apos;, &apos;__sizeof__&apos;, &apos;__str__&apos;, &apos;__subclasshook__&apos;, &apos;__weakref__&apos;, &apos;edgeArray&apos;, &apos;equals&apos;, &apos;findChildren&apos;, &apos;forEach&apos;, &apos;getClass&apos;, &apos;getEdgeArray&apos;, &apos;getWordArray&apos;, &apos;getWordArrayWithRoot&apos;, &apos;hashCode&apos;, &apos;iterator&apos;, &apos;notify&apos;, &apos;notifyAll&apos;, &apos;spliterator&apos;, &apos;toString&apos;, &apos;wait&apos;, &apos;word&apos;, &apos;wordArray&apos;, &apos;wordArrayWithRoot&apos;]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_array = sentence.getWordArray()</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># print(word_array[0])</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> word_array:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(<span class="string">"%s --(%s)--&gt; %s"</span> % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))</span></pre></td></tr></table></figure>

<pre><code>徐先生 --(主谓关系)--&gt; 帮助
还 --(状中结构)--&gt; 帮助
具体 --(状中结构)--&gt; 帮助
帮助 --(核心关系)--&gt; ##核心##
他 --(兼语)--&gt; 帮助
确定 --(动宾关系)--&gt; 帮助
了 --(右附加关系)--&gt; 确定
把 --(状中结构)--&gt; 作为
画 --(介宾关系)--&gt; 把
雄鹰 --(动宾关系)--&gt; 画
、 --(标点符号)--&gt; 松鼠
松鼠 --(并列关系)--&gt; 雄鹰
和 --(左附加关系)--&gt; 麻雀
麻雀 --(并列关系)--&gt; 雄鹰
作为 --(动宾关系)--&gt; 确定
主攻 --(定中关系)--&gt; 目标
目标 --(动宾关系)--&gt; 作为
。 --(标点符号)--&gt; 帮助</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 还可以直接遍历子树，从某棵子树的某个节点一路遍历到虚根</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">CoNLLWord = JClass(<span class="string">"com.hankcs.hanlp.corpus.dependency.CoNll.CoNLLWord"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">head = word_array[<span class="number">15</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> head.HEAD:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    head = head.HEAD</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">if</span> (head == CoNLLWord.ROOT):</span></pre></td></tr><tr><td class="code"><pre><span class="line">        print(head.LEMMA)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">else</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        print(<span class="string">"%s --(%s)--&gt; "</span> % (head.LEMMA, head.DEPREL))</span></pre></td></tr></table></figure>

<pre><code>目标 --(动宾关系)--&gt; 
作为 --(动宾关系)--&gt; 
确定 --(动宾关系)--&gt; 
帮助 --(核心关系)--&gt; 
##核心##</code></pre><h1 id="StanfordNLP"><a href="#StanfordNLP" class="headerlink" title="StanfordNLP"></a>StanfordNLP</h1><p>pip install stanfordnlp 安装即可<br>项目地址：<a href="https://github.com/stanfordnlp/stanfordnlp" target="_blank" rel="noopener">https://github.com/stanfordnlp/stanfordnlp</a><br>依存句法关系符号解释：<a href="https://www.cnblogs.com/sherry-yang/p/9061341.html" target="_blank" rel="noopener">https://www.cnblogs.com/sherry-yang/p/9061341.html</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> stanfordnlp</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nlp = stanfordnlp.Pipeline(lang=<span class="string">'zh'</span>)</span></pre></td></tr></table></figure>

<pre><code>Use device: gpu
---
Loading: tokenize
With settings: 
{&apos;model_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd_tokenizer.pt&apos;, &apos;lang&apos;: &apos;zh&apos;, &apos;shorthand&apos;: &apos;zh_gsd&apos;, &apos;mode&apos;: &apos;predict&apos;}
---
Loading: pos
With settings: 
{&apos;model_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd_tagger.pt&apos;, &apos;pretrain_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt&apos;, &apos;lang&apos;: &apos;zh&apos;, &apos;shorthand&apos;: &apos;zh_gsd&apos;, &apos;mode&apos;: &apos;predict&apos;}
---
Loading: lemma
With settings: 
{&apos;model_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd_lemmatizer.pt&apos;, &apos;lang&apos;: &apos;zh&apos;, &apos;shorthand&apos;: &apos;zh_gsd&apos;, &apos;mode&apos;: &apos;predict&apos;}
Building an attentional Seq2Seq model...
Using a Bi-LSTM encoder
Using soft attention for LSTM.
Finetune all embeddings.
[Running seq2seq lemmatizer with edit classifier]
---
Loading: depparse
With settings: 
{&apos;model_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd_parser.pt&apos;, &apos;pretrain_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt&apos;, &apos;lang&apos;: &apos;zh&apos;, &apos;shorthand&apos;: &apos;zh_gsd&apos;, &apos;mode&apos;: &apos;predict&apos;}
Done loading processors!
---</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">doc = nlp(<span class="string">"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">doc.sentences[<span class="number">0</span>].print_dependencies()</span></pre></td></tr></table></figure>

<pre><code>(&apos;徐&apos;, &apos;2&apos;, &apos;nmod&apos;)
(&apos;先生&apos;, &apos;4&apos;, &apos;nsubj&apos;)
(&apos;还&apos;, &apos;4&apos;, &apos;mark&apos;)
(&apos;具体&apos;, &apos;0&apos;, &apos;root&apos;)
(&apos;帮助&apos;, &apos;4&apos;, &apos;obj&apos;)
(&apos;他&apos;, &apos;7&apos;, &apos;nsubj&apos;)
(&apos;确定&apos;, &apos;4&apos;, &apos;ccomp&apos;)
(&apos;了&apos;, &apos;7&apos;, &apos;case:aspect&apos;)
(&apos;把&apos;, &apos;15&apos;, &apos;aux:caus&apos;)
(&apos;画雄鹰&apos;, &apos;15&apos;, &apos;obj&apos;)
(&apos;、&apos;, &apos;12&apos;, &apos;punct&apos;)
(&apos;松鼠&apos;, &apos;10&apos;, &apos;conj&apos;)
(&apos;和&apos;, &apos;14&apos;, &apos;cc&apos;)
(&apos;麻雀&apos;, &apos;10&apos;, &apos;conj&apos;)
(&apos;作&apos;, &apos;7&apos;, &apos;ccomp&apos;)
(&apos;为&apos;, &apos;15&apos;, &apos;mark&apos;)
(&apos;主攻&apos;, &apos;18&apos;, &apos;nmod&apos;)
(&apos;目标&apos;, &apos;16&apos;, &apos;obj&apos;)
(&apos;。&apos;, &apos;4&apos;, &apos;punct&apos;)</code></pre><h1 id="HIT-LTP"><a href="#HIT-LTP" class="headerlink" title="HIT LTP"></a>HIT LTP</h1><p>项目地址：  </p>
<ul>
<li><a href="https://github.com/HIT-SCIR/pyltp" target="_blank" rel="noopener">https://github.com/HIT-SCIR/pyltp</a>  </li>
<li><a href="https://pyltp.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener">https://pyltp.readthedocs.io/zh_CN/latest/</a>  </li>
</ul>
<p>安装步骤：</p>
<ul>
<li>pip install pyltp</li>
<li>下载模型文件：<a href="http://ltp.ai/download.html" target="_blank" rel="noopener">七牛云</a>，当前模型版本 3.4.0</li>
</ul>
<p>输出：</p>
<ul>
<li>arc.head 表示依存弧的父节点词的索引。ROOT节点的索引是0，第一个词开始的索引依次为1、2、3…  </li>
<li>arc.relation 表示依存弧的关系。  </li>
</ul>
<p>标注集请参考: <a href="https://ltp.readthedocs.io/zh_CN/latest/appendix.html#id5" target="_blank" rel="noopener">https://ltp.readthedocs.io/zh_CN/latest/appendix.html#id5</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyltp <span class="keyword">import</span> Parser</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyltp <span class="keyword">import</span> Segmentor</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyltp <span class="keyword">import</span> Postagger</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dependency_parser</span><span class="params">(sentences)</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    output = []</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        parser = Parser()</span></pre></td></tr><tr><td class="code"><pre><span class="line">        parser.load(<span class="string">'./ltp_data_v3.4.0/parser.model'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        segmentor = Segmentor() </span></pre></td></tr><tr><td class="code"><pre><span class="line">        segmentor.load(<span class="string">'./ltp_data_v3.4.0/cws.model'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        postagger = Postagger() </span></pre></td></tr><tr><td class="code"><pre><span class="line">        postagger.load(<span class="string">'./ltp_data_v3.4.0/pos.model'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">        words = segmentor.segment(sentence)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        postags = postagger.postag(words)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        arcs = parser.parse(words, postags)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        output.append(&#123;<span class="string">'words'</span>:words,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                       <span class="string">'postags'</span>:postags,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                       <span class="string">'arcs'</span>:arcs</span></pre></td></tr><tr><td class="code"><pre><span class="line">                      &#125;)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#         print(' '.join(output[0]['words']))</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#         print(' '.join(output[0]['postags']))</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    segmentor.release() </span></pre></td></tr><tr><td class="code"><pre><span class="line">    postagger.release()</span></pre></td></tr><tr><td class="code"><pre><span class="line">    parser.release()</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">return</span> output</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sentences = [<span class="string">'徐先生还具体帮助他确定了把画雄鹰，松鼠和麻雀作为主攻目标。'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">output = dependency_parser(sentences)</span></pre></td></tr><tr><td class="code"><pre><span class="line">Arcs = [each[<span class="string">'arcs'</span>] <span class="keyword">for</span> each <span class="keyword">in</span> output]</span></pre></td></tr><tr><td class="code"><pre><span class="line">[<span class="string">" "</span>.join(<span class="string">"%d:%s"</span> % (arc.head, arc.relation) <span class="keyword">for</span> arc <span class="keyword">in</span> arcs) <span class="keyword">for</span> arcs <span class="keyword">in</span> Arcs]</span></pre></td></tr></table></figure>




<pre><code>[&apos;2:ATT 5:SBV 4:ADV 5:ADV 0:HED 5:DBL 5:VOB 7:RAD 10:ADV 7:VOB 10:VOB 5:WP 16:SBV 15:LAD 13:COO 5:COO 18:ATT 16:VOB 5:WP&apos;]</code></pre><h1 id="FudanNLP-FNLP"><a href="#FudanNLP-FNLP" class="headerlink" title="FudanNLP (FNLP)"></a>FudanNLP (FNLP)</h1><p><a href="https://github.com/FudanNLP/fnlp" target="_blank" rel="noopener">https://github.com/FudanNLP/fnlp</a><br>java 接口，且不再更新，现在已经推出FastNLP</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>parser</tag>
      </tags>
  </entry>
  <entry>
    <title>SQuAD2.0 刷榜top3模型分析</title>
    <url>/SQuAD2-0/</url>
    <content><![CDATA[<p>由于<a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" rel="noopener">SQuAD2.0榜单</a>一直在更新，所以top3模型也在更新。</p>
<a id="more"></a>
<h2 id="top1-BERT-DAE-AoA"><a href="#top1-BERT-DAE-AoA" class="headerlink" title="top1: BERT + DAE + AoA"></a>top1: BERT + DAE + AoA</h2><ul>
<li>AoA: attention over attention [1]</li>
<li>DAE: DA Enhanced<ul>
<li>Data Augmentation</li>
<li>Domain Adaptation<br><img src="/images/aoa.png" alt="AoA"></li>
</ul>
</li>
</ul>
<p><a href="https://www.infoq.cn/article/M7NpCAAMrPzRo-RViOKs" target="_blank" rel="noopener">https://www.infoq.cn/article/M7NpCAAMrPzRo-RViOKs</a><br><a href="https://zhuanlan.zhihu.com/p/27361305" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27361305</a></p>
<pre><code>[1] Cui Y, Chen Z, Wei S, et al. Attention-over-attention neural networks for reading comprehension[J]. arXiv preprint arXiv:1607.04423, 2016.</code></pre><h2 id="top2-BERT-ConvLSTM-MTL-Verifier"><a href="#top2-BERT-ConvLSTM-MTL-Verifier" class="headerlink" title="top2: BERT + ConvLSTM + MTL + Verifier"></a>top2: BERT + ConvLSTM + MTL + Verifier</h2><ul>
<li>MTL: 多任务学习<ul>
<li><del>预测一个问题是否可答</del></li>
<li><del>预测该问题在篇章中的答案</del></li>
</ul>
</li>
<li>Verifier: 验证器 [1]</li>
<li>convLSTM [2]</li>
</ul>
<p><a href="https://msd.misuland.com/pd/12136984602514128" target="_blank" rel="noopener">https://msd.misuland.com/pd/12136984602514128</a><br><a href="https://blog.csdn.net/maka_uir/article/details/83650978" target="_blank" rel="noopener">https://blog.csdn.net/maka_uir/article/details/83650978</a></p>
<pre><code>[1] Hu M, Peng Y, Huang Z, et al. Read+ verify: Machine reading comprehension with unanswerable questions[J]. arXiv preprint arXiv:1808.05759, 2018.
[2] Shi X , Chen Z , Wang H , et al. Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting[J]. 2015.</code></pre><h2 id="top3-BERT-N-Gram-Masking-Synthetic-Self-Training"><a href="#top3-BERT-N-Gram-Masking-Synthetic-Self-Training" class="headerlink" title="top3: BERT + N-Gram Masking + Synthetic Self-Training"></a>top3: BERT + N-Gram Masking + Synthetic Self-Training</h2><ul>
<li>N-Gram Masking: 类似百度的ERNIE模型</li>
<li>Synthetic Self-Training: BERT官方PPT  </li>
</ul>
<p>(这个方法全部在预训练上做改进，没有对bert上层模型做什么改进)<br>Unclear if adding things on top of BERT really helps by very much.  </p>
<p><a href="https://zhuanlan.zhihu.com/p/63126803" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/63126803</a><br><a href="https://nlp.stanford.edu/seminar/details/jdevlin.pdf" target="_blank" rel="noopener">https://nlp.stanford.edu/seminar/details/jdevlin.pdf</a></p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p><a href="http://web.stanford.edu/class/cs224n/posters/15845024.pdf" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/posters/15845024.pdf</a></p>
<pre><code>[1] Ensemble BERT with Data Augmentation and Linguistic Knowledge on SQuAD2.0</code></pre>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>SQuAD2.0</tag>
      </tags>
  </entry>
  <entry>
    <title>GitHub Pages + Hexo + Next：博客搭建、功能设置及美化</title>
    <url>/next/</url>
    <content><![CDATA[<p>使用Hexo博客框架搭建自己的个人博客，并部署到个人的GitHub上，选用NexT主题，添加一些使用小功能并进行界面的美化。</p>
<a id="more"></a>

<h1 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h1><p>Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。</p>
<h2 id="搭建博客框架"><a href="#搭建博客框架" class="headerlink" title="搭建博客框架"></a>搭建博客框架</h2><p>搭建流程参照官网中文说明文档：</p>
<pre><code>https://hexo.io/zh-cn/docs/</code></pre><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>将搭建在本地的博客通过GitHub Pages部署在个人的GitHub中。从而能通过 <code>&lt;你的 GitHub 用户名&gt;.github.io</code> 域名访问博客</p>
<p>参考教程：</p>
<pre><code>https://www.jianshu.com/p/05289a4bc8b2
https://www.cnblogs.com/jackyroc/p/7681938.html</code></pre><h1 id="Next-主题"><a href="#Next-主题" class="headerlink" title="Next 主题"></a>Next 主题</h1><p>部署完成后，即可对博客做一些细节上的优化，增添小工具以及美化界面</p>
<p>参考教程：</p>
<pre><code>https://zhuanlan.zhihu.com/p/30836436
https://io-oi.me/tech/hexo-next-optimization/</code></pre><h2 id="字体调节"><a href="#字体调节" class="headerlink" title="字体调节"></a>字体调节</h2><p>Next主题默认字体为 <code>font-size-medium = 1em</code> ，有点大。通常来讲，Next主题控制字体大小的文件是在主题文件夹中的 <code>source\css_variables</code> 目录下的 <code>base.styl</code> 文件中，修改如下文件 ：</p>
<pre><code>// Font size
$font-size-base           = (hexo-config(&apos;font.enable&apos;) and hexo-config(&apos;font.global.size&apos;) is a &apos;unit&apos;) ? unit(hexo-config(&apos;font.global.size&apos;), em) : 1em;
$font-size-smallest       = .75em;
$font-size-smaller        = .8125em;
$font-size-small          = .875em;
$font-size-medium         = 1em;
$font-size-large          = 1.125em;
$font-size-larger         = 1.25em;
$font-size-largest        = 1.375em;</code></pre><p>把 <code>1em</code> 改为 <code>0.875em</code> 即可:</p>
<pre><code>// Font size
$font-size-base           = (hexo-config(&apos;font.enable&apos;) and hexo-config(&apos;font.global.size&apos;) is a &apos;unit&apos;) ? unit(hexo-config(&apos;font.global.size&apos;), em) : .875em;
$font-size-smallest       = .75em;
$font-size-smaller        = .8125em;
$font-size-small          = .875em;
$font-size-medium         = 1em;
$font-size-large          = 1.125em;
$font-size-larger         = 1.25em;
$font-size-largest        = 1.375em;</code></pre><h2 id="插入本地图片"><a href="#插入本地图片" class="headerlink" title="插入本地图片"></a>插入本地图片</h2><p>资源（Asset）代表 source 文件夹中除了文章以外的所有文件，例如图片、CSS、JS 文件等。比方说，如果你的Hexo项目中只有少量图片，那最简单的方法就是将它们放在 <code>source/images</code> 文件夹中。然后通过类似于 <code>![](/images/image.jpg)</code> 的方法访问它们。</p>
<h2 id="添加❤脚注"><a href="#添加❤脚注" class="headerlink" title="添加❤脚注"></a>添加❤脚注</h2><p>更改主题配置文件：</p>
<pre><code>icon:
  # Icon name in Font Awesome. See: https://fontawesome.com/v4.7.0/icons/
  # `heart` is recommended with animation in red (#ff0000).
  name: heart #user
  # If you want to animate the icon, set it to true.
  animated: true
  # Change the color of icon, using Hex Code.
  # color: &quot;#808080&quot;
  color: &quot;#ff0000&quot;</code></pre><h2 id="搜索引擎优化"><a href="#搜索引擎优化" class="headerlink" title="搜索引擎优化"></a>搜索引擎优化</h2><h3 id="标题优化"><a href="#标题优化" class="headerlink" title="标题优化"></a>标题优化</h3><p>给标题增加详细信息,更改 <code>index.swig</code> 文件 <code>your-hexo-site\themes\next\layout</code>: </p>
<pre><code>{% block title %}{{ title }}{%- if theme.index_with_subtitle and subtitle %} - {{ subtitle }}{%- endif %}{% endblock %}

	{% block title %}{{ title }}{%- if theme.index_with_subtitle and subtitle %} - {{ subtitle }}{%- endif %} - {{ theme.keywords }} - {{ config.title }}{{ theme.description }}{% endblock %}</code></pre><h3 id="修改链接"><a href="#修改链接" class="headerlink" title="修改链接"></a>修改链接</h3><p>HEXO默认的文章链接形式为 <code>domain/year/month/day/postname</code> ，默认就是一个四级url，并且可能造成url过长，对搜索引擎是十分不友好的，我们可以改成 <code>domain/postname</code> 的形式。编辑站点 <code>_config.yml</code> 文件，修改其中的 <code>permalink</code> 字段改为 <code>permalink: :title.html</code> 即可。</p>
<pre><code>permalink: :year/:month/:day/:title/
permalink: :title/</code></pre><p>参考:</p>
<pre><code>http://www.ehcoo.com/seo.html</code></pre><h3 id="SOE配置"><a href="#SOE配置" class="headerlink" title="SOE配置"></a>SOE配置</h3><p>参考：</p>
<pre><code>https://www.jianshu.com/p/86557c34b671
https://blog.junyu.io/posts/0008-blog-seo.html</code></pre><h2 id="首页阅读全文设置"><a href="#首页阅读全文设置" class="headerlink" title="首页阅读全文设置"></a>首页阅读全文设置</h2><ol>
<li>在文章中手动加入 <code>&lt;!--more--&gt;</code> 进行截断</li>
<li>通过在配置文件中加入代码，自动截断（但实验失败）。</li>
</ol>
<h2 id="字数统计及阅读时长"><a href="#字数统计及阅读时长" class="headerlink" title="字数统计及阅读时长"></a>字数统计及阅读时长</h2><p>安装 <code>hexo-wordcount</code> 失败后采用下面方法：</p>
<pre><code>npm install hexo-symbols-count-time --save</code></pre><p>在配置文件加入设置：</p>
<pre><code>symbols_count_time:  
  symbols: true  
  time: true  
  total_symbols: true  
  total_time: true</code></pre><h2 id="阅读次数统计"><a href="#阅读次数统计" class="headerlink" title="阅读次数统计"></a>阅读次数统计</h2><p>主题配置文件设置：</p>
<pre><code>busuanzi_count:
  enable: true
  total_visitors: true
  total_visitors_icon: user
  total_views: true
  total_views_icon: eye
  post_views: true
  post_views_icon: eye</code></pre><h2 id="评论功能"><a href="#评论功能" class="headerlink" title="评论功能"></a>评论功能</h2><p>Next主题已经集成了GitTalk,直接使用即可。首先，注册GitHub OAuth Apps，生成<code>client_id</code> 和 <code>client_secret</code> 。再修改主题配置文件：</p>
<pre><code>gitalk:
  enable: true
  github_id: troublemeeter # GitHub repo owner
  repo: troublemeeter.github.io # Repository name to store issues
  client_id: # GitHub Application Client ID
  client_secret: # GitHub Application Client Secret
  admin_user: troublemeeter # GitHub repo owner and collaborators, only these guys can initialize gitHub issues
  distraction_free_mode: true # Facebook-like distraction free mode
  # Gitalk&apos;s display language depends on user&apos;s browser or system environment
  # If you want everyone visiting your site to see a uniform language, you can set a force language value
  # Available values: en | es-ES | fr | ru | zh-CN | zh-TW
  language:</code></pre><p>指定页面不添加评论功能，在文章头部设置 <code>comments: false</code>：</p>
<pre><code>---
title: 标签
date: 2019-12-10 00:21:09
type: &quot;tags&quot;
comments: false
---</code></pre><p>参考教程：</p>
<pre><code>https://mrluyc.github.io/2019/07/30/HexoNexT%E9%9B%86%E6%88%90Gitalk/</code></pre><h2 id="数学公式支持"><a href="#数学公式支持" class="headerlink" title="数学公式支持"></a>数学公式支持</h2><p>安装 <code>hexo-math</code>：</p>
<pre><code>$ npm install hexo-math --save</code></pre><p>在配置文件加入设置：</p>
<pre><code>math:
  engine: &apos;mathjax&apos; # or &apos;katex&apos;
  mathjax:
    # src: custom_mathjax_source
    config:
      # MathJax config</code></pre><p>在Next主题配置文件更改设置为：</p>
<pre><code># Math Formulas Render Support
math:
  # Default (true) will load mathjax / katex script on demand.
  # That is it only render those page which has `mathjax: true` in Front-matter.
  # If you set it to false, it will load mathjax / katex srcipt EVERY PAGE.
  per_page: true
  # hexo-renderer-pandoc (or hexo-renderer-kramed) required for full MathJax support.
  mathjax:
    enable: true
    # See: https://mhchem.github.io/MathJax-mhchem/
    mhchem: false</code></pre><p>在需要加载mathjax的文件的头部加入<code>mathjax: true</code>：</p>
<pre><code>---
title: transformer
date: 2019-12-10 17:50:42
tags:
mathjax: true
---</code></pre><h2 id="代码高亮"><a href="#代码高亮" class="headerlink" title="代码高亮"></a>代码高亮</h2><p>不喜欢五颜六色，所以暂时未处理。</p>
<h2 id="迁移"><a href="#迁移" class="headerlink" title="迁移"></a>迁移</h2><pre><code>https://blog.csdn.net/White_Idiot/article/details/80685990</code></pre><p>迁移完后，相关命令：新建Markdown文章，编辑文章</p>
<pre><code>hexo new &quot;post-name&quot;</code></pre><p>将相关更改推送到hexo分支</p>
<pre><code>git add
git commit -m &quot;...&quot;
git push origin hexo</code></pre><p>将静态文件推送到master分支</p>
<pre><code>hexo clean # 如果配置文件没有更改，忽略该命令
hexo g -d</code></pre><h2 id="打赏功能"><a href="#打赏功能" class="headerlink" title="打赏功能"></a>打赏功能</h2><p>制作好微信收款码和支付宝收款码，保存至<code>themes/next/source/images</code>。并修改主题配置文件如下：</p>
<pre><code># Reward (Donate)
# Front-matter variable (unsupport animation).
reward_settings:
  # If true, reward will be displayed in every article by default.
  enable: true
  animation: true
  comment: 🤣~疯狂暗示~🤣</code></pre><p>效果如下，不信试一试~</p>
]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
</search>
