<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>机器学习面试知识点总结</title>
    <url>/ml-overview/</url>
    <content><![CDATA[<p>对面试中机器学习模型常碰到的问题进行总结对比。</p>
<a id="more"></a>

<h2 id="模型汇总"><a href="#模型汇总" class="headerlink" title="模型汇总"></a>模型汇总</h2><ol>
<li><h3 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h3><ol>
<li><h4 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h4> 多叉树：按选择的特征个数分叉;<br> 信息增益：max（A）：I(D,A) = H(D) - H(D|A);<br> 无法处理连续特征;<br> 信息增益倾向于选择特征个数多的特征;<br> 无法处理缺失值;<br> 没考虑过拟合;<br> 不支持缺失值处理;</li>
<li><h4 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h4> 多叉树，连续节点处为二叉树;<br> 信息增益比;<br> 剪枝：预剪枝，后剪枝;<br> 只能用于分类;<br> 支持缺失值处理;</li>
<li><h4 id="cart"><a href="#cart" class="headerlink" title="cart"></a>cart</h4> 二叉树，支持缺失值处理;<br> 分类：gini系数;<br> 回归：均方差;</li>
</ol>
</li>
<li><h3 id="Boost（改变样本分布）"><a href="#Boost（改变样本分布）" class="headerlink" title="Boost（改变样本分布）"></a>Boost（改变样本分布）</h3><ol>
<li><h4 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h4><ul>
<li>分类：<ul>
<li>指数损失函数 + 加法模型 + 前向分步算法</li>
<li>根据当前的学习误差率更新训练样本的权重</li>
<li>样本权重w-&gt;分类器样本误差率e-&gt;分类器权重α-&gt;样本权重w（和前一个总分类器有关）</li>
</ul>
</li>
<li>回归：<ul>
<li>平方损失函数，拟合残差</li>
<li>根据划分区域，搜索使得损失函数最小的取值 </li>
</ul>
</li>
<li>弱学习器不固定；对异常点敏感，样本权重较高；精度高  </li>
</ul>
</li>
<li><h4 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h4><p> 弱学习器固定为cart树;<br> 损失函数不固定;<br> 拟合损失函数的负梯度;<br> 分类：对数似然损失函数（指数损失变为adaboost）;<br> 回归：均方差，绝对损失;<br> 采样是不放回采样; </p>
<pre><code>Q：怎样设置单棵树的停止生长条件？  
A：节点分裂时的最小样本数，最大深度，最多叶子节点数，loss满足约束条件  

Q：评估特征的权重大小  
A：  
1. 通过计算每个特征在训练集下的信息增益，最后计算每个特征信息增益与所有特征信息增益之和的比例为权重值；  
2. 借鉴投票机制。用相同的gbdt参数对w每个特征训练出一个模型，然后在该模型下计算每个特征正确分类的个数，最后计算每个特征正确分类的个数与所有正确分类个数之和的比例为权重值。  

Q：增加样本数量时，训练时长是线性增加吗？  
A：不是。因为生成单棵决策树时，损失函数极小值与样本数量N不是线性相关  ？？？  

Q：增加树的棵树时，训练时长是线性增加吗？  
A：不是。因为每棵树的生成的时间复杂度不一样。   

Q：如何防止过拟合  
A：  
1. 增加样本（data bias or small data的缘故），移除噪声。  
2. 减少特征，保留重要的特征（可以用PCA等对特征进行降维）。  
3. 对样本进行采样（类似bagging）。就是建树的时候，不是把所有的样本都作为输入，而是选择一个子集。  
4. 对特征进行采样。类似样本采样一样,每次建树的时候，只对部分的特征进行切分。  
5. 加正则项，剪枝  

Q：gbdt在训练和预测的时候都用到了步长，这两个步长一样么？  
A：一样。 ？？？  

Q：gbdt中哪些部分可以并行？  
A：  
1. 计算每个样本的负梯度  
2. 分裂挑选最佳特征及其分割点时，对特征计算相应的误差及均值时  
3. 更新每个样本的负梯度时  
4. 最后预测过程中，每个样本将之前的所有树的结果累加的时候  </code></pre></li>
<li><h3 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h3></li>
</ol>
</li>
<li><h3 id="图模型"><a href="#图模型" class="headerlink" title="图模型"></a>图模型</h3><ol>
<li><h4 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h4><ul>
<li>两个假设：<ul>
<li>齐次马尔科夫链假设：<br>隐藏状态之和之前的一个有关</li>
<li>观测独立假设：<br>观测状态之和当前的隐藏状态有关</li>
</ul>
</li>
<li>三个问题：<ul>
<li>已知参数，观测序列，求观测序列概率，前向后向算法</li>
<li>已知观测序列，估计模型参数，EM算法</li>
<li>已知参数，观测序列，推测状态序列，维特比算法</li>
</ul>
</li>
</ul>
</li>
<li><h3 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h3><p> 参数估计，极大似然估计<br> 推测状态序列，维特比算法<br> 特征函数设计</p>
<p>HMM    生成模型，马尔可夫假设<br>CRF 判别模型，没有马尔可夫假设（所以容易更好的采纳上下文信息）</p>
</li>
</ol>
</li>
</ol>
<h2 id="模型对比"><a href="#模型对比" class="headerlink" title="模型对比"></a>模型对比</h2><ol>
<li><h3 id="LR-vs-SVM"><a href="#LR-vs-SVM" class="headerlink" title="LR vs SVM"></a>LR vs SVM</h3><ul>
<li><p>不同</p>
<ul>
<li>LR的损失函数是Cross entropy loss，SVM的损失函数是Hinge loss，自带L2正则</li>
<li>LR的解是受数据本身分布影响的，而SVM的解不受数据分布的影响（支持向量）</li>
<li>LR的输出具有自然的概率意义，而SVM的输出不具有概率意义</li>
<li>SVM依赖数据表达的距离测度，需要对数据normalization，LR则不需要</li>
<li>SVM受惩罚系数C的影响较大，实验中需要做Validation，LR则不需要</li>
<li>LR适合于大样本学习，SVM适合于小样本学习 ？？？</li>
<li>SVM可以处理大型特征空间，LR对此性能不好 ？？？</li>
<li><a href="https://shomy.top/2017/03/09/support-vector-regression/" target="_blank" rel="noopener">SVM 回归</a> </li>
</ul>
</li>
<li><p>相同</p>
<ul>
<li>线性决策</li>
<li>kernel trick，ker-SVM只需要计算支持向量的核函数；而<a href="https://shomy.top/2017/03/07/kernel-lr/" target="_blank" rel="noopener">ker-LR</a>需要所有的样本 ？？？</li>
<li>都会受到outlier的影响</li>
</ul>
<p>只有将最优解w表示为xi的线性组合，才能够利用核函数K<br>如果样本数量小于特征数，那么就没必要选择非线性核，简单的使用线性核就可以了<br>如果样本数量大于特征数目，这时可以使用非线性核，将样本映射到更高维度，一般可以得到更好的结果</p>
<p><a href="https://shomy.top/archives/page/2/" target="_blank" rel="noopener">https://shomy.top/archives/page/2/</a></p>
</li>
</ul>
</li>
<li><h3 id="LR-vs-决策树"><a href="#LR-vs-决策树" class="headerlink" title="LR vs 决策树"></a>LR vs 决策树</h3><ul>
<li>决策树可以处理缺失值，LR不可以</li>
<li>LR线性决策边界，可能欠拟合（但是可以核方法），决策树非线性决策边界，但是对线性拟合效果容易过拟合。（如；x+y=1）（但是可以剪枝，正则，bagging）</li>
<li>LR有概率值解释，决策树有决策过程直观解释</li>
<li>LR对数据整体结构的分析优于决策树，而决策树对局部结构的分析优于LR。</li>
<li>LR对极值比较敏感，容易受极端值的影响</li>
<li>决策树容易过拟合；</li>
<li>GDBT + LR 融合</li>
</ul>
</li>
<li><h3 id="RF-vs-GDBT"><a href="#RF-vs-GDBT" class="headerlink" title="RF vs GDBT"></a>RF vs GDBT</h3><ul>
<li>不同<ul>
<li>组成随机森林的树可以并行生成，而GBDT是串行生成</li>
<li>随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和</li>
<li>随机森林对异常值不敏感，而GBDT对异常值比较敏感</li>
<li>随机森林是通过减少模型的方差来提高性能，而GBDT是减少模型的偏差来提高性能的</li>
<li>随机森林不需要进行数据预处理，即特征归一化。GBDT则需要进行特征归一化 ？？？为什么要归一化</li>
<li>组成随机森林的树可以分类树也可以是回归树，GBDT由cart树组成</li>
</ul>
</li>
<li>相同<ul>
<li>集成算法</li>
</ul>
</li>
</ul>
</li>
<li><h3 id="XgBoost-vs-GBDT"><a href="#XgBoost-vs-GBDT" class="headerlink" title="XgBoost vs GBDT"></a>XgBoost vs GBDT</h3></li>
</ol>
<p><a href="https://zhuanlan.zhihu.com/p/46831267" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46831267</a><br><a href="https://zhuanlan.zhihu.com/p/34679467" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34679467</a></p>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer详解：各个特征维度分析推导</title>
    <url>/transformer/</url>
    <content><![CDATA[<p>谷歌在文章《Attention is all you need》中提出的transformer模型。如图主要架构：同样为encoder-decoder模式，左边部分是encoder，右边部分是decoder。<br>TensorFlow代码：<em><a href="https://www.github.com/kyubyong/transformer" target="_blank" rel="noopener">https://www.github.com/kyubyong/transformer</a></em><br><img src="/images/transformer1.png" alt=""></p>
<a id="more"></a>

<h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><p>用 sentencepiece 进行分词。</p>
<h1 id="Encoder-输入"><a href="#Encoder-输入" class="headerlink" title="Encoder 输入"></a>Encoder 输入</h1><p>初始输入为待翻译语句的embedding矩阵，由于句子长度不一致，需要做统一长度处理，长度取maxlength1，不够长的句子padding 0值，句尾加上 <code>&lt;/s&gt;</code> 。</p>
<pre><code>d = 512, [batchsize，maxlen1，d]</code></pre><p>考虑到词语间的相对位置信息，还要加上语句的position<br>encoding，由函数形式直接求出。</p>
<pre><code>PE(pos,2i) = sin(pos/10002i/d)
PE(pos,2i+1) = cos(pos/10002i/d)</code></pre><p>Padding的值不做position encoding。 <code>[batchsize，maxlen1，d]</code> ，最终:</p>
<pre><code>encoder input = position encoding + input embedding。
encoder input : [batchsize，maxlen1，d]</code></pre><h1 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h1><p>Encoder 由N = 6个相同的layer连接组成。每个layer中有两个sublayer，分别是multihead<br>self-attention以及FFN。</p>
<pre><code>Q = K = V = input
MultiHead(Q, K, V) = concat(head1, …, headh)Wo
headi = Attention(QW­iQ，KW­ik，VW­iV)
Attention(Q, K, V) = softmax(QKT/$$\sqrt{d}$$) V</code></pre><p><img src="/images/transformer1.png" alt=""><br><img src="/images/transformer3.png" alt=""><br><img src="/images/transformer4.png" alt=""></p>
<p>softmax前要做key_mask，把pad 0 的地方赋值为-inf，softmax后权重做query mask，赋值0。</p>
<pre><code>h = 8
W­iQ, W­ik, W­iV : [d, d/h]
Q : [maxlen_q, d]
K = V : [maxlen_k, d]
Maxlen_q = maxlen_k so: Q = K = V : [maxlen1, d]
QW­kQ，KW­ik，VW­iV : [maxlen1, d/h]
headi : [maxlen1, d/h] \* [d/h, maxlen1] \* [maxlen1, d/h] = [maxlen1, d/h]
Wo : [d, d]
MultiHead(Q,K,V): [maxlen, d]</code></pre><p><code>Softmax([maxlen_q, maxlen_k])</code> 在最后一个维度即 <code>maxlen_k</code> 上做 <code>softmax</code>。<br>position-wise是因为处理的attention输出是某一个位置i的attention输出。</p>
<pre><code>FFN(x) = ReLU ( xW1 + b1 ) \* W2 + b2
ReLU(x) = max( 0, x )
dff = 4 \* d = 2048
W1 : [d, dff]
W2 : [dff, d]</code></pre><p>流程：</p>
<pre><code>Input -&gt; dropout -&gt;
(
multihead self-attention -&gt; dropout -&gt; residual connection -&gt; LN -&gt;
FFN-&gt; dropout -&gt; residual connection -&gt; LN -&gt;
) * 6
-&gt; memory [batchsize，maxlen，d]</code></pre><p>代码中在multihead attention中对score做dropout，FFN后没有dropout，但文章说每个sublayer的output都有一个dropout。</p>
<h1 id="Decoder-输入"><a href="#Decoder-输入" class="headerlink" title="Decoder 输入"></a>Decoder 输入</h1><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>目标句子首尾分别加上 <code>&lt;s&gt;</code> , <code>&lt;/s&gt;</code>。</p>
<pre><code>Decoder input = Output embedding + position encoding
Decoder input : [batchsize，maxlen2，d]</code></pre><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>初始向量为<code>&lt;s&gt;</code>对应embedding，之后将前一步的输出拼接到当前的所有预测构成当前的decoder输入。</p>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>Decoder由N = 6 个相同的layer组成，每个layer中有三个sublayer，分别是multihead self-attention, mutihead attention以及FFN。</p>
<pre><code>decoder input -&gt; dropout -&gt;
(
   Masked multihead self-attention(dec, dec, dec) = dec-&gt; dropout -&gt;
   multihead attention(dec, memory, memory) -&gt; dropout -&gt; residual connection
   -&gt; LN -&gt; FFN -&gt; dropout -&gt; residual connection -&gt; LN -&gt;
) * 6
-&gt; dec -&gt; linear -&gt; softmax</code></pre><p>Self-attention 的mask为一个和dec相同维度的上三角全为-inf的矩阵。</p>
<pre><code>Linear( x ) = xW
Dec : [batchsize，maxlen2，d]
W : [d, vocabsize]</code></pre><p>W为词汇表embedding矩阵的转置, 输入输出的词汇表embedding矩阵为W。即三个参数共享。</p>
<pre><code>Linear( x ) : [batchsize，maxlen2，vocabsize]</code></pre><p>Softmax函数：</p>
<center>
$p\left( k\|x \right)=\frac{\exp({{z}_{k}})}{\sum\nolimits_{i=1}^{K}{\exp ({{z}_{i}})}}$
</center>
其中zi一般叫做 logits，即未被归一化的对数概率。

<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>损失函数：cross entropy。用p代表predicted probability，用q代表groundtruth。即：</p>
<center>$cross\_entropy\_loss=\sum\limits_{k=1}^{K}{q\left( k\|x\right)\log (p\left( k\|x \right))}$</center>


<p>groundtruth为one-hot，即每个样本只有惟一的类别，$q(k)={{\delta}_{k,y}}$，y是真实类别。</p>
<center>${{\delta }_{k,y}}\text{=}\left\{\begin{matrix} 1,k=y \\0,k\ne y \\\end{matrix} \right.$</center>


<p>对目标句子onehot 做labelmsmooth用$\tilde{q}(k|x)$代替$q(k|x)$。（为了正则化，防止过拟合）</p>
<center>$\tilde{q}(k\|x)=(1-\varepsilon ){{\delta }_{k,y}}+\varepsilon u(k)$</center>


<p>可以理解为，对于$q(k)={{\delta}_{k,y}}$函数分布的真实标签，将它变成以如下方式获得：首先从标注的真实标签的$\delta$分布中取定，然后以一定的概率$\varepsilon$，将其替换为在$u(k)$分布中的随机变量。$u(k)$为均匀分布，即$u(k)=1/K$</p>
<h1 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h1><p>Adam优化器：<br><img src="/images/transformer5.png" alt=""><br>学习率使用warm up learning rate:</p>
<pre><code>learningrate = dmodel-0.5 \* min ( step_num-0.5, step_num \* warmup_steps-1.5 )
warmup_steps ：4000</code></pre>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>transoformer</tag>
      </tags>
  </entry>
  <entry>
    <title>MarkDown 使用细节</title>
    <url>/markdown/</url>
    <content><![CDATA[<p>记录使用遇到的一些常用的MarkDown命令</p>
<a id="more"></a>

<p>行内标签</p>
<pre><code>`&lt;你的 GitHub 用户名&gt;.github.io`</code></pre><p>文字居中</p>
<pre><code>&lt;center&gt;文字    &lt;/center&gt;</code></pre>]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>常用句法分析工具包使用说明：Hanlp、StanfordNLP等</title>
    <url>/parser/</url>
    <content><![CDATA[<p>对比各个常用的自然语言处理工具包中的句法分析模块。</p>
<a id="more"></a>

<h1 id="Hanlp"><a href="#Hanlp" class="headerlink" title="Hanlp"></a>Hanlp</h1><p>pip install pyhanlp 安装即可<br>项目地址：<a href="https://github.com/hankcs/pyhanlp" target="_blank" rel="noopener">https://github.com/hankcs/pyhanlp</a></p>
<p><strong>基于神经网络的高性能依存句法分析器</strong></p>
<p>输出为CONLL格式中，每个词语占一行，无值列用下划线代替，列的分隔符为制表符 <code>&#39;\t&#39;</code> ，行的分隔符为换行符 <code>&#39;\n&#39;</code>；句子与句子之间用空行分隔。<br>CONLL标注格式包含10列，分别为：  </p>
<table>
<thead>
<tr>
<th align="center">ID</th>
<th align="center">FORM</th>
<th align="center">LEMMA</th>
<th align="center">CPOSTAG</th>
<th align="center">POSTAG</th>
<th align="center">FEATS</th>
<th align="center">HEAD</th>
<th align="center">DEPREL</th>
<th align="center">PHEAD</th>
<th align="center">PDEPREL</th>
</tr>
</thead>
</table>
<p>只用到前８列，其含义分别为：  </p>
<table>
<thead>
<tr>
<th align="center">id</th>
<th align="center">name</th>
<th align="center">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="center">ID</td>
<td align="center">当前词在句子中的序号，１开始.</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">FORM</td>
<td align="center">当前词语或标点</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">LEMMA</td>
<td align="center">当前词语（或标点）的原型或词干，在中文中，此列与FORM相同</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">CPOSTAG</td>
<td align="center">当前词语的词性（粗粒度）</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">POSTAG</td>
<td align="center">当前词语的词性（细粒度）</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">FEATS</td>
<td align="center">句法特征，在本次评测中，此列未被使用，全部以下划线代替。</td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">HEAD</td>
<td align="center">当前词语的中心词</td>
</tr>
<tr>
<td align="center">8</td>
<td align="center">DEPREL</td>
<td align="center">当前词语与中心词的依存关系</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyhanlp <span class="keyword">import</span> *</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(HanLP.parseDependency(<span class="string">"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"</span>))</span></pre></td></tr></table></figure>

<pre><code>1    徐先生    徐先生    nh    nr    _    4    主谓关系    _    _
2    还    还    d    d    _    4    状中结构    _    _
3    具体    具体    a    ad    _    4    状中结构    _    _
4    帮助    帮助    v    v    _    0    核心关系    _    _
5    他    他    r    r    _    4    兼语    _    _
6    确定    确定    v    v    _    4    动宾关系    _    _
7    了    了    u    u    _    6    右附加关系    _    _
8    把    把    p    p    _    15    状中结构    _    _
9    画    画    v    v    _    8    介宾关系    _    _
10    雄鹰    雄鹰    n    n    _    9    动宾关系    _    _
11    、    、    wp    w    _    12    标点符号    _    _
12    松鼠    松鼠    n    n    _    10    并列关系    _    _
13    和    和    c    c    _    14    左附加关系    _    _
14    麻雀    麻雀    n    n    _    10    并列关系    _    _
15    作为    作为    v    v    _    6    动宾关系    _    _
16    主攻    主攻    v    vn    _    17    定中关系    _    _
17    目标    目标    n    n    _    15    动宾关系    _    _
18    。    。    wp    w    _    4    标点符号    _    _</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sentence = HanLP.parseDependency(<span class="string">"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> sentence.iterator():  <span class="comment"># 通过dir()可以查看sentence的方法</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(<span class="string">"%s --(%s)--&gt; %s"</span> % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))</span></pre></td></tr></table></figure>

<pre><code>徐先生 --(主谓关系)--&gt; 帮助
还 --(状中结构)--&gt; 帮助
具体 --(状中结构)--&gt; 帮助
帮助 --(核心关系)--&gt; ##核心##
他 --(兼语)--&gt; 帮助
确定 --(动宾关系)--&gt; 帮助
了 --(右附加关系)--&gt; 确定
把 --(状中结构)--&gt; 作为
画 --(介宾关系)--&gt; 把
雄鹰 --(动宾关系)--&gt; 画
、 --(标点符号)--&gt; 松鼠
松鼠 --(并列关系)--&gt; 雄鹰
和 --(左附加关系)--&gt; 麻雀
麻雀 --(并列关系)--&gt; 雄鹰
作为 --(动宾关系)--&gt; 确定
主攻 --(定中关系)--&gt; 目标
目标 --(动宾关系)--&gt; 作为
。 --(标点符号)--&gt; 帮助</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(dir(sentence))</span></pre></td></tr></table></figure>

<pre><code>[&apos;__class__&apos;, &apos;__delattr__&apos;, &apos;__dict__&apos;, &apos;__dir__&apos;, &apos;__doc__&apos;, &apos;__eq__&apos;, &apos;__format__&apos;, &apos;__ge__&apos;, &apos;__getattribute__&apos;, &apos;__gt__&apos;, &apos;__hash__&apos;, &apos;__init__&apos;, &apos;__init_subclass__&apos;, &apos;__javaclass__&apos;, &apos;__javaobject__&apos;, &apos;__le__&apos;, &apos;__lt__&apos;, &apos;__metaclass__&apos;, &apos;__module__&apos;, &apos;__ne__&apos;, &apos;__new__&apos;, &apos;__reduce__&apos;, &apos;__reduce_ex__&apos;, &apos;__repr__&apos;, &apos;__setattr__&apos;, &apos;__sizeof__&apos;, &apos;__str__&apos;, &apos;__subclasshook__&apos;, &apos;__weakref__&apos;, &apos;edgeArray&apos;, &apos;equals&apos;, &apos;findChildren&apos;, &apos;forEach&apos;, &apos;getClass&apos;, &apos;getEdgeArray&apos;, &apos;getWordArray&apos;, &apos;getWordArrayWithRoot&apos;, &apos;hashCode&apos;, &apos;iterator&apos;, &apos;notify&apos;, &apos;notifyAll&apos;, &apos;spliterator&apos;, &apos;toString&apos;, &apos;wait&apos;, &apos;word&apos;, &apos;wordArray&apos;, &apos;wordArrayWithRoot&apos;]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_array = sentence.getWordArray()</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment"># print(word_array[0])</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> word_array:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(<span class="string">"%s --(%s)--&gt; %s"</span> % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))</span></pre></td></tr></table></figure>

<pre><code>徐先生 --(主谓关系)--&gt; 帮助
还 --(状中结构)--&gt; 帮助
具体 --(状中结构)--&gt; 帮助
帮助 --(核心关系)--&gt; ##核心##
他 --(兼语)--&gt; 帮助
确定 --(动宾关系)--&gt; 帮助
了 --(右附加关系)--&gt; 确定
把 --(状中结构)--&gt; 作为
画 --(介宾关系)--&gt; 把
雄鹰 --(动宾关系)--&gt; 画
、 --(标点符号)--&gt; 松鼠
松鼠 --(并列关系)--&gt; 雄鹰
和 --(左附加关系)--&gt; 麻雀
麻雀 --(并列关系)--&gt; 雄鹰
作为 --(动宾关系)--&gt; 确定
主攻 --(定中关系)--&gt; 目标
目标 --(动宾关系)--&gt; 作为
。 --(标点符号)--&gt; 帮助</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 还可以直接遍历子树，从某棵子树的某个节点一路遍历到虚根</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">CoNLLWord = JClass(<span class="string">"com.hankcs.hanlp.corpus.dependency.CoNll.CoNLLWord"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">head = word_array[<span class="number">15</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> head.HEAD:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    head = head.HEAD</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">if</span> (head == CoNLLWord.ROOT):</span></pre></td></tr><tr><td class="code"><pre><span class="line">        print(head.LEMMA)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">else</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        print(<span class="string">"%s --(%s)--&gt; "</span> % (head.LEMMA, head.DEPREL))</span></pre></td></tr></table></figure>

<pre><code>目标 --(动宾关系)--&gt; 
作为 --(动宾关系)--&gt; 
确定 --(动宾关系)--&gt; 
帮助 --(核心关系)--&gt; 
##核心##</code></pre><h1 id="StanfordNLP"><a href="#StanfordNLP" class="headerlink" title="StanfordNLP"></a>StanfordNLP</h1><p>pip install stanfordnlp 安装即可<br>项目地址：<a href="https://github.com/stanfordnlp/stanfordnlp" target="_blank" rel="noopener">https://github.com/stanfordnlp/stanfordnlp</a><br>依存句法关系符号解释：<a href="https://www.cnblogs.com/sherry-yang/p/9061341.html" target="_blank" rel="noopener">https://www.cnblogs.com/sherry-yang/p/9061341.html</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> stanfordnlp</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nlp = stanfordnlp.Pipeline(lang=<span class="string">'zh'</span>)</span></pre></td></tr></table></figure>

<pre><code>Use device: gpu
---
Loading: tokenize
With settings: 
{&apos;model_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd_tokenizer.pt&apos;, &apos;lang&apos;: &apos;zh&apos;, &apos;shorthand&apos;: &apos;zh_gsd&apos;, &apos;mode&apos;: &apos;predict&apos;}
---
Loading: pos
With settings: 
{&apos;model_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd_tagger.pt&apos;, &apos;pretrain_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt&apos;, &apos;lang&apos;: &apos;zh&apos;, &apos;shorthand&apos;: &apos;zh_gsd&apos;, &apos;mode&apos;: &apos;predict&apos;}
---
Loading: lemma
With settings: 
{&apos;model_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd_lemmatizer.pt&apos;, &apos;lang&apos;: &apos;zh&apos;, &apos;shorthand&apos;: &apos;zh_gsd&apos;, &apos;mode&apos;: &apos;predict&apos;}
Building an attentional Seq2Seq model...
Using a Bi-LSTM encoder
Using soft attention for LSTM.
Finetune all embeddings.
[Running seq2seq lemmatizer with edit classifier]
---
Loading: depparse
With settings: 
{&apos;model_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd_parser.pt&apos;, &apos;pretrain_path&apos;: &apos;/home/haha/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt&apos;, &apos;lang&apos;: &apos;zh&apos;, &apos;shorthand&apos;: &apos;zh_gsd&apos;, &apos;mode&apos;: &apos;predict&apos;}
Done loading processors!
---</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">doc = nlp(<span class="string">"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">doc.sentences[<span class="number">0</span>].print_dependencies()</span></pre></td></tr></table></figure>

<pre><code>(&apos;徐&apos;, &apos;2&apos;, &apos;nmod&apos;)
(&apos;先生&apos;, &apos;4&apos;, &apos;nsubj&apos;)
(&apos;还&apos;, &apos;4&apos;, &apos;mark&apos;)
(&apos;具体&apos;, &apos;0&apos;, &apos;root&apos;)
(&apos;帮助&apos;, &apos;4&apos;, &apos;obj&apos;)
(&apos;他&apos;, &apos;7&apos;, &apos;nsubj&apos;)
(&apos;确定&apos;, &apos;4&apos;, &apos;ccomp&apos;)
(&apos;了&apos;, &apos;7&apos;, &apos;case:aspect&apos;)
(&apos;把&apos;, &apos;15&apos;, &apos;aux:caus&apos;)
(&apos;画雄鹰&apos;, &apos;15&apos;, &apos;obj&apos;)
(&apos;、&apos;, &apos;12&apos;, &apos;punct&apos;)
(&apos;松鼠&apos;, &apos;10&apos;, &apos;conj&apos;)
(&apos;和&apos;, &apos;14&apos;, &apos;cc&apos;)
(&apos;麻雀&apos;, &apos;10&apos;, &apos;conj&apos;)
(&apos;作&apos;, &apos;7&apos;, &apos;ccomp&apos;)
(&apos;为&apos;, &apos;15&apos;, &apos;mark&apos;)
(&apos;主攻&apos;, &apos;18&apos;, &apos;nmod&apos;)
(&apos;目标&apos;, &apos;16&apos;, &apos;obj&apos;)
(&apos;。&apos;, &apos;4&apos;, &apos;punct&apos;)</code></pre><h1 id="HIT-LTP"><a href="#HIT-LTP" class="headerlink" title="HIT LTP"></a>HIT LTP</h1><p>项目地址：  </p>
<ul>
<li><a href="https://github.com/HIT-SCIR/pyltp" target="_blank" rel="noopener">https://github.com/HIT-SCIR/pyltp</a>  </li>
<li><a href="https://pyltp.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener">https://pyltp.readthedocs.io/zh_CN/latest/</a>  </li>
</ul>
<p>安装步骤：</p>
<ul>
<li>pip install pyltp</li>
<li>下载模型文件：<a href="http://ltp.ai/download.html" target="_blank" rel="noopener">七牛云</a>，当前模型版本 3.4.0</li>
</ul>
<p>输出：</p>
<ul>
<li>arc.head 表示依存弧的父节点词的索引。ROOT节点的索引是0，第一个词开始的索引依次为1、2、3…  </li>
<li>arc.relation 表示依存弧的关系。  </li>
</ul>
<p>标注集请参考: <a href="https://ltp.readthedocs.io/zh_CN/latest/appendix.html#id5" target="_blank" rel="noopener">https://ltp.readthedocs.io/zh_CN/latest/appendix.html#id5</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyltp <span class="keyword">import</span> Parser</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyltp <span class="keyword">import</span> Segmentor</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyltp <span class="keyword">import</span> Postagger</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dependency_parser</span><span class="params">(sentences)</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    output = []</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        parser = Parser()</span></pre></td></tr><tr><td class="code"><pre><span class="line">        parser.load(<span class="string">'./ltp_data_v3.4.0/parser.model'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        segmentor = Segmentor() </span></pre></td></tr><tr><td class="code"><pre><span class="line">        segmentor.load(<span class="string">'./ltp_data_v3.4.0/cws.model'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        postagger = Postagger() </span></pre></td></tr><tr><td class="code"><pre><span class="line">        postagger.load(<span class="string">'./ltp_data_v3.4.0/pos.model'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">        words = segmentor.segment(sentence)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        postags = postagger.postag(words)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        arcs = parser.parse(words, postags)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        output.append(&#123;<span class="string">'words'</span>:words,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                       <span class="string">'postags'</span>:postags,</span></pre></td></tr><tr><td class="code"><pre><span class="line">                       <span class="string">'arcs'</span>:arcs</span></pre></td></tr><tr><td class="code"><pre><span class="line">                      &#125;)</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#         print(' '.join(output[0]['words']))</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="comment">#         print(' '.join(output[0]['postags']))</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    segmentor.release() </span></pre></td></tr><tr><td class="code"><pre><span class="line">    postagger.release()</span></pre></td></tr><tr><td class="code"><pre><span class="line">    parser.release()</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="keyword">return</span> output</span></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sentences = [<span class="string">'徐先生还具体帮助他确定了把画雄鹰，松鼠和麻雀作为主攻目标。'</span>]</span></pre></td></tr><tr><td class="code"><pre><span class="line">output = dependency_parser(sentences)</span></pre></td></tr><tr><td class="code"><pre><span class="line">Arcs = [each[<span class="string">'arcs'</span>] <span class="keyword">for</span> each <span class="keyword">in</span> output]</span></pre></td></tr><tr><td class="code"><pre><span class="line">[<span class="string">" "</span>.join(<span class="string">"%d:%s"</span> % (arc.head, arc.relation) <span class="keyword">for</span> arc <span class="keyword">in</span> arcs) <span class="keyword">for</span> arcs <span class="keyword">in</span> Arcs]</span></pre></td></tr></table></figure>




<pre><code>[&apos;2:ATT 5:SBV 4:ADV 5:ADV 0:HED 5:DBL 5:VOB 7:RAD 10:ADV 7:VOB 10:VOB 5:WP 16:SBV 15:LAD 13:COO 5:COO 18:ATT 16:VOB 5:WP&apos;]</code></pre><h1 id="FudanNLP-FNLP"><a href="#FudanNLP-FNLP" class="headerlink" title="FudanNLP (FNLP)"></a>FudanNLP (FNLP)</h1><p><a href="https://github.com/FudanNLP/fnlp" target="_blank" rel="noopener">https://github.com/FudanNLP/fnlp</a><br>java 接口，且不再更新，现在已经推出FastNLP</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>parser</tag>
      </tags>
  </entry>
  <entry>
    <title>SQuAD2.0 刷榜top3模型分析</title>
    <url>/SQuAD2-0/</url>
    <content><![CDATA[<p>由于<a href="https://rajpurkar.github.io/SQuAD-explorer/" target="_blank" rel="noopener">SQuAD2.0榜单</a>一直在更新，所以top3模型也在更新。</p>
<a id="more"></a>
<h2 id="top1-BERT-DAE-AoA"><a href="#top1-BERT-DAE-AoA" class="headerlink" title="top1: BERT + DAE + AoA"></a>top1: BERT + DAE + AoA</h2><ul>
<li>AoA: attention over attention [1]</li>
<li>DAE: DA Enhanced<ul>
<li>Data Augmentation</li>
<li>Domain Adaptation<br><img src="/images/aoa.png" alt="AoA"></li>
</ul>
</li>
</ul>
<p><a href="https://www.infoq.cn/article/M7NpCAAMrPzRo-RViOKs" target="_blank" rel="noopener">https://www.infoq.cn/article/M7NpCAAMrPzRo-RViOKs</a><br><a href="https://zhuanlan.zhihu.com/p/27361305" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27361305</a></p>
<pre><code>[1] Cui Y, Chen Z, Wei S, et al. Attention-over-attention neural networks for reading comprehension[J]. arXiv preprint arXiv:1607.04423, 2016.</code></pre><h2 id="top2-BERT-ConvLSTM-MTL-Verifier"><a href="#top2-BERT-ConvLSTM-MTL-Verifier" class="headerlink" title="top2: BERT + ConvLSTM + MTL + Verifier"></a>top2: BERT + ConvLSTM + MTL + Verifier</h2><ul>
<li>MTL: 多任务学习<ul>
<li><del>预测一个问题是否可答</del></li>
<li><del>预测该问题在篇章中的答案</del></li>
</ul>
</li>
<li>Verifier: 验证器 [1]</li>
<li>convLSTM [2]</li>
</ul>
<p><a href="https://msd.misuland.com/pd/12136984602514128" target="_blank" rel="noopener">https://msd.misuland.com/pd/12136984602514128</a><br><a href="https://blog.csdn.net/maka_uir/article/details/83650978" target="_blank" rel="noopener">https://blog.csdn.net/maka_uir/article/details/83650978</a></p>
<pre><code>[1] Hu M, Peng Y, Huang Z, et al. Read+ verify: Machine reading comprehension with unanswerable questions[J]. arXiv preprint arXiv:1808.05759, 2018.
[2] Shi X , Chen Z , Wang H , et al. Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting[J]. 2015.</code></pre><h2 id="top3-BERT-N-Gram-Masking-Synthetic-Self-Training"><a href="#top3-BERT-N-Gram-Masking-Synthetic-Self-Training" class="headerlink" title="top3: BERT + N-Gram Masking + Synthetic Self-Training"></a>top3: BERT + N-Gram Masking + Synthetic Self-Training</h2><ul>
<li>N-Gram Masking: 类似百度的ERNIE模型</li>
<li>Synthetic Self-Training: BERT官方PPT  </li>
</ul>
<p>(这个方法全部在预训练上做改进，没有对bert上层模型做什么改进)<br>Unclear if adding things on top of BERT really helps by very much.  </p>
<p><a href="https://zhuanlan.zhihu.com/p/63126803" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/63126803</a><br><a href="https://nlp.stanford.edu/seminar/details/jdevlin.pdf" target="_blank" rel="noopener">https://nlp.stanford.edu/seminar/details/jdevlin.pdf</a></p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p><a href="http://web.stanford.edu/class/cs224n/posters/15845024.pdf" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n/posters/15845024.pdf</a></p>
<pre><code>[1] Ensemble BERT with Data Augmentation and Linguistic Knowledge on SQuAD2.0</code></pre>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>SQuAD2.0</tag>
      </tags>
  </entry>
  <entry>
    <title>GitHub Pages + Hexo + Next：博客搭建、功能设置及美化</title>
    <url>/next/</url>
    <content><![CDATA[<p>使用Hexo博客框架搭建自己的个人博客，并部署到个人的GitHub上，选用NexT主题，添加一些使用小功能并进行界面的美化。</p>
<a id="more"></a>

<h1 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h1><p>Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。</p>
<h2 id="搭建博客框架"><a href="#搭建博客框架" class="headerlink" title="搭建博客框架"></a>搭建博客框架</h2><p>搭建流程参照官网中文说明文档：</p>
<pre><code>https://hexo.io/zh-cn/docs/</code></pre><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>将搭建在本地的博客通过GitHub Pages部署在个人的GitHub中。从而能通过 <code>&lt;你的 GitHub 用户名&gt;.github.io</code> 域名访问博客</p>
<p>参考教程：</p>
<pre><code>https://www.jianshu.com/p/05289a4bc8b2
https://www.cnblogs.com/jackyroc/p/7681938.html</code></pre><h1 id="Next-主题"><a href="#Next-主题" class="headerlink" title="Next 主题"></a>Next 主题</h1><p>部署完成后，即可对博客做一些细节上的优化，增添小工具以及美化界面</p>
<p>参考教程：</p>
<pre><code>https://zhuanlan.zhihu.com/p/30836436
https://io-oi.me/tech/hexo-next-optimization/</code></pre><h2 id="字体调节"><a href="#字体调节" class="headerlink" title="字体调节"></a>字体调节</h2><p>Next主题默认字体为 <code>font-size-medium = 1em</code> ，有点大。通常来讲，Next主题控制字体大小的文件是在主题文件夹中的 <code>source\css_variables</code> 目录下的 <code>base.styl</code> 文件中，修改如下文件 ：</p>
<pre><code>// Font size
$font-size-base           = (hexo-config(&apos;font.enable&apos;) and hexo-config(&apos;font.global.size&apos;) is a &apos;unit&apos;) ? unit(hexo-config(&apos;font.global.size&apos;), em) : 1em;
$font-size-smallest       = .75em;
$font-size-smaller        = .8125em;
$font-size-small          = .875em;
$font-size-medium         = 1em;
$font-size-large          = 1.125em;
$font-size-larger         = 1.25em;
$font-size-largest        = 1.375em;</code></pre><p>把 <code>1em</code> 改为 <code>0.875em</code> 即可:</p>
<pre><code>// Font size
$font-size-base           = (hexo-config(&apos;font.enable&apos;) and hexo-config(&apos;font.global.size&apos;) is a &apos;unit&apos;) ? unit(hexo-config(&apos;font.global.size&apos;), em) : .875em;
$font-size-smallest       = .75em;
$font-size-smaller        = .8125em;
$font-size-small          = .875em;
$font-size-medium         = 1em;
$font-size-large          = 1.125em;
$font-size-larger         = 1.25em;
$font-size-largest        = 1.375em;</code></pre><h2 id="插入本地图片"><a href="#插入本地图片" class="headerlink" title="插入本地图片"></a>插入本地图片</h2><p>资源（Asset）代表 source 文件夹中除了文章以外的所有文件，例如图片、CSS、JS 文件等。比方说，如果你的Hexo项目中只有少量图片，那最简单的方法就是将它们放在 <code>source/images</code> 文件夹中。然后通过类似于 <code>![](/images/image.jpg)</code> 的方法访问它们。</p>
<h2 id="添加❤脚注"><a href="#添加❤脚注" class="headerlink" title="添加❤脚注"></a>添加❤脚注</h2><p>更改主题配置文件：</p>
<pre><code>icon:
  # Icon name in Font Awesome. See: https://fontawesome.com/v4.7.0/icons/
  # `heart` is recommended with animation in red (#ff0000).
  name: heart #user
  # If you want to animate the icon, set it to true.
  animated: true
  # Change the color of icon, using Hex Code.
  # color: &quot;#808080&quot;
  color: &quot;#ff0000&quot;</code></pre><h2 id="搜索引擎优化"><a href="#搜索引擎优化" class="headerlink" title="搜索引擎优化"></a>搜索引擎优化</h2><h3 id="标题优化"><a href="#标题优化" class="headerlink" title="标题优化"></a>标题优化</h3><p>给标题增加详细信息,更改 <code>index.swig</code> 文件 <code>your-hexo-site\themes\next\layout</code>: </p>
<pre><code>{% block title %}{{ title }}{%- if theme.index_with_subtitle and subtitle %} - {{ subtitle }}{%- endif %}{% endblock %}

	{% block title %}{{ title }}{%- if theme.index_with_subtitle and subtitle %} - {{ subtitle }}{%- endif %} - {{ theme.keywords }} - {{ config.title }}{{ theme.description }}{% endblock %}</code></pre><h3 id="修改链接"><a href="#修改链接" class="headerlink" title="修改链接"></a>修改链接</h3><p>HEXO默认的文章链接形式为 <code>domain/year/month/day/postname</code> ，默认就是一个四级url，并且可能造成url过长，对搜索引擎是十分不友好的，我们可以改成 <code>domain/postname</code> 的形式。编辑站点 <code>_config.yml</code> 文件，修改其中的 <code>permalink</code> 字段改为 <code>permalink: :title.html</code> 即可。</p>
<pre><code>permalink: :year/:month/:day/:title/
permalink: :title/</code></pre><p>参考:</p>
<pre><code>http://www.ehcoo.com/seo.html</code></pre><h2 id="首页阅读全文设置"><a href="#首页阅读全文设置" class="headerlink" title="首页阅读全文设置"></a>首页阅读全文设置</h2><ol>
<li>在文章中手动加入 <code>&lt;!--more--&gt;</code> 进行截断</li>
<li>通过在配置文件中加入代码，自动截断（但实验失败）。</li>
</ol>
<h2 id="字数统计及阅读时长"><a href="#字数统计及阅读时长" class="headerlink" title="字数统计及阅读时长"></a>字数统计及阅读时长</h2><p>安装 <code>hexo-wordcount</code> 失败后采用下面方法：</p>
<pre><code>npm install hexo-symbols-count-time --save</code></pre><p>在配置文件加入设置：</p>
<pre><code>symbols_count_time:  
  symbols: true  
  time: true  
  total_symbols: true  
  total_time: true</code></pre><h2 id="阅读次数统计"><a href="#阅读次数统计" class="headerlink" title="阅读次数统计"></a>阅读次数统计</h2><p>主题配置文件设置：</p>
<pre><code>busuanzi_count:
  enable: true
  total_visitors: true
  total_visitors_icon: user
  total_views: true
  total_views_icon: eye
  post_views: true
  post_views_icon: eye</code></pre><h2 id="评论功能"><a href="#评论功能" class="headerlink" title="评论功能"></a>评论功能</h2><p>Next主题已经集成了GitTalk,直接使用即可。首先，注册GitHub OAuth Apps，生成<code>client_id</code> 和 <code>client_secret</code> 。再修改主题配置文件：</p>
<pre><code>gitalk:
  enable: true
  github_id: troublemeeter # GitHub repo owner
  repo: troublemeeter.github.io # Repository name to store issues
  client_id: # GitHub Application Client ID
  client_secret: # GitHub Application Client Secret
  admin_user: troublemeeter # GitHub repo owner and collaborators, only these guys can initialize gitHub issues
  distraction_free_mode: true # Facebook-like distraction free mode
  # Gitalk&apos;s display language depends on user&apos;s browser or system environment
  # If you want everyone visiting your site to see a uniform language, you can set a force language value
  # Available values: en | es-ES | fr | ru | zh-CN | zh-TW
  language:</code></pre><p>参考教程：</p>
<pre><code>https://mrluyc.github.io/2019/07/30/HexoNexT%E9%9B%86%E6%88%90Gitalk/</code></pre><h2 id="数学公式支持"><a href="#数学公式支持" class="headerlink" title="数学公式支持"></a>数学公式支持</h2><p>安装 <code>hexo-math</code>：</p>
<pre><code>$ npm install hexo-math --save</code></pre><p>在配置文件加入设置：</p>
<pre><code>math:
  engine: &apos;mathjax&apos; # or &apos;katex&apos;
  mathjax:
    # src: custom_mathjax_source
    config:
      # MathJax config</code></pre><p>在Next主题配置文件更改设置为：</p>
<pre><code># Math Formulas Render Support
math:
  # Default (true) will load mathjax / katex script on demand.
  # That is it only render those page which has `mathjax: true` in Front-matter.
  # If you set it to false, it will load mathjax / katex srcipt EVERY PAGE.
  per_page: true
  # hexo-renderer-pandoc (or hexo-renderer-kramed) required for full MathJax support.
  mathjax:
    enable: true
    # See: https://mhchem.github.io/MathJax-mhchem/
    mhchem: false</code></pre><p>在需要加载mathjax的文件的头部加入<code>mathjax: true</code>：</p>
<pre><code>---
title: transformer
date: 2019-12-10 17:50:42
tags:
mathjax: true
---</code></pre><h2 id="代码高亮"><a href="#代码高亮" class="headerlink" title="代码高亮"></a>代码高亮</h2><p>不喜欢五颜六色，所以暂时未处理。</p>
<h2 id="打赏功能"><a href="#打赏功能" class="headerlink" title="打赏功能"></a>打赏功能</h2><p>制作好微信收款码和支付宝收款码，保存至<code>themes/next/source/images</code>。并修改主题配置文件如下：</p>
<pre><code># Reward (Donate)
# Front-matter variable (unsupport animation).
reward_settings:
  # If true, reward will be displayed in every article by default.
  enable: true
  animation: true
  comment: 🤣~疯狂暗示~🤣</code></pre><p>效果如下，不信试一试~</p>
]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
</search>
